\documentclass{cambridge7A}%[multi]
\usepackage[natbibapa]{apacite}
\usepackage{xcolor}
\usepackage{tikz} %used for drawing colored boxes 
\usepackage{todonotes}
\usepackage{soul}
\usepackage{rotating}
\usepackage{floatpag}
\rotfloatpagestyle{empty}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{multind}\ProvidesPackage{multind}
\usepackage{amsmath}
\usepackage{amssymb}

%\usepackage[margin=1in]{geometry}

% \usepackage{tocloft}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{gensymb} %for symbols such as the degree sign
% \usepackage{titlesec}
% \usepackage{array}% http://ctan.org/pkg/array

% \usepackage{bibentry} %for full citations
% \usepackage{subfig} %for creating panels
% \usepackage{booktabs}% http://ctan.org/pkg/booktabs
% \usepackage{ctable}% http://ctan.org/pkg/booktabs
% \usepackage[countmax]{subfloat} %for creating panels
% \usepackage{enumitem} %better environment for lists 
% \usepackage{multirow} %to have multiple row entries in tables 
% \usepackage{breakurl} %to break urls
% \usepackage{wrapfig} %to wrap figures
% \usepackage{float}  %for floating figures

% \usepackage{hyperref} %to have links within the document

% \hypersetup{
%     colorlinks,%
%     citecolor=black,%
%     filecolor=black,%
%     linkcolor=black,%
%     urlcolor=black
% }

\graphicspath{
{figures/}
{../figures/}
}


%Nice to do notes
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}
\newcommand{\sntodo}[2][]{\todo[caption={\textbf{NB}}, size=\footnotesize, color = aliceblue, #1]{#2}~}
\newcommand{\ntodo}[2][]{\vspace{0.1cm} \hfil \todo[caption={\textbf{NB}}, size=\footnotesize, color = aliceblue, inline, #1]{#2}}
\newcommand{\sttodo}[2][]{\todo[caption={\textbf{TG}}, size=\footnotesize, color = orange, #1]{#2}~}
\newcommand{\ttodo}[2][]{\vspace{0.1cm} \hfil \todo[caption={\textbf{TG}}, size=\footnotesize, color = orange, inline, #1]{#2}}

% %For possessive citing
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}

% %Operators %TODO do we use all these?
\DeclareMathOperator*{\Do}{do}
\DeclareMathOperator*{\pa}{ca} %Parents

\newcommand{\ww}{\mathbf{w}} %Parameter
\newcommand{\cald}{\mathcal{D}} %Set of data
\newcommand{\cali}{\mathcal{A}} %Set of interventions
\newcommand{\cals}{S} %Set of models
\newcommand{\E}{\mathbb{E}} %Set of models

\newcommand{\cm}{a} %Single intervention
\newcommand{\dm}{d} %datum

\newcommand{\ca}{\mathbf{a}} %Multiple interventions
\newcommand{\da}{\mathbf{d}} %Multiple trials of data

% \DeclareMathOperator*{\zz}{\mathbf{z}} %Actual causation
% \DeclareMathOperator*{\zzz}{\mathbf{Z}} %Actual causation

%\renewcommand{\baselinestretch}{2.0}

\begin{document}

% \alphafootnotes
\chapterauthor{Neil R. Bramley (NYU), Tobias Gerstenberg (MIT), Ralf Mayrhofer (University of G{\"o}ttingen), and David A. Lagnado (UCL)}
\chapter{Intervening in Time}

% \footnotetext[1]{NYU}
% \footnotetext[2]{Supported by NSF Grant 43645.}
% \arabicfootnotes

\abstract{Much of what we know about the world comes from acting on it, and observing the consequences of our actions. In the literature, causal learning from interventions and from observing temporal dynamics have largely received separate attention due to the different datasets they are usually applied to.  However, we argue that in human cognition, interventions and temporal dynamics are inseparable.  We trace how causal inference tools developed in data science have been applied to understanding human causal learning and reasoning, highlight the current shortcomings of both intervention-based and time-based approaches taken separately, and describe recent work that starts to bring the two together.  We end by sketching an account of interventional and temporal evidence as constituents of a unified online causal learning process.
}

% \abstract{Both interventions --- actions that manipulate the world --- and temporal information have received considerable attention in psychology as cues for learning causal structure.  Inverventional learning has been studied within the frameworks of causal Bayesian networks and optimal experimental design.  We argue that, while a useful starting point, these frameworks do not provide an adequate account of human active causal learning.  We propose a more nuanced perspective:  Interventions inject signals into causal systems.  Evidence about the structure of those systems then comes via detecting the propagation of these signals.}

\section{Introduction}

Uncovering and describing the deep causal structure of reality is a fundamental goal of science, but it is also at the heart of human cognition.  We are born into a ``blooming, buzzing confusion'' \citep[p. 462]{james1890principles} of sensory information, and spend much of our development building up a causal model of reality that is both rich enough and accurate enough to guide us in pursuing our goals. On this view, science plays a supporting role in causal cognition: extending the domain of causal understanding beyond what can be inferred through everyday experience; integrating evidence at scales beyond the capabilities of the brain; and so helping resolve disagreements about hypothesized causal connections.  However, attempts to understand the psychology of causal leaning and reasoning have recently flowed in the other direction, with causal inference methods from science being used as models of causal inference in the brain. Bayesian networks have become a ubiquitous tool for modeling both non-causal and causal inference in large data sets across the sciences \citep{pearl1988probabilistic}.  Causal Bayesian networks provide a convenient calculus for learning from, and reasoning about, \emph{interventions} --- actions or experiments that manipulate things in the world \citep{pearl2000causality,woodward2003making}.  As a result causal Bayesian networks have been applied to the task of modeling human active causal learning and reasoning.  Separately, a plethora of statistical methods are used for drawing causal inferences from time series data \citep{friston2014granger,granger2004time}, and a separate line of research explores the interplay between time, causal beliefs, and perception \citep[e.g.,][]{buehner2006temporal,bechlivanidis2016time}.% and predictive processing accounts suggest that cognition is often engaged in real time causal prediction \citep{clark2013whatever}.\sntodo{Would be good to revisit predictive brain in a sentence in section 1.2.}%real time causal prediction.

In this chapter, we contend that the division in the scientific literature between intervention-driven and time-driven modes of causal inference is an artifact of the kinds of datasets that scientists have to work with.  Experimental datasets are typically aggregated over many independent samples from a population in a setting where temporal dynamics are unavailable or unmeasured, while non-experimental datasets often track multiple factors across time.  We argue that there is often no such data distinction in human experience --- we experience the world as a single ongoing event stream and are constantly choosing how and when to next intervene.\footnote{Although, we are also able to learn from data that takes more abstract forms, using language and other cultural artifacts to bootstrap from the accumulated knowledge of society.} We must be sensitive to what goes on before, during, and after our actions if we want them to be effective or informative.  As such, we advocate for a move toward modeling human causal learning as an inherently online process, involving the continuous integration and interplay of both temporal and interventional information.  We illustrate limitations of treating interventions and time as separate cues, and then highlight novel approaches and data that pave the way for a new conception of the role of intervention and time in human causal learning.

The chapter is organized as follows.  Section~\ref{section:CBN} introduces the causal Bayesian network formalism and interventional calculus, describing its successes in capturing  aspects of everyday causal cognition before highlighting other aspects that it cannot capture due to its very limited representation of time.  Section~\ref{section:DN} introduces a new framework for representing causal structure and reasoning about interventions, where causal connections are associated with expectations about causal delays \citep{bramley2018time}.  We show how this representation helps capture several aspects of causal cognition outside the scope of Bayesian networks.  Section~\ref{section:ctcv} introduces an even richer representation of continuous causal influences between continuous valued variables, again highlighting how this can shed light on other aspects of everyday causal cognition and intervention choice.  Finally, we discuss how these forms of temporal sensitivity fit into a broader picture in which structured priors, or intuitive theories \citep{gerstenberg2017intuitive}, guide learning, and discuss the interplay of representations at different levels of granularity.
%\ttodo{maybe we should have a little more detailed roadmap for the rest of the paper here?}

\section{The Causal Bayesian network account of learning from interventions}\label{section:CBN}

A core challenge for causal inference both in science and in everyday human learning is distinguishing genuine causal relationships from spurious or coincidental ones.  Many things in the world are statistically associated --- sun exposure is associated with melanin production; smoking is associated with lung cancer;  %the number of clicks an advert gets online is related to its position on a website, 
%police numbers are related to crime levels;
ice cream sales are associated with deaths by drowning; and homeopathic remedies are often associated with positive health outcomes.  Some of these associations are due to direct causal relationships --- sun exposure really causes skin to tan and smoking really causes cancer. %and ad position really does matter.  
But, in many others cases, the relationship is due to shared causal factors --- people are both more likely to eat ice cream and go swimming when the weather is warm, %police numbers and crime levels might both be effects of poverty or other aspects of governance rather, 
and people often feel better after taking a medicine they believe works, regardless of whether it is actually effective \citep{di2001influence}.  Intervention is a way of assessing whether an association is causal.  Interventions are actions that perturb the world and so reveal its structure in ways that are unavailable from mere observation \citep{woodward2003making}.  In science we call these experiments, and plan them carefully, systematically manipulating things on a large enough scale to resolve causal questions of interest in the face of irreducible noise.  When we manipulate one variable, we no longer have to worry that a resulting association between that variable and another is due to a shared cause.  For instance, an experiment systematically exposing some participants to sunlight and others to darkness, measuring their skin tones before and after, will reveal a positive relationship between the intervened-on factor (sun exposure) and its putative effect (skin tone).  %Assigning different policing policies in different regions and comparing their before and after crime statistics reveals that additional police do lower 
%In contrast, systematically giving some people homeopathic remedies and others sugar pills without telling them which, normally removes any relationship between treatment (whether the pill is a homeopathic remedy) and its putative effect (health outcomes), revealing that it is the belief in the efficacy of the drug rather than the drug itself that causes the association.  
%\ttodo{i'd remove the last paragraph about homeopathy}

Interventions in human experience are more diverse and ubiquitous than those practiced in science.  Our every action affects our proximal world in some way, from small movements that affect our pose or field of view, to the extended actions through which we interact with other objects and one another.  Sometimes we act with the goal of resolving uncertainty about a causal system (``What does this button do?'', ``What does this taste like?'',``What will happen if I shout loudly in the library?''), other times we act primarily to pursue our goals (turning on the PC, feeding ourselves, getting someone's attention).  Causal Bayesian networks \citep{pearl2000causality} provide a framework for drawing inferences based on both observations and interventions. 
% In both science and causal cognition, intervention choice (experimental design) and  causal inference have been modeled within the framework of causal Bayesian networks \citep{pearl2000causality}.\footnote{Structural equation models can be interpreted in a similar way but we restrict our discussion to Causal Bayesian networks for simplicity.}  
We now briefly describe this framework and how it has been applied to the study of causal cognition.

%Optimal experimental design \citep{fedorov1972theory}, provides mathematical answers to what interventions best resolve particular forms uncertainty.  

\subsection{The causal Bayesian network framework}

Causal Bayesian networks (hereafter ``CBNs'')  are parametrized graphs that capture aggregate patterns of covariation between variables in terms of  a network of probabilistic causal dependencies \citep{pearl2000causality}. %\ttodo{first sentence is most likely confusing to a less informed reader -- maybe just describe what they are first, before saying what they do}
Nodes represent variables (i.e. the component parts of a causal system); arrows represent causal connections; and parameters encode the combined influence of parents (the source of an arrow) on children (the arrow's target, see Figure~\ref{fig:CBN}a for an example).  CBNs can represent discrete or continuous valued variables and the functional dependence between the state of an effect on the states of its causes can take arbitrary form.  However, the majority of psychology research has focused on  systems of binary $\{0=\mathrm{absent},1=\mathrm{present}\}$ variables and has commonly assumed a simple parameterization for both generative and preventative causal relationships that we describe in more detail below \citep{cheng1997from}.  %In this setting, each variable $X_i\in\mathbf{X}$ has some basic probability $w_{0i}$ of occurrence encapsulating the influence of any causes exogenous to the model.  Generative connections \emph{increase} the probability that the effect is present given the cause is present via a noisy-OR combination of baserate $w_{0i}$ and the \emph{power} or strength $w_{ji}$ of any of its causes $X_j$ that are present, giving 
%\begin{equation}
%P(x_i=1|\pa(x_i),\ww_s)=1-(1-w_{0i})\prod_{j\in \pa(X_i)}(1-w_{ji})
%\label{noisyor} 
%\end{equation}
%where $\pa_s(X_i)$ denotes the causes of $X_i$ in causal structure $s$ and $\ww_s$ denotes the collection of $w_{ji}$ parameters in $s$, where $i\in1\ldots N, j\in0\ldots N$. Preventative causal connections decrease the probability that the effect is present via a noisy-AND-NOT combination with the baserate\footnote{For a mixture of generative and preventative influences on a single variable, one must also model their order of influence.  We do not do this here but see \ref{stephan2018preemption}.\ntodo{Has anyone ever modelled this case before?}}, giving
%\begin{equation}
%P(X_i=1|\pa(X_i),\ww_s)=w_{0i}\prod_{j\in \pa(X_i)}(1-w_{ji}).
%\label{noisyandnot} 
%\end{equation}

Figure \ref{fig:CBN}a depicts a causal network relating three binary variables $X_1$, $X_2$ and $X_3$, with one generative $X_1\!\stackrel{+}\rightarrow\!X_2$ connection and one preventative $X_2\!\stackrel{-}\rightarrow\!X_3$ connection.  Under this model, $X_1$, $X_2$ and $X_3$ all occur with their own ``base rate'' probability (i.e. due to causes exogneous to the model).  However the probability that $X_2$ and $X_3$ occur also depend on the state of their causes, such that  $P(X_2=1)$ is higher than baseline when $X_1$ is present while $P(X_3=1)$ is lower than baseline when $X_2$ is present.

%$P(X_1=1)$ is always equal to its base rate $w_{01}$ because it has no causes.  However, the probability of $X_2$ depends on the state of $X_1$ and $X_3$ depends on the state of $X_2$.  If $X_1=0$, $P(X_2=1) = w_{02}$ but if $X_1=1$ it is higher (i.e., $1-(1-w_{02})(1-w_{12})$).  If $X_2=0$, $P(X_3=1) = w_{03}$ but if $X_2=1$, it is lower (i.e., $w_{03}(1-w_{23})$).   
Figure~\ref{fig:CBN}b depicts some possible observations %$\da=\{\dm_1,\ldots,\dm_4\}$ 
produced by the causal system in Figure~\ref{fig:CBN}a. %in which different combinations of $X_1\ldots X_3$ are observed to be present or absent on what we assume are independent observations.    
Any parameterized causal model $s$ over variables $\mathbf{X} =\{X_1, \ldots, X_N\}$ assigns a probability to each datum ${\dm}=\{X_1=x_1, \ldots, X_N=x_n\}$, meaning that Bayesian inference can be used to assess which of a set of potential CBNs (which $s\in\mathcal{S}$) does the best job of accounting for data.  This will be whichever model captures the pattern of statistical (in)dependencies between the variables with the minimum number of connections.  %conditioning on known parameters $\ww_s$ or integrating over them if they are unknown.% corresponding to a true network in which $w_{01}=, w_{02}=, w_{03}=, w_{12}=$, and $w_{23}=$.

Bayesian networks embody the ambiguity described above, about how observed correlations relate to causality.  Without causal insight, we might construe the dependence between $X_1$ and $X_2$ and between $X_2$ and $X_3$ in several different ways.  One is as depicted ($X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$), but the reverse ($X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\leftarrow\!X_3$) is also possible, as is the case where both $X_1$ and $X_3$ depend on $X_2$ (i.e., $X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$).  In each case, the best-fitting parameters $\ww_s$ differ, but the marginal and conditional probability structure is 
% still captured 
preserved and the overall goodness of fit to the data will be the same.  These classes of observationally indistinguishable networks are known as ``Markov equivalent'' \citep{pearl2000causality}.

Interventions break this deadlock.  An intervention in a CBN is conceived as an action that sets the values of some of the variables in the network which will then propagate through the rest of the network.   This is analogous to reaching into the causal system, changing things within it, then observing what happened as a result. CBNs model interventions by fixing ``intervened on'' variables to their chosen values and disconnecting them from their normal causes, using Pearl's $\Do[.]$ operator \citep{pearl2000causality} to denote what is fixed on a given test.  Figure~\ref{fig:CBN}c gives an example in which $X_2$ is intervened on and set to 1 (i.e., $\Do[X_2=1]$).  If the model is correct, we should expect $X_1$ to be present with its base rate probability, while $X_3$ should be subject to the preventative influence of $X_2$ (i.e. it should be less likely to occur than if $X_2$ had been set to 0).  Figure~\ref{fig:CBN}d gives examples of interventional evidence, under which various subsets of the variables are set through intervention either to 1 or 0 with the remaining variable states generated by running the causal system.

%The space of all possible interventions $\cali$ is made up of all possible combinations of fixed and unfixed variables, and for each intervention $\cm$ the possible data $\cald_\cm$ is made up of all combinations of absent/present on the unfixed variables.  
%Incorporating an intervention $\cm$, the probability of datum $\dm$, is just the product of the probability of each variable that was not intervened upon, given the states of its parents in the model
%\begin{equation} P({\dm}|s, \ww;\cm) = \prod\nolimits_{X_i\in (\mathcal{X}\notin \cm)}
 % P(X_i|\{{\dm},\cm\}_{\pa(X_i)},\ww). \label{likelihood}
%where $\{{\dm},\cm\}_{\pa(X_i)}$ indicates that those parents might either be observed (part of $\dm$) or fixed by the intervention (part of $\cm$).\sntodo{I know this is all still too fussy! but I'm adapting it from psych rev paper, so at least its comprehensive.  What can we cut?}

% For instance, $\Do[X_1\!=\!1,X_2\!=\!0]$ means a variable $X_1$ has been fixed ``on'' and variable $X_2$ has been fixed ``off'', with all other variables free to vary.
%, propagating information from the variables that are fixed through intervention $\ci$, to the others. The space of all possible interventions $\cali$ is made up of all possible combinations of fixed and unfixed variables, and for each intervention $\ci$ the possible data $\cald_$ is made up of all combinations of absent/present on the unfixed variables.  
  %Interventions allow a learner to override the normal flow of causal influence in a system, initiating activity at some components and blocking potential influences between others.  This means they can provide information about the presence and direction of influences between variables that is typically unavailable from purely observational data \citep[see][ for a more detailed introduction]{pearl2000causality,bramley2015fcs}, without additional cues such as temporal information \citep{bramley2014order}. For instance, in Figure~\ref{fig:cbn}b, we fix $y$ to 1 and leave $x$ and $z$ free ($\ci=\Do[y\!=\!1]$).  Under the $x\rightarrow y\rightarrow z$ model we would then expect $x$ to activate with probability $w_B$ and $z$ with a probability of $1-(1-w_B)(1-w_S)$.

% Working within a space of possible models (e.g. the space of possible causal Bayesian networks) allows quantification of the evidence that observations and interventions lend to particular causal theories.\footnote{Aside from explicitly Bayesian inference --- working backwards from data to determine which of a class of generative models is most likely to have produced it --- many practical frequentist statistical techniques like mediation analysis \citep{baron1986moderator} and granger causality \citep{granger1969investigating} provide ways of testing for causality without being tied to a particular generative model.}  Thus, a natural strategy for studying causal learning and reasoning in human cognition has been assume a class of causal models that has proved successful in science, is roughly how humans represent causality.  Then try to make sense of human learning and judgment as operating within that space.\ntodo{Make clearer}  For last 20-30 years, the dominant framework for studying human causal cognition has been the CBN \citep{pearl2000causality}.

% \subsubsection{Structure inference}

% We can apply Bayesian inference to the problem of identifying the true causal structure from observational or interventional data.  Considering true model to be a random variable $S$, our prior belief $P(S)$ is then an assignment of probabilities, adding up to 1 across possible models $s\in S$. %in the set of models $\mathcal{S}$.  
% When we observe some data $\da=\{{\dm}_i\}$, associated with interventions $\ca=\{\cm_i\}$, we can update these beliefs with Bayes theorem by multiplying our prior by the probability of the observed data under each model, integrating over possible settings of the parameters $p(\ww_s)$, and dividing by the weighted average probability of those data across all the possible models:
% \begin{equation}
% P(s|\da;\ca)=\frac{\int_{\ww}P(\da|s,\ww;\ca)p(\ww)P(s)\ \mathrm{d}\ww}{\sum_{s'
%      \in S}\int_{\ww}P(\da|s',\ww;\ca)p(\ww)P(s')\ 
%    \mathrm{d}\ww}
% \end{equation}

% Crucially, the data must be independent and identically distributed, so that 
% $P(\da|m,\ww;\ca)=\prod_i P({\dm}_i|m,\ww;\cm_i)$.


\begin{figure}[t]
   \centering
   \includegraphics[width = .7\columnwidth]{CBN}
   \caption{a) A causal Bayesian network containing a generative connection (``+'' symbol) and a preventative connection (``-'' symbol) parameterized with base rates $w_{0i}$ and causal strengths $w_{ji}$.  b) Example observational data.  Yellow shading indicates a variable was present (i.e., took the value 1) while indicates a variable was absent (i.e., took the value 0) c) An intervention $\Do[X_2 = 1]$, disconnecting $X_2$ from $X_1$ and any background factors. d) Example interventional evidence.%\ntodo{I have the noisy OR probabilities $1-(1-w_{02})(1-w_{12})$ etc written out, could add them in.}
   }
   \label{fig:CBN}
\end{figure}

\subsection{Modeling intervention choice}

Different interventions yield different outcomes, which in turn have different probabilities under different models.  This means that which interventions are valuable for identifying the true structure depends strongly on the hypothesis space and prior.  For instance fixing $X_2$ to 1 ($\Do[X_2\!=\!1]$) is (probabilistically) diagnostic if you are primarily unsure whether $X_2$ causes $X_3$ because $P(X_3|\Do[X_2\!=\!1])$ differs depending whether $X_2$ causes $X_3$.  However, it is not diagnostic if you are primarily unsure whether $X_1$ causes $X_2$ because $X_2$ will take the value 1 irrespective of its causes.  Optimal experimental design theory allows us to reason about the expected value of different interventions relative to a notion of uncertainty \citep{fedorov1972theory,raiffa1974applied}.  For instance, we can define the value of an intervention as the expected reduction in uncertainty about the true model after seeing what happened. This expectation can be calculated by averaging, prospectively, over the different possible outcomes of each potential intervention %${\dm}^\prime \in \da_{\cm}$ where $\da_{\cm}$ is the space of possible outcomes of intervention $\cm$ weighted by their marginal likelihoods under the prior.  When performing interventions one after another, as in $\cm^1,\ldots,\cm^t$, we take $P(S|\dm_{t-1},\ww;\cm_{t-1})$ as our prior each time. The most valuable intervention $\cm_t$ at a given time point is then
%\begin{equation}
%\arg\max_{\cm\in \cali}\E_{{\dm}^{\prime} \in \da_{\cm}}
%\left[\Delta \mathrm{H}(S|{\dm}^{\prime}, \da^{t-1},\ww;\ca^{t-1},\cm)\right] ,
%\label{eq:info_gain}
%\end{equation}
%where $\E[.]_{{\da}^\prime \in \mathcal{D}_{C}}$ denotes the average over outcomes ${\da}'$ and $\Delta \mathrm{H}(.)$ denotes
and computing the
reduction in uncertainty in each case \citep[e.g.,][]{shannon1951prediction}. See \cite{bramley2017neurath} for a detailed mathematical account.%Shannon is just one of a broad family of possible entropy measures \citep{nielsen2011closed}.  However, it is one that has proved at least as long-run successful as a number of variants when applied as a greedy strategy for choosing interventions \citep{bramley2014should} or asking binary questions \citep{nelson2005finding}.



Well-chosen interventions can be much more informative about a causal system than mere observations.  The depicted observational data (Figure~\ref{fig:CBN}b) is much less informative than the interventional data (Figure~\ref{fig:CBN}d) with respect to the set $\mathcal{S}$ of all possible causal Bayesian networks relating the three variables.  %We demonstrate this by computing the posterior Shannon entropy $\mathrm{H}(P(\mathcal{S}|\da;\ca))$ for the sequence of observations and interventions in Figure~\ref{fig:CBN}b and d, based on an initially uniform prior over structure hypotheses $P(\mathcal{S})$ and model parameters $p(\ww_s)$. 
This is partly because the observational data does not distinguish between models with the same dependency structure, 
% (i.e. between $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$, $X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\leftarrow\!X_3$, and $X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$) 
%\ttodo{commented out these structures as you had mentioned them before}
but also because a well-selected sequence of interventions can systematically target and resolve residual sources of uncertainty, for instance by testing a particular causal connection that previous tests have not provided clear evidence about.

%\ttodo{this section feels like it's part of causal cognition already (since it's about a learner trying to minimize uncertainty -- so the next subheading is a little surprising).  maybe `Inferences from interventions' instead? otherwise it sounds a bit like were are intervening on causal cognition\ntodo{Changed heading as suggested}}

%CBNs have been widely adopted by psychologists interested in how people learn and reason about causality \citep[e.g.][]{lagnado2002learning,lagnado2004advantage,lagnado2006time,lagnado2007cues,sloman2005do,sloman2005causal,waldmann2006beyond,griffiths2009theory,gopnik2004maps,mayrhofer2011heuristics,rehder2003causal,coenen2015strategies,steyvers2003intervention,rehder2001causal, oppenheimer2013categorization, kemp2009structured,oppenheimer2004spontaneous, lee2008role, meder2014structure}.  %They have also entered into philosophical debate, providing an interventionist perspective on how one might formally ground causal claims \citep[e.g.][]{woodward2011mechanisms,danks2014unifying}.

%The CBN approach found its first success in psychology in making sense of everyday examples of reasoning that seem inherently causal yet are hard to capture under a traditional associative account of human learning and representation \citep{holyoak2011causal}.   In particular, the CBN framework captures the ways in which people's judgments exhibit ``explaining away'' --- where multiple potential causes of a common effect compete as explanations.  For example, suppose you learn that someone suffers from tinnitus.  Other things being equal, this might increase your belief that they play in a rock band.  However, if you now learn that they work in an extremely noisy factory, your suspicion that they play in a rock band will be reduced.  %CBNs also capture another phenomenon called ``screening off'', in which the information that one variable provides about another is reduced to the extent that one already knows the values of any causally intervening variables.  This is like saying that, if you already know someone is in a rock band, learning that they also like rock music will have less influence on your expectation that they have tinnitus, removing it completely if you believe the association between liking rock music and having tinnitus is entirely mediated by playing in a rock band.\ntodo{Too offtopic?}

%CBNs have also helped make sense of people's judgments of causal strength from covariation information.  Before the development of the CBN, there was a long-running debate about how people go from contingency information (i.e. patterns of cases in which variables take different as depicted in the observational evidence in \ref{fig:CBN}b) to judgments of the strength of a causal relationship.  An early proposal was that causal strength judgments reflect the change in probability of an effect given a cause --- known as $\Delta P$ \citep{allan1980note,jenkins1965judgment,lopez1998rational}.  After finding systematic judgments that $\Delta P$ could not capture, \cite{cheng1997from} proposed that causal strength estimates were better understood through the concept of \emph{causal power}, which captures the probability that a cause brought about its effect, after accounting for the chance it was caused by other, background,  factors.  \cite{griffiths2005structure} found that putative strength judgments across a number of experiments could often be better explained by the idea that learners are actually making \emph{structure judgments} within the CBN framework.  So, rather than assuming a relationship and judging its strength, participants' estimates reflected the rational judgments of the probability of a causal relationship given the evidence they had been shown.  \emph{Causal power} turns out to embody a natural way of parameterizing CBNs on binary variables, known as noisy-OR for generative causal connections and noisy-AND-NOT for preventative connections.\footnote{For a mixture of generative and preventative influences on a single variable, one must also model their order of influence.  We do not do this here but see \cite{stephan2017preemption}}  Noisy-OR considers the probability of an effect to be the probability that at least one of its causes was effective while noisy-AND-NOT captures preventative causation as the probability that the effect as caused by exogenous factors and that none of its endogenous causes was effective at blocking it.   Subsequent work on causal cognition has frequently assumed both the CBN framework and the noisy-OR/noisy-AND-NOT parametrization \citep[e.g.][]{yeung2015identifying,lu2008bayesian,lagnado2006time,coenen2015strategies}.  The example in Figure~\ref{fig:CBN}b and d is, in fact, a noisy-OR parameterized causal Bayesian network, in which variables activate by chance with probability 0.3, and both the generative and preventative causal connections have a strength of 0.8.\ntodo{Too offtopic?  Could lose some or all of prev two paragraphs on CBNs early successes (they're stolen from my thesis) since story is more about the interventions...}

\subsection{Causal Bayesian networks in psychology}

CBNs were initially adopted in psychology because of their ability to account for qualitative patterns of human judgments that are hard to capture under simple associative accounts of learning \citep[e.g.,][]{rescorla1972theory,holyoak2011causal,waldmann1992predictive,waldmann1998bayesian}.  %For example, CBNs capture ``explaining away'' effects. where the causal roles of variables lead to asymmetries in judgments of their associative strength \citep{holyoak2011causal,waldmann1992predictive,waldmann1998bayesian}. %\ttodo{i don't think the `explaining away' will be understood as currently stated} 
%For example, suppose you learn that someone suffers from tinnitus.  Other things being equal, this might increase your belief that they play in a rock band.  However, if you now learn that they work in an extremely noisy factory, your suspicion that they play in a rock band will be reduced, or ``explained away''.  The same does not occur for multiple effects.  %If you know someone works in a factory, learning that they suffer from tinnitus need not compete with beliefs about other effects of their factory work such as whether they also have calloused hands.  %that someone who has reduced hearing from tinnitus shouldn't also have ringing sounds in their ears.
%They also capture ``blocking'' effects in which learning the state of a mediator or common-cause overrides inference from one of the distal variables to the other. \ttodo{this sentence is also a little too abstract -- maybe start with the concrete example and then mention the more general principle; also ``blocking'' is only informative for someone who already knows the associative learning terminology -- many philosophers won't}  For example, in Figure~\ref{fig:CBN}, knowing that $X_1=1$ only tells you something about $P(X_3)$ (i.e., that it is less likely) if you do not already know the value of $X_2$.  
%More generally, CBNs have shed light on how people go from from contingency information to judgments of causal strength.  
The idea that evidence is filtered through a causal model, provides a powerful account of human causal learning and reasoning \citep{cheng1997from,waldmann2000competition,griffiths2005structure}.  Several studies have also shown that people make structure judgments  from contingency information qualitatively in accordance with Bayesian network theory \citep{steyvers2009bayesian,mayrhofer2016sufficiency,rothe2018successful}.  
% That is, people typically infer causal models that fall within the Markov equivalence class consistent with the data, and are able to use prior beliefs about parameters or causal roles to rule between these options where possible.

Pearl's interventional ``Do'' calculus is an effective way of capturing the different ways in which people draw inferences based on observations or interventions \citep{sloman2005do,waldmann2005seeing}. While observations license backward inferences, interventions do not. Observing that your officemate arrives at the office soaked suggests that it may be raining outside, while if you had intervened and poured a bucket of water on them, their wet clothes would not tell you anything about the weather.  The notion that people can imagine virtual interventions helps explain important aspects of thinking.  For example, virtual interventions that ``try out'' different possible actions and play out their consequences are an important part of planning \citep{pfeiffer2013hippocampal,bramley2017phd}.  Explanation often makes reference to counterfactuals \citep{lagnado2013causal,rips2013inference,rips2010two,lucas2015improved,gerstenberg2015how}, which are ``if statements'' referring to situations contrary to fact, such as: ``If Oswald did not shoot Kennedy, then someone else did''.  These kinds of inferences require a learner be able to imagine an intervention that makes the counterfactual true with minimal revision of the causal history \citep{gerstenberg2013back,rips2010two,lagnado2013causal}. 

The importance of interventional learning is well-established in education, and developmental psychology, where self-directed ``play'' is seen as vital to healthy development \citep[e.g.][]{piaget1930child, bruner1976play}.  Accordingly, a number of developmental psychologists have adopted a ``child as scientist'' analogy, which views children as fundamentally engaged in causal hypothesis testing within the CBN framework \citep{gopnik2000detecting,gopnik2004maps,sobel2006importance,mccormack2016children}.  In adults, a number of studies have found that people benefit from the ability to perform (or watch others perform) interventions during causal learning \citep{lagnado2002learning, lagnado2006time, lagnado2004advantage, schulz2001do, sobel2006importance}. 

Several recent studies have also explored \emph{how} people select what interventions to perform, comparing adults' and children's choices against the dictates of optimal intervention selection,  considering constrained and heuristic variants of this \citep{bramley2015fcs,bramley2017neurath,mccormack2016children,coenen2015strategies,steyvers2003intervention}.

\subsection{Shortcomings of CBNs for modeling everyday interventions}

In this section we highlight several mismatches between CBNs and the kinds of causal inference problems that characterize everyday causal cognition.  These break down into: 1. The requirement that evidence be gathered over independent trials; 2. the absence of temporal considerations relating interventions and subsequent measurements of the system; and 3. problems with representing cycles and feedback dynamics.

\subsubsection{Independent trials}

As we noted above, CBNs are very limited in their representation of time.  The interventional calculus embodies the minimal assumption that causes precede their effects, but it says nothing about the relative delays of competing causal pathways. %For example, a CBN in which $X_1\leftarrow X_2\rightarrow X_3$ implies the partial ordering $X_1\succ\{X_2,X_3\}$, but does not say anything about whether one should expect to see $X_1$ or $X_3$ change state first if influenced by $X_2$.  They also do not not capture that, other things being equal, we might expect a chain of causal influences (e.g., $X_1\rightarrow X_2\rightarrow X_3$) to take longer to play out than a single causal variable directly causing many different variables.  
Thus, CBNs are a natural partner to evidence that really has been gathered across multiple independent trials, in which causal relationships play out too fast to measure, or in which each variable is only measured once.  By design, these features are standard in real experimental data sets.  For instance, medical studies will typically involve a procedure for randomized recruitment to ensure participants are roughly independently sampled from the desired population. Then, there will be a protocol for when to measure outcome variables (such as blood pressure, insulin levels etc) following an intervention (i.e. delivery of a treatment).  The use of a fixed trial protocol makes it straightforward to aggregate results across the sample.  However, choosing an appropriate schedule of treatment and measurement seems to require substantial preexisting causal expertise about how long the relevant mechanisms will take to work, and about when any effects will be most measurable.  %For any single participant, much more fine grained information may be available such as continuous measures of outcome factors before, during, and after the delivery of the medicine.
%Because the mechanisms are reasonably well established, it is appropriate to aggregate and throw out majority of the more fine grained evidence that could be measured in an individual patient (e.g. continuous measures of outcome factors before during and after the delivery of the medicine).    %Presumaby the latter timeseries data is used in other contexts such as choosing an appropriate schedule but is not so easily aggregated or reported.%; a software engineer may roll out one version of a website to some users and a different version to others, and later measure their propensity to make purchases or spend time on the site.  Thus, in these settings the outcome factors are measured at some fixed, prechosen time after treatment and aggregated over many independent patients or users.

A consequence of the CBN framework being widely used to study causal cognition is that many of the experiments in the literature provide learning data that is already ``packaged'' into a CBN suitable format.  %This means the experimental set up implies that there evidence comes from independent trials, and that, within a trial, temporal information is unavailable or uninformative.  We highlight several ways in which this setting does not match well onto everyday causal learning, before turning to the role of time in causal learning. %suitably analogous to the data science problems that CBNs excel at.
%\paragraph{Independent trials}
In early studies, participants were asked to make causal judgments based on data provided in tabular form detailing how often several variables appeared together or separately \citep{cheng1990probabilistic}.   In more recent studies, participants were shown a series of cases \citep{gopnik2000detecting,sobel2004children,deverett2012learning,lagnado2002learning},  or invited to choose a sequence of interventions of their own devising \citep{coenen2015strategies,bramley2015fcs,bramley2017neurath}.  But in these studies participants are instructed or given a cover story implying that each observation should be treated as a completely independent trial.   

% \citep{deverett2012learning,lagnado2002learning} or interventions \citep{gopnik2000detecting,sobel2004children}, or invited to choose a sequence of independent interventions of their own devising \citep{coenen2015strategies,bramley2015fcs,bramley2017neurath}.  
In some cases, the cover stories involved artificial mechanisms that reset on each trial --- this includes the ``black box'' toys used in developmental studies \citep{mccormack2016children,gopnik2007causal,coenen2017beliefs}; but also a range of artificial mechanisms involving lights, sensors, circuits, and switches \citep{waldmann2000competition,sobel2006importance,coenen2017beliefs}.  In other cases, the nature of variables are not described at all \citep{bramley2015fcs,bramley2017neurath,rehder2005feature}, or independence is established by instructing participants that each trial is performed with a different sample from a population %--- such as different petri dishes of bacteria, or different shrimp pulled from Lake Victoria %with different levels of retirement savings, investment rates and trade deficits a different shrimp pulled from Lake Victoria, 
\citep{rehder2003causal, rottman2012causal, rehder2017failures}.

While ingenious, these cover stories create situations that rarely obtain in everyday learning.  The causality we encounter in physical and social systems of everyday life do not, generally, reset themselves between each interaction \citep{greville2007influence,rottman2016searching}.  In life, one learning episode typically bleeds into the next, with no clear boundaries.  We might try a medicine on multiple occasions, but if we want to treat these tests as independent we had better leave a substantial amount of time between each test, relative to our beliefs about how long the drug takes to pass through our system.  We also must keep in mind the ongoing evolution of our health and potential adaptation in our receptivity to the medication. Choosing when to act, and how to delineate between and aggregate over ``trials'' in continuous experience \citep[cf][]{tulving1972episodic}, are important aspects of real world causal reasoning, that are missed by the studies focusing on CBN-packaged data.

%A story of causal cognition that does not capture these aspects of intervention selection and inference seems to miss the larger picture.  %In life, unlike in science, one cannot ``jump into the same river twice'' \citep{barnes2013presocratics}.

\subsubsection{Timing of interventions and measurements}

In everyday life, observations cannot easily be separated \emph{between} trials, and learning from actions requires paying close attention to how things play out \emph{before, during} and \emph{after} interventions.  
% As well as separation \emph{between} trials being hard to achieve in everyday life, sensitivity to how things play out \emph{before, during} and \emph{after} a real world interventions is also intuitively important.   
\cite{rottman2012causal} point out that it is often the \emph{change} in the state of a variable from one time point to another that people seek to explain.  Furthermore, it is hard to imagine intervening on something in the world without first observing its pre-interventional state.  The metaphysics of an intervention in a CBN not only presumes independence of one test from one another, but also imposes an implausible sequence of actions and measurements on the part of the learner.  An idealized intervention involves setting variables \emph{before} observing the system.  Any causal effects of one's interventions are assumed to have entirely propagated through the system by the time the observation is made, while the trace of the resulting variable states must persist long enough to be measured together at the end.  %So, if the true structure is a strong $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$ chain and a learner performs the intervention $\Do[X_1=1]$, they expect to then look and see both $X_2=1$ and $X_3=1$.  
To make this work in psychology experiments, cover stories are used that are often either (i) vague about the measurement of time, (ii) pertain to variables whose states are only observable after the fact, or (iii) involve causal relationships that would propagate too fast to be observed.  %For example, \citeapos{steyvers2003intervention} cover story involved aliens reading each other's minds.  It was implied that aliens would keep thinking the same symbol string (i.e. ``XYZ'') unless they chose to read another alien's mind, in which case they would then think the same thought as that other alien and keep doing so until the measurement was made. 
%\ttodo{maybe ok to not give a concrete example here; i think we'd like for this `negative' section to be quite short -- i.e. point out these limitations rather generally without going into too much detail, before we then explain our extension/contribution in more detail} 

%With the Blicket detector, the outcome variable (playing a song) ensures thatwhile the mechanism is instantaneous, the outcome continues long enough to be observed.   \cite{lagnado2002learning} use several cover stories including one where the temperature and pressure inside a rocket are related to whether it successfully launches.  In these cases, there is an outcome variable that clearly occurs later but two other variables that are ambiguous enough that they  might play role of cause, mediator or effect, requiring the timing of their measurement to be left quite vague.  A very common and puzzling finding in these studies is that people struggle to separate direct and indirect causal influences %(i.e. where $X_1$ has a direct influence on $X_2$ but only an indirect one on $X_3$ in the $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$ chain)
% \citep{bramley2017neurath,fernbach2009causal,mccormack2016children}.  They thus, often appear to form something more like a successor representation \citep{dayan1993improving,momennejad2017successor}, in both direct and indirect effects are collapsed together.


\subsubsection{Cycles}

CBNs are based on a factorization of a joint probability distribution, meaning they cannot naturally represent relationships that form loops or cycles.  However, such dynamic relationships are pervasive in the world --- for example an increase in the abundance of a food source (e.g., grass) causes a population increase for animals that rely on that food source (e.g., locusts) that then reduce the abundance of the food source by eating it, which then causes most of them to die out, which again allows the food source to recover, and so on \citep{white2008beliefs,odum1959fundamentals}.  All sorts of real world processes, from population change \citep{malthus1888essay}, to economic, biological, and physical interactions, are characterized by reciprocal and dynamical causal processes giving rise to emergent behavior like periodic oscillation, self regulation or self reinforcement .  In experiments, people frequently report causal beliefs that include cyclic relationships when allowed to do so (\citealp{nikolic2015there,kim2002clinical,sloman1998feature}; but see also \citealp{white2008beliefs}).
While there are ways of adapting the CBN formalism to capture cycles (\citealp[e.g.][]{dean1989model,lauritzen2002chain}; and \citealp{rehder2016cycles} for a recent review) these either evoke a sequence of equally spaced discrete time steps or model the equilibrium behavior of the system.  Thus, none of these proposals capture how cause--effect relationships unfold in continuous time, where some relationships might occur much faster or slower than others.%\footnote{Although see \cite{pacer2015upsetting,pacer2011rational} for a model that covers related, rate-based, cases.}

%\ntodo{Seems too long and cycles doesn't fit in well}
 %In the experiments measured above, the learner is invited to trust the experimenter that these sequential trials are truly independent.  However, none of these things really hold in everyday human causal learning.  We experience the world as a single ongoing event stream.  As Hereclitus put it, one ``cannot step into the same river twice'' and so are stuck in a situation where independent trials are a luxury brought about through careful curated scenarios, rather than the basic mode of data.  %and so must act and synthesize a control condition.

%Thus, in all these studies, there participants are expected to trust the experimenter that the right amount of time was left between any interventions and measurements of other variables such that any effects will be visible, and the allocation of people to testing conditions was done appropriately randomly


\subsection{Summary and discussion}

In sum, as we move away from carefully organized experimental scenarios toward more naturalistic interventional learning data, the assumptions that lend the CBN framework its mathematical simplicity also make it less adequate for the challenge.  %The intervener is supposed to set variables without checking what state they are already in.  And they then wait ``long enough'' for all the causal processes to have taken place before looking at the new state of the system.  They are then expected to either discard the current sample and pick a new one independently from a population, or to wait long enough to be sure that there is no residual dependence in the system state before starting their next test.  
%The CBN's time free representation make a lot of sense when reasoning about experimental protocols.  I.e. if testing the efficacy of a drug, it makes sense to use one's prior expectations of how long the drug's mechanism should take to work on the body, and so choose a single appropriate time after drug delivery at which to measure all participant's health.  The resulting data then naturally aggregates over instances and allows for probabilities and propensities to be derived.  
When we interact with the causal world, we typically have access to a lot more evidence than independent joint state measurements.  We can monitor variables continuously, tracking when events occur relative to one another and how continuous quantities ebb and flow over time.  We are sensitive not just to the ``final'' states of variables after causality has been and gone, but their states prior to interventions and their subsequent transitions.  To intervene effectively and to draw sensible causal conclusions by aggregating over extended experience, we must not only worry about what variables are relevant, but also when they should be measured, how much time to leave between tests, or how to best ``reset'' a system before testing it anew.  
%These considerations suggest that much of the richness of causal evidence available from action and experience is buried by the CBNs decoupling from temporal evidence. %it is assumed that one already knows \emph{when} and where to expect effects. %(i.e., give medication, \emph{wait 2 hours}, measure health factors, discretise, fill contingency table)
This suggests that much of the important and interesting causal inference work in cognition takes place while packaging a learning problem into a CBN suitable format.  Independent trials are a luxury brought about through carefully curated scenarios, and aggregation to the level of contingencies and probabilities only becomes possible when we have rich enough knowledge of functional form to abstract away from time's arrow.  Thus, it is instructive to model this finer grain of human causal learning and reasoning in which time's arrow is fully represented.     %We are instead stuck in a situation where 
In the next sections, we build on these considerations and classic results about learning from temporal information, sketching two new approaches to modelling causal representation and interventional learning in continuous time.


%It would also be perverse to take a new medication without first taking into account our state of health prior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Incorporating time: Event data}\label{section:DN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Existing research}

%The order of events provide an obvious cue to causal directionality since causes, by definition, precede their effects \citep{hume1740treatise}.  In addition to event order, expectations about the length and reliability of delays between causes and effects can provide substantial additional information.  
%A basic associative learning result is that, as the average interval between two events increases, the associative strength between the two events decreases \citep{wolfe1921effect,shanks1987associative, grice1948relation}.  %Cognitively, this might be because the more distant two events are in time, the more costly it is to sustain the first event in working memory long enough to relate it to the second, leading to monotonic reduction in causal judgments \citep{ahn1995role,einhorn1986judging}. Normatively, it also makes sense since, ceteris paribus, the longer the gap between putative cause and effect, the more likely it is that other events may have occurred in the meantime that could have also caused the effect \citep{buehner2003rethinking, lagnado2010influence}.  However, shorter intervals do not always lead to stronger causal inferences. Rats form food aversions even when sickness is induced hours after eating --- reflecting the true time course of food poisoning \citep{garcia1966learning}.   
%Likewise, longer fixed-interval reinforcement schedules in pigeons result in longer delays between the appearance of the reinforcer and pecking responses \citep{skinner1938behaviour,gallistel2000time}.  %These results show that time-delayed associations are formed and can be used to guide action in animals.\footnote{Although, whether or not all these behaviors rely on causal beliefs is debated \citep{clayton2006rational,blaisdell2012rational}.}
People have been shown to make systematic use of both event order \citep{bramley2014order} and delay information in inferences about causal relationships \citep{buehner2003rethinking,buehner2004abolishing,buehner2006temporal}. Causal beliefs have also been shown to influence time perception \citep{bechlivanidis2013does,buehner2009causal,haggard2002voluntary}.  A basic associative learning result is that, as the average interval between two events increases, the strength with which these events are associated decreases \citep{wolfe1921effect,shanks1987associative, grice1948relation}.  However, people do not always see shorter intervals as more causal, but rather prefer intervals that match their expectations, where these might come from prior experience or through familiarity with causal mechanisms.  
Both shorter-than-expected and longer-than-expected intervals have been shown to reduce causal strength judgments \citep{buehner2004abolishing, schlottmann1999seeing,buehner2002knowledge,greville2010temporal,hagmayer2002temporal,buehner2003rethinking,greville2016temporal}.  Reliability also appears to be key to strong causal attributions from time, with variability in inter-event intervals normally associated with reduced causal judgments (\citealp{greville2010temporal,lagnado2010influence,greville2013structural}, although see \citealp{young2009problem}, for a counterexample).

Supporting the notion that temporal considerations inevitably feed into causal judgments, several studies have pitted temporal order cues against statistical contingencies.  These studies generally found that causal judgments were dominated by temporal information \citep{lagnado2006time,lagnado2004advantage,burns2009temporal,schlottmann1999seeing,frosch2012causal}.  For example,  \cite{lagnado2006time} explore a setting where a virus propagates through a network of computers infecting each with some probability, but also with variable delays in transmission from one computer to another. Participants' task was to infer the structure of these computer networks based on having observed viruses spreading through the network on multiple trials.  Participants preferred causal models that matched the experienced order in which computers displayed infection, even when covariational information (which subset of computers got the virus on each trial) suggested a different structure.  Furthermore, even when researchers have tried to instruct participants to ignore event timing, participants still often treated the observed timings of events to be diagnostic \citep{white2006structure,mccormack2016children}.  Additionally, a result common to several of the studies that have explored causal learning through interventions on CBNs have found that participants tend to draw direct connections from intervened-on components to their indirect effects \citep{bramley2015fcs, bramley2017neurath, mccormack2016children, fernbach2009causal}, ending with a kind of ``successor representation'' where each variable is associated with all of its proximal and distal consequences \citep{dayan1993improving,momennejad2017successor}.  Many of these papers explain this results by suggesting people find it unnatural to consider events that seem to occur at the same time as causing one another.


%Since effects cannot precede their causes, the order in which events occur is an important cue to causality.  In line with this, much of the human and animal learning literature is built around the notion that learners %both segment their experience into events \citep{eichenbaum1993memory} and 
%readily form associations from one event to the next \citep{watson1913psychology,pavlov1928lectures,skinner1938behaviour}.  Associative theories try to account for learning as an automatic pairwise association of stimuli. However, recent re-analyses of classical conditioning phenomena have suggested that learning is often better understood as involving inferences about the causal structure that is responsible for the observed events \citep{courville2006latent,courville2003model,courville2006bayesian,courville2004similarity,gershman2012exploring}.

%In the causal learning tradition, several papers have explored the role of temporal information in structure inference. \cite{rottman2012causal} investigated how people infer the causal structure of multiple variables measured at discrete time points at which variables may be subject to exogenous influences or interventions.  For example, suppose you are interested in an amoeba that occasionally produces two different hormones.  Suppose it is producing neither hormone at time $t-1$.  If, at time $t$, you stimulate the production of one of the hormones and the other hormone is also produced, this invites the inference that the first hormone causes the production of the second.  Importantly, this inference is based on the fact that the second hormone's level changed state relative to the preceding time point, while pure covariational inference would treat each measurement as independent.  
%In seven experiments, the authors found that people readily attribute causal relationships from variables influenced at time $t$ to others whose state changed relative to $t-1$, doing so even if a cover story strongly suggests independence (i.e., if a new amoeba is measured at each time point). %consider a series of measurements which shows that barometric pressure was high on Monday and Friday, and that it rained on Tuesday and Saturday. This evidence invites the inference that high pressure on one day causes rain on the next day. 

%Experienced event order also affects people's causal judgments when events take place in continuous rather than discretized time.  \cite{lagnado2006time} explored a situation that contrasted trial-by-trial covariation with temporal order cues. In  their experiment, a virus propagates through a network and infects computers at different times. Participants' task was to infer the structure of these computer networks based on having observed the virus spreading through the network multiple times.  Participants preferred causal models that matched the experienced order in which computers displayed infection, even when covariation cues went against temporal order cues.


%Not only the order in which events occur but also their exact timing is important for causal inference \citep{hagmayer2002temporal}.  
%A basic associative learning result is that, as the average interval between two events increases, the associative strength between the two events decreases \citep{wolfe1921effect,shanks1987associative, grice1948relation}. %Early cognitive theories predict this effect by suggesting that the more distant two events are in time, the more costly it is to sustain the first event in working memory long enough to relate it to the second, leading to monotonic reduction in causal judgments \citep{ahn1995role,einhorn1986judging}. 
%Cognitively, this might be because the more distant two events are in time, the more costly it is to sustain the first event in working memory long enough to relate it to the second, leading to monotonic reduction in causal judgments \citep{ahn1995role,einhorn1986judging}. Normatively, it also makes sense since, ceteris paribus, the longer the gap between putative cause and effect, the more likely it is that other events may have occurred in the meantime that could have also caused the effect \citep{buehner2003rethinking, lagnado2010influence}.  However, shorter intervals do not always lead to stronger causal inferences. Rats form food aversions even when sickness is induced hours after eating --- reflecting the true time course of food poisoning \citep{garcia1966learning}.   
%Likewise, longer fixed-interval reinforcement schedules in pigeons result in longer delays between the appearance of the reinforcer and pecking responses \citep{skinner1938behaviour,gallistel2000time}.  %These results show that time-delayed associations are formed and can be used to guide action in animals.\footnote{Although, whether or not all these behaviors rely on causal beliefs is debated \citep{clayton2006rational,blaisdell2012rational}.}
%Humans make causal inferences that are sensitive to expectations about delays due to causal mechanisms.
%Seeing shorter-than- as well as longer-than-expected intervals leads to reduced causal strength judgments \citep{buehner2004abolishing, schlottmann1999seeing,buehner2002knowledge,greville2010temporal,hagmayer2002temporal,buehner2003rethinking,greville2016temporal}.  Variability in inter-event intervals has usually been found to reduce causal judgments \citep{greville2010temporal,lagnado2010influence,greville2013structural}.\footnote{Although see \citep{young2009problem} for a counterexample}  %Young and Nguyen explain this increase as a consequence of experiencing occasional very short delays when there is high variability.  While these studies have focused on situations in which there is a single candidate cause--effect pair, we are interested in the more general problem of inferring the causal structure of multiple variables based on observations of events in time.  

%\cite{griffiths2005causes} showed how different expectations about delay distributions allow for strong one-shot causal structure inferences. 
%In his experiments, participants made causal judgments about ``nitroX'' barrels that were causally connected and exploded in different sequences. Because different causal models imply different event timings, the Bayesian model rapidly inferred the causal structure from an observed sequence of exploding barrels. Building on this work, \cite{pacer2012elements} model causal inference in situations where a discrete event affects the rate of occurrence of another variable in continuous time \citep[cf.][]{greville2007influence}, and 
%\cite{pacer2015upsetting} capture situations where causal influences last for some time before they gradually dissipate.

%Pacer and Griffiths' approach is well-suited for capturing situations where events alter the \emph{rate} of occurrence of other events. It does not readily apply to situations in which causes bring about their effects exactly once. In this paper, we focus on situations in which the relationship between causes and effect is singular.




%Before the causal bayesian network... human and animal learning modelled as based in association between things that occur close together in time.
%The considerations in the previous section highlight that CBNs are not equipped for modeling evidence that arrives in continuous time. Since causes must precede their effects, and we often observe events as they unfold, timeseries information contains important cues to causality. Thee fact that the light turned on, right after we switched the switch, intuitively makes us confident in the evidence provided by our intervention.  If it had come on 10 minutes later things would have been much less clear.  On the other hand, if the doorbell rings seconds after I hang up on my delivery order, I would not expect my action (ordering pizza) to have caused the bell ring; it was too quick.  So it is clear we often know how long things should take and that this influences what we learn from our observations and actions.

%As well as failing to account for the role of temporal information in learning, CBNs also cannot account for the natural problems that we expect our causal beliefs to support in everyday life, such as when to act EXAMPLE, and when to expect the outcome of our actions --- e.g. over what time scale should we expect a medication to be affecting our health, or a .

 %--- i.e. since sun exposure often precedes reddened skin, or economic downturns often precede increases in extremism, we sensibly attribute the former as causing the latter.  The formation of associations between time-locked events --- famously exemplified by Pavlov's dog salivating upon hearing a bell previously paired with his supper --- is the most basic associative learning mechanism, common to all humans and animals \citep{pavlov1941lectures}.  
%However, relying on time alone is also notoriously fallible.  %Many things are associated in time without the former causing the latter --- cockerels crow reliably before sunrise without causing it and 

\subsection{Representing causal events and interventions in time}


\begin{figure}[t]
   \centering
   \includegraphics[width = .6\columnwidth]{DN}
   \caption{a) Example causal delay network %$X_1\!\stackrel{+}\rightarrow\!X_2$ preventative $X_2\!\stackrel{-}\rightarrow\!X_3$ connection \citep[see][]{bramley2018time,bramley2017dynamic}. 
   b) Example observational evidence.  Rows denote variables and white circles mark activations over time.  c) Independent interventions (rows) on a chain structure. %with unreliable $X_1\!\stackrel{+}\rightarrow\!X_2$ connection and reliable $X_2\!\stackrel{+}\rightarrow\!X_3$ connection.   The timing of $X_2$ predicts the timing of $X_3$ because it is on the causal pathway.  
   d) Independent interventions on a common cause structure%Same for a common cause with reliable but longer $X_1\!\stackrel{+}\rightarrow\!X_3$ connection.  Here timing of $X_2$ does not predict timing of $X_3$ because it is not on the causal pathway. 
   e) Example freeform interventions that are not well spaced.   Rows denote variables, hand symbols and thick borders denote interventions, circles denote activations.  Dashed gray lines show the actual causal influences.  f) As in e) but interventions more widely spaced.  g) Interventions in a cyclic network. h) Including ``blocking'' interventions that temporarily prevent activations of the blocked variable and so break feedback loops.}
   \label{fig:DN}
\end{figure}

To account for time sensitivity in structure induction over multiple variables, we need a causal representation rich enough to encode beliefs about causal inter-event delays.  % as well as contingencies.  
\cite{bramley2018time} recently developed a normative framework that does this.  % for the case of causally related events.
Concretely, our framework captures causality between events (hereafter ``activations'') that occur at components of a system $X_1,\ldots,X_n$ at particular points in time, but have no measurable duration of their own \citep{cox1980point}.  \citeapos{bramley2018time} approach captures activation patterns within a causal system as being produced by a mixture of exogenous influences and endogenous causal relationships, with parametric delays governed by Gamma distributions with some mean $\mu$ and shape $\alpha$. Figure~\ref{fig:DN}a gives an example of an  $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$ chain analogous to the CBN in Figure~\ref{fig:CBN}.  
As in a CBN we can assume all three variables are activated due to exogenous factors with their own ``base rate'' probability.  This can be captured by a Gamma distribution where shape $\alpha$ is set to 1, equivalent to an Exponential distribution.  % with rate $1/\mu$.  
This means that the expected delay before the next activation of $X_1$ is constant, regardless how long it has been since the last activation of $X_1$ or any other variable.  Activations of $X_1$ then cause activations of $X_2$ with a %, here captured by a Gamma($\mu=1,\alpha=10$) -- i.e.,  with a mean of 1 second and standard deviation of 0.1 seconds
short and somewhat variable delay (see Subfigure~\ref{fig:DN}a, ii).  These caused activations are in addition to any activations of $X_2$ caused by its base rate.  Thus, the model implies that we should expect to see $X_2$ activate shortly after observing $X_1$ activating.  Activations of $X_2$ then have a preventative effect on $X_3$.  This is modeled by having activations of $X_2$ block the subsequent activation of $X_3$.  Mathematically, this changes the distribution for $X_3$ from an Exponential back to a Gamma, %(\lambda = 1/y)~\equiv~$Gamma$(\mu = y, \alpha = 1)$ to Gamma$(\mu = 2y, \alpha = 2)$, 
doubling the time we expect to wait until seeing $X_3$ again but also introducing a temporary dependence between latest $X_2$ and the next $X_3$ (see Subfigure~\ref{fig:DN}a, iii).  \cite{bramley2018time} provide the mathematical details for estimating model parameters and incorporating base rates and failure rates (similar to causal strengths in CBNs), as well as modeling conjunctive or disjunctive combined causal influences. However, here we simply highlight at a high level what we learn about causal cognition by applying this framework to human judgment and intervention patterns.  

\subsection{Modeling inference}

Unlike the CBN case, \citeapos{bramley2018time} approach captures how timing information informs structure judgments across potentially open-ended learning periods, without the necessity of independent trials.  If a learner already has a strong domain prior, for instance that a particular causal relationship will take a certain length of time to work, this will drive their judgments from temporal information in a straightforward way.  That is, they will only think an event caused an outcome if the interval between cause and effect matches their expectation.  However, different structures map inter-event intervals to different cause-effect delays, meaning that even without specific prior expectations about delays, the ability of candidate structures to parsimoniously account for the data can be compared, analogous to fitting a CBN to contingencies without a prior knowledge of the plausible parameters.  If a learner starts out with no expectations about how long or how reliable causal delays will be, %--- mathematically, if they start out with an uninformative ``improper'' prior on $\mu_{ij}$ and $\alpha_{ij}$ for all $i,j\in N$ --- \ttodo{remove the mathematically stuff} 
Bayesian Ockham's razor will still favor whatever structure can assign the highest likelihood to the patterns of time dependence observed between variables, while requiring the fewest separate delay parameters \citep{bramley2018time}.   For example, the reliable $X_1-X_2$ intervals %and unreliable $X_2-X_1$ delays 
in Figure~\ref{fig:DN}b will favor models that include a generative $X_1\!\stackrel{+}\rightarrow\!X_2$ connection since this can explain the activations of $X_2$ better than a model presuming them to be exponentially distributed or to be caused by any of the other variables.   %However, \cite{bramley2018time} framework goes beyond this to integrate these expectations into the causal representation and show how they can be learned through mere exposure to event data.

\cite{bramley2018time} show that people spontaneously make structure judgments from event data, broadly in line with the normative framework described above.  In three experiments, participants were asked to judge which of a range possible causal structures best describe the activations of components of a set of causal devices.    In one experiment, participants were presented with four video clips of each causal device, each showing all of the devices' components activate over 3-4 seconds.  Each video begins with the activation of the root component ($X_1$) followed by the activation of three other output components ($X_2$ and $X_3$ and $X_4$) in varying orders with varying intervals.  Participants not only ruled out structures that could not have produced all of the observed patterns, but also assigned more probability to structures to the extent that could have reliably produced the observed delay patterns. %and a minimum number of exogeneous activations.

While \cite{bramley2018time} modeled the induction of general causal structure knowledge (e.g., learning that $X_1$ causes $X_2$), \cite{stephan2018latency} have adopted the framework and proposed a model of causal attribution explaining how reasoners apply their knowledge about causal delay distributions to answer causal queries about singular cases (e.g., did $X_1$ cause $X_2$ on a particular occasion for which it is known that $X_1 = 1$ and $X_2 = 1$). In particular, \cite{stephan2018latency}  showed that causal delay information must be considered to account for the possibility of causal preemption of a target cause (e.g, $X_1$) by an alternative cause (e.g, $X_3$).  That is, the possibility that even though $X_1$ was on its way to causing $X_2$, some other background cause $X_3$ might have ``gotten in first'' and already caused $X_2$. This is a problem that singular causation judgments are highly sensitive to, but that cannot be handled by preexisting accounts of causal attribution.

\subsection{Modeling interventions}

Just as with covariational observational data in Figure~\ref{fig:CBN}b, pure observations of events in time cannot unambiguously reveal causal structure.  For example, it could be the case that the tight temporal relationship between $X_1$ and $X_2$ in Figure~\ref{fig:DN}b comes about because the variables share an unmeasured cause that brings about $X_1$ faster than it brings about $X_2$.  Fortunately, intervention again makes it possible to resolve these kinds of ambiguity.  Any temporal dependence between the intervened-on variable and activations of other variables suggests the presence of a causal influence.  For this to hold, interventions must be performed at random or prechosen intervals so as to be temporally independent of any alternative causes.  Concretely, for two variables $X_i$ and $X_j$, if the distribution of inter-event delays $p(t_{\Do[X_j], X_i})$ has a best-fitting $\alpha$ parameter $>1$, this means that $X_j$ hastens $X_i$ relative to its baseline, implying that $X_j$ (directly or indirectly) influences $X_i$.  
A key property of \citeapos{bramley2018time} modeling framework is that it captures how the sequence of events following an intervention carries information about the underlying causal structure. Post-interventional event \emph{order} places constraints on what caused what on a given occasion.  But beyond this, the causal delays inherit time variability from their parents relative to interventions.  Thus, even if an intervention causes the same activations in the same order each time, if the timing of one of these downstream events carries information about another later one, this tells us something about the causal ordering of the variables.  Figures~\ref{fig:DN}c and d visualize evidence produced by interventions on the root component of a chain and a common cause device.   The device in Figure~\ref{fig:DN}c is in fact a chain where $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_2$ with two fast connections, one unreliable connection from $X_1\!\stackrel{+}\rightarrow\!X_2$ and another reliable connection $X_2\!\stackrel{+}\rightarrow\!X_3$.  This is revealed by the data for the chain structure: if $X_2$ happens late, $X_3$ tends to happen late as well.  However below we see a $X_3\!\stackrel{+}\leftarrow\!X_1\!\stackrel{+}\rightarrow\!X_2$ with one unreliable fast connection $X_1\!\stackrel{+}\rightarrow\!X_2$ and one reliable slow one $X_1\!\stackrel{+}\rightarrow\!X_3$.  Here the $X_1\!\stackrel{+}\rightarrow\!X_2$ and the $X_1\!\stackrel{+}\rightarrow\!X_2$ delays are uncorrelated.  

We can think of the propagation of variability through the system as acting akin to a game of ``broken telephone''.  In broken telephone, players form a line and then whisper a message from one person to the next, becoming increasingly garbled along the way.  Knowing what message a player heard tells you roughly where in the line of people they were.  Similarly, the accumulation of errors in a signal (here in the lateness or earliness of the event timing) can reveal the causal sequence.  As with broken telephone, if there is no noise (if everyone passes the message perfectly; or if all causal relationships are perfectly reliable) there is no way to tell in what order the message was passed on. But, a moderate amount of noise can lead to a ``blessing of variability'' effect \footnote{The same thing holds for covariational data under certain parameterizations.  For example with no background noise and weak causal connections the latter links in a chain become progressively less likely to be present.}   \cite{bramley2018time} show that people are sensitive to this subtle signal, favoring chain structures for a range of scenarios in which a mediating variable carries information about a distal effect $X_3$ and common cause structures when it does not.%, although also finding evidence that people rely on easier to track statistics than correlation to make these judgments.

%Our post-interventional observations can also tell us about structure relating multiple downstream variables.  Postinterventional event \emph{order} places constraints on what could have caused what.  Moreover, delays inherit time variability from parents relative to intervention, therefore interventions + noise reveals structure (i.e. if late $X_2$ means late $X_3$?, like how Chinese whispers accumulation of errors implies the order of the people asked).   \hl{Demonstrate with chain vs fork inference example both from order} (currently in Figure~\ref{fig:DN} a) and from pure variability \hl{adapt figure from role\_of\_time}.  [NB note the possibility of unmeasured shared mediators resurfaces though]


%But this could easily be ancestral or indirect (Figure \ref{fig:DN}a \hl{to add, noninterventional events in time example}).

%Time is also intimately related to the idea of intervention.   

\subsection{Choosing when and where to intervene}

\citeapos{bramley2018time} approach is applicable to situations where variables may activate or be intervened on multiple times during a single learning period, and in which the underlying structure might contain cycles.  Thus, \cite{bramley2017dynamic} also used the framework to explore learners' choices of interventions in extended interactions with unknown causal systems.  Instead of giving participants a sequence of independent trials in which to set variables as in traditional interventional learning studies, participants were given a short time to interact with a dynamic causal system and perform a small number of interventions by clicking on components on the computer screen to activate them.\footnote{See \url{https://neilrbramley.com/experiments/it/videos/example\_trial.html} for a video of a trial.}   In this setting, learners had to choose both when and where to intervene.  \cite{bramley2017dynamic} explored learning about a mixture of acyclic structure (no feedback loops) and cyclic structures (containing at least one feedback loop), and contrasted learning about devices whose causal delays were known to be reliable (1.5 seconds with standard deviation 0.1 seconds) with those that were known to be unreliable (1.5 seconds with standard deviation 0.7 seconds) .  Causal connections had a small failure rate and there were no exogenous activations aside from the learners' own interventions.  Consistent with the predictions of the normative model sketched above, \cite{bramley2017dynamic} found that participants were able to identify the majority of the causal connections based on the delayed activation information they produced, and were more accurate at identifying the structure of the devices when the delays were reliable.  %However, unlike the normative model predictions, people were considerably worse at identifying cyclic structure than acyclic.  %Figure~\ref{fig:DN}f and g give example timelines of interactions with acyclic and cyclic devices.  

\cite{bramley2017dynamic} found that successful participants tended to leave large and regular intervals between each intervention, especially when the true causal relationships were unreliable.  Successful participants also tended to wait longer after the most recent activation before intervening again, suggesting they waited until they had a low expectation that any other confounding activity would occur during their intervention.  For example, Figure ~\ref{fig:DN}e shows a poorly chosen set of interventions that occur close together, making it ambiguous which activations caused which.  By contrast, the well-spaced interventions in Figure ~\ref{fig:DN}f make it clear that $X_1$ is a common cause of $X_2$ and $X_3$.  Both $X_2$ and $X_3$ reliably follow from the two interventions at $X_1$ but, on the second occasion, $X_2$ and $X_3$ reverse their order of activation, essentially ruling out the $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$ chain hypothesis that was a plausible up to that point.  No study has explored preventative causation in the domain of continuous time causal events. However, by contra-position to the above, the ideal time to test for a preventative relationship (as in the $X_2\!\stackrel{-}\rightarrow\!X_3$ link in Figure~\ref{fig:DN}a) should be when one has a strong expectation the effect is otherwise about to occur.  

For the acyclic devices tested in \cite{bramley2017dynamic}, participants exhibited a substantial preference for intervening on the root components, an effect also found in CBN-based studies \citep[e.g.,][]{coenen2015strategies, bramley2015fcs}.  Unlike in most CBN-based experimental settings, these positive test interventions \emph{are} informative about the relationships between the downstream variables due to the blessing of variability described above.   By intervening at the root of the causal device, learners can  observe how the causality propagated through the system making use of ordering and delay correlations to uncover the causal sequencing. Thus, it could be that positive testing effects are largely a result of the overapplication of a strategy that works well in the real world where we can normally expect to experience the effects of interventions playing out over time.%\ttodo{do we want to go as far as trying to explain this positive testing strategy as potentially resulting from overapplying a strategy that works well in the real world, where we normally experience the effects of interventions over time?} 

\citeapos{bramley2017dynamic} experiment is also the first comparison of human learning about acyclic and cyclic devices in continuous time.  %The two obvious differences in the evidence available in the two conditions were its abundance and its degree of ambiguity about which events caused which even when conditioning on a causal structure \citep{halpern2016causality}.  
Participants' interventions on cyclic devices often led to sustained patterns of activation that continued until a failed connection caused the system to acquiesce.  This meant that participants experienced substantially more events in total even though they tended, reactively, to perform substantially fewer interventions on the cyclic devices.  While this resulted in stronger evidence about structure for the cyclic problems according to the normative learning model, the data was also much more computationally taxing to deal with in real time given the variability in the true causal delays.  The pattern in Figure~\ref{fig:DN}g demonstrates this.  It is clear from the repeated activations that $X_1$, $X_2$ and $X_3$ are related in some cyclic way.   However, establishing the exact set of connections (i.e. whether $X_2$ or $X_3$ feeds back to $X_1$) is much harder. There are also many plausible patterns of actual causation that could have given rise to this data --- that is, there are many ways one could draw dashed lines as in the figure, to attribute each activation uniquely to a unique previous activation or intervention.   To estimate the posterior probability distribution over potential structures, the model needs to sum over large numbers of patterns of possible actual causation \citep{halpern2016causality}.  Thus, it is not surprising that human learners with limited cognitive resources, were substantially worse at identifying the structure of the cyclic devices.  \cite{bramley2017dynamic} account for this result by positing that human learners build up their causal models incrementally, reacting to each new activation by attributing it to a recent intervention or activation.  When there are many causal influences going on simultaneously, this approach becomes less accurate.

\subsection{Summary and discussion}

We have developed a normative framework for representing causal structure relating events that unfold in time, and used it to explore real time causal learning and intervention selection.   Applying this framework to experimental data demonstrated that people use temporal delays between events systematically to infer the causal structure that most parsimoniously explains what happened.  %
The framework also showed that people are able to choose sensibly \emph{when} to intervene, so as to not confound the consequences of their interventions with the unfolding dyanmics of the system. The framework captured why people have a preference toward initiating the root component of causal systems --- unlike the CBN-packaged data, this more natural temporally extended data often contains evidential signals about the structure of causality as action propagates though the system.

The analyses also shed light on the intuition that cyclic systems are naturally harder to learn and reason about. The computational cost of exact inference in \citeapos{bramley2018time} framework increased with the number of events that might be related, and cyclic systems tended to produce more activations and noncausal regularities 
% such as that between $X_2$ and $X_3$ in Figure~\ref{fig:DN}g
.  
The interventions investigated by \cite{bramley2017dynamic} can be thought of as ``shocks'' to the system, where an additional event is injected into the system by the learner.  One way that learners might get clearer structural evidence about dynamic systems in real world contexts is by using preventative interventions to ``block'' rather that just ``shock'' the system.  Figure~\ref{fig:DN}h considers a case in which a hypothetical learner uses a mixture of punctate ``shock'' interventions that create activations of variables similar to above, with extended ``block'' interventions that prevent a variable from activating for a period of time.  This ``blocking'' action is analogous to setting a variable to 0 in the CBN setting explored in \ref{section:CBN}.  %, since causal influences in the CBN were interpreted as working when the cause was active.  
Blocking breaks the feedback in the system allowing the learner to use interventions to identify one part of the structure at a time as they would in the acyclic cases.  Future work could explore how people use a combination of such ``shocks'' and ``blocks'' to learn about cyclic systems.
%The interventions investigated by \cite{bramley2017dynamic} can be thought of as ``shocks'' to the system, where an additional event is created by the learner.  Another kind of intervention t hat might be more useful for uncovering the component parts of cyclic systems could be a block, where a component is temporarily disabled, potentially short-circuiting the feedback loop within the target system and allowing learning to proceed similarly as in the acyclic systems.  

\cite{lagnado2004advantage} proposed that real-time interventions exhibit a strong order cue: events that happen shortly after interventions are likely to have been caused by these interventions.  \citeapos{bramley2018time} framework captures under what conditions this heuristic works well, but also formalizes how interventions provide structure evidence regardless of whether this holds.  If the base rates of variable activations are low relative to the duration of actual causal delays, interventions that activate the cause will tend to create a reliable order cue.  That is, effects of the intervention will frequently be the next activations to occur as they are in Figure~\ref{section:DN}e.  The fact that participants struggled when there was a dense flurry of activations in \cite{bramley2017dynamic} is suggestive that people really do rely on this kind of cue to build up their hypotheses.  
However, the normative framework also captures how inference is possible even when this does not hold.  Wherever interventions on one variable affect the distribution of delays between that variable and another, either hastening or delaying them relative to their baseline, this is evidence for some form of causation.

%As with CBNs, interventions must be performed to rule out specter of unmeasured common causes.  However, unlike the CBN case, the post-interventional dynamics also carry useful evidence that human learners pick up on.  
%FROM P7!
%Finally, we noted in the previous section that children and adults' often displayed a preference for ``positive tests'' that made all the variables activate but were typically not very informative in the CBN setting due to the lack of observable variable ordering \cite[e.g., in][]{coenen2015strategies,mccormack2016children}.  A common explanation for these patterns is that they reflect the expectation that a positive test gives the best chance of observing a sequence of causal influences propagating through the system.  %PASTED SECTION ENDS
Unlike with a CBN, a learner who forms a time-extensive representation is also positioned to make real-time predictions about ongoing dynamics \citep{clark2013whatever}.  This allows them to allow for ongoing causal processes in choosing when to intervene.   This was evident in \citeapos{bramley2017dynamic} experiment in which successful participants waited for activity to die out before intervening again.  %By comparison, recall that CBNs only support atemporal judgments about variables propensity to occur ``together'' within predefined independent trial windows.
%It is instructive to imagine dividing the evidence depicted in Figures~\ref{fig:DN}e--h into independent trials and measure the joint states of the variables in each --- i.e.,  which variables activated within each trial window.  This makes it clear that much of the important information in the evidence would be lost and that the choice of final window size and separation between windows would have a large effect on causal conclusions one might draw.

One weakness of \citeapos{bramley2018time} approach is that normative inference scales rapidly with the number of events experienced, meaning that cyclic or densely connected structures that produce large numbers of events become very hard to evaluate.  One has to reason about many paths of actual causation playing out simultaneously.  Fortunately, as the number of events becomes unmanageable for reasoning at the level of one-to-one cause-effect mappings, one can start to reason instead about whether activations and interventions affect the \emph{rates} of occurrence of other variables.  \cite{pacer2015upsetting} model this setting, reanalyzing an experiment from \cite{lagnado2010influence} in which the occurrence of several types of ``seismic wave'' could temporarily alter the rate at which earthquakes occur.  This approach matches with a range of findings suggesting that beyond a small number of unique entities, people switch to approximate counting, known as subitizing, rather than attempting to focus on explicitly on each individual entity \citep{mandler1982subitizing}.
%On \citeapos{pacer2015upsetting} approach, there is no exact mapping between a cause and an individual effect,  the inference can then be performed for situations where there are arbitrarily large numbers of events

Another limitation of \citeapos{bramley2018time} approach is its assumption that causality relates to point events.  This idealization seems reasonable for processes where the causal influences are occasional and sudden --- such as spiking neurons, earthquakes, gunshots, explosions and so on.  However, many other causal influences are continuous in time, and many causally relevant variables can take a continua of values with no transition or state being causally privileged.  Thus, in the next section we extend the continuous time framework to capture learning and inference about variables that evolve and affect one another continuously in time.

% Representing causality in this way supports predictions about \emph{when} causally related events will occur and %ccur, have occurred, or would occur following interventions, as well as 
% captures how observed event timings provide evidence that helps identify the true causal model.  
%\citep{stephan2018latency}


% We survey the way that event timing adds to the evidence available from interventions
% \begin{enumerate}
% \item At a more fine grained level, 
% \item We partially solve this problem by intervening in time \citep[][]{lagnado2010influence,pacer2012elements} -- this at least lets us rule out the shared parent worry (show how it solves ambiguity from above).
% % \ttodo{relevant references by \cite{stephan2018preemption} and they have a more recent cogsci paper coming out this year as well}
% \end{enumerate}


% While events \emph{can} be collected up into frequencies and 'probabilities of co-ocurance' within time windows (e.g. over some time following an intervention, in raw experience they occur in time and are separated by delays and it is not clear under what conditions, ``binning'' this data will give us reliable causal evidence. 

%Events are a very general class of phenomena that need not be construed as punctate.   We discuss this assumption below, and consider how to model casual influences between temporally continuous variables in Section~\ref{section:ctcv}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Incorporating time: Continuous variables}\label{section:ctcv}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


While events are often a natural level for reasoning about the world, they need not be construed as discrete or punctate, and change need not be thought of as being constituted by \emph{events} at all \citep{soo2018causal}.  Many variables change continuously over time without clear change points, and their consequences can often ``linger on and mix with the effects of other actions'' \citep[p224]{jordan1992forward}. This means that in many contexts, better causal evidence can be had, and finer grained causal predictions can be made, if one represents the causal world as a system of continually interacting continuous-valued variables.  Many real-world applications of causal inference methods focus on this kind of data.  Financial forecasting, climate research, and many aspect of structure estimation in neuroscience involve reasoning about causal influences between continuous variables.   The most well-established of these approaches is the so called ``Granger causality'', which uses time-delayed regressions to assess whether one variable predicts the later values of another \citep{granger1969investigating}.  However, this approach suffers from the same limitations as any observational method for assessing causality.\footnote{Although, see \cite{eichler2010granger} for preliminary work linking interventions with estimates of Granger causality.}  Frequently variables \emph{Granger cause} one another without truly causing one another, due to their both being influenced by some unmeasured common cause.  For example, because neural activity waxes and wanes due to natural circadian rhythms, regressing one neural signal on another, even with a short delay, will often result in a significant correlation because both measurements share patterns of variation that stem from the brain's overall energy consumption rather than from specifically directed communication.  %This is the case with the example at the start, ice cream sales may Granger cause drowning deaths but fluctuations in both these variables are much more likely to be due to changes in weather and season.  
A number of studies have also explored how people learn continuous valued functional relationships more generally, but not in the context of inferring causal structure \citep{pacer2011rational,griffiths2009modeling,schulz2017compositional}.  So, in this last section, we survey a framework for modeling interventional causal learning about continuous valued variables that influence one another in continuous time. Note we are note arguing this framework supercedes that of \cite{bramley2018time}, but that rather that it is able to capture another form of time sensitivity that is appropriate for some learning contexts but inappropriate for others.%\ttodo{a reader might wonder here whether this new framework supersedes the old one they have just learned about, or whether it extends it -- i.e. tackles a different problem} 

\subsection{Representing continuous causality}

\begin{figure}[t]
   \centering
   \includegraphics[width = \columnwidth]{OU}
   \caption{Observations and interventions on a causal system relating continuous variables in an OU network.  Adapted from \cite{davis2018ctcv}.    Black dashed lines indicate interventions that hold variables in some position or move them systematically.}
   \label{fig:OU}
\end{figure}

\cite{davis2018ctcv} recently developed a new framework for modeling networks of continuous valued variables interacting causally with one another.  Their approach is based on the Ornstein-Uhlenbeck (OU) process \citep{uhlenbeck1930theory}.  %An OU process is a stationary Gaussian Markov process capturing random motion that tends to revert to some mean value. \ttodo{lots of concepts that many readers won't have heard about; if you think you'll need them, then fine; if you think you can do without, then better :) } 
OU processes describe something like Brownian motion that is subject to a corrective force that increases the further the variable strays from its mean and  %. OU processes 
have been used to model a variety of phenomena including physical \citep{lacko2012planning} and financial \citep{barndorff2001non} systems as well as attention during multiple object tracking \citep{vul2009explaining}.  \citeapos{davis2018ctcv} insight was to treat all the relationships in a causal network as simultaneously evolving OU processes, such that each variable is noisily attracted to some function of the current states of its cause(s).  %\ntodo{Ralf sensibly asked whether this is really continuous.  I think that while OU processes are truly continuous (defined by stochastic differential equations, the fact that we use the value of the case at timestep t to determine the attractor state for the effect at t+1 makes the causality part discrete time.  I'm not sure what would happen to the model if one would to shrink the timestep toward zero.}

%In a normal OU process, the change in $X_i$ from time $t$ to $t$+1 is defined as

% \begin{equation}
% \Delta X_i^t =  N(\theta(\mu - x^t),\sigma)
% \end{equation}
%where $\mu$ is the mean that the process reverts to in asymptote, $\sigma$ is the noise level and $\theta$ controls how strongly the process is attracted to its mean.  
To model ongoing causal relationships, the mean attraction state for a variable $X_i\in\mathcal{X}$ is defined as some function of the latest value of its putative cause variable(s) $X^t_j$.\footnote{Here we express OU dynamics between small but discrete equal time steps.  However, in its general form an OU process is described by a stochastic differential equation, making it truly continuous over time.}  \cite{davis2018ctcv} focus on linear relationships, with a single multiplicative parameter $w_{ji}$, such that the updated value of $X_i^{t^{\prime}}$ is given by

\begin{equation} \label{cont_OU_def}
X_i^{t^{\prime}} = X_i^{t} + N(\theta ( w_{ji} \cdot X_j^t - X_i^t),\sigma).
%dx_t = \theta (\beta_{YX} \cdot y_t - x_t)dt + \sigma dW_t
\end{equation}
where $\mu$ is the mean that the process reverts to in asymptote, $\sigma$ is the noise level and $\theta$ controls how strongly the process is attracted to its mean.  
Thus, if $w_{ji}>0$, $X_i$ will be attracted to some positive fraction of the value of the latest value of $X_j$.  In line with the previous sections, we call this a ``generative'' connection as it implies that increases in $X_j$ cause increases in $X_i$.  If $w_{ji}<0$, $X_i$ will be attracted to some negative fraction of the latest value of $X_j$, with $X_i$ tending to decrease as $X_j$ increases and visa versa.  Values of $w_{ji}$ between $-1$ and $1$ will make $X_i$ undershoot the absolute value of $X_j$, while values greater than 1 or less than -1 will make $X_i$ overshoot. There is no causal relationship from $X_i$ to $X_j$ if %$w_{ji}=0$ or
the attraction strength $\theta_{ji}=0$.  If a variable has no causes, its movement can be modeled as a classical random walk or as tending revert toward some static mean value.  %In the former case, the value of $X_i$ simply trends toward zero, and in the latter its movement is a classical random walk.  
\cite{davis2018ctcv} assume multiple causal influences sum in determining the attractor state for the effect. % such that

% \begin{equation}
% \mathbb{E}(\Delta X_i^t) = \theta \Big(\sum_{j=1}^{n} w_{ji} X_j^t - X_i^t \Big)
% \end{equation}
% and

% \begin{equation}
% p(\ww, \Theta| \Delta X_i^t, X_j^t) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(\Delta X_i-\mathbb{E}(\Delta X_i^t))^2}{2 \sigma^2}}
% \end{equation}
% where $\ww$ collects all $w_{ji}$ parameters for $i,j\in N$ and similarly for $\Theta$ and $\theta_i$.  
Figure~\ref{fig:OU}a shows an example continuous variable network $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$ analogous to those explored in Sections~\ref{section:CBN} and \ref{section:DN}.  In it, $X_1$ has no causes.  $X_1$ is then a generative cause of $X_2$ %($\theta_2=.1, w_{21} = 1.25$) 
which is a preventative cause of $X_3$. %($\theta_3=.1, w_{32} = -1.25$).  
The subplots visualize the distribution of possible $X_i^{\prime}$ values for each variable relative to the variable's current position and the current position of its cause.%Each variable moves $\theta=.1$ of the way between their current and asymptotic state per ``timestep'' with $\sigma=4$ normally distributed noise.  

%\ttodo{i think this is too mathy for a philosophy journal; it would be best to get rid of equations and parameters as much as possible, and instead explain this conceptually (and by pointing to a figure)} 

Figure~\ref{fig:OU}b shows example data generated by the network in Figure~\ref{fig:OU}a.  The root component $X_1$ (red) drifts around randomly, while $X_2$ (green) chases noisily behind $X_1$, and $X_3$ (blue) chases the inverse of $X_2$.  As with previous settings, it is not possible to tell for certain what the right causal relationships are, based on this purely observational data.  This is partly because all three variables could be effects of some common cause with different time lags (i.e. with different $\theta$ values). Additionally though, the sustained corrective causal influences mean the downstream variables $X_2$ and $X_3$ rapidly asymptote to be close to their target values, making it hard to see which moved first unless $X_1$ happens to move dramatically.  

One interesting property about these continuous time networks that is not captured by the event networks in Section~\ref{section:DN} is their ability to produce rich emergent feedback dynamics of the kinds observed in the natural world.  %Figure~\ref{fig:OU}c--e give examples of cyclic networks with very different emergent behavior.  %Figure~\ref{fig:OU}c shows an 
For example a pair of variables with bidirectional influences that undershoot one another (i.e., $w_{ji}=w_{ij}<1$) creates an inhibitory feedback loop where both variables trend toward and then stay around zero no matter their starting point, while settings where the feedback leads to progressively larger overshooting (i.e., $w_{ji}=w_{ij}>1$), excitatory feedback, where the variables rapidly approach positive or negative infinity.  %Finally, Figure~\ref{fig:OU}e 
Feedback with opposite signs (i.e. $w_{ji} = -w_{ij}$) leads to oscillations such as those seen in predator--prey dynamics, where variables chase one another up and down in oscillations that can either acquiesce or can increase in magnitude over time.  Since these kinds of emergent dynamics are common in the natural world, it is valuable to have a simple representation that is able to mimic them.  However, it remains to be seen how commonly observation of high level dynamics uniquely reveals the internal structure of the system, as it may be that there are multiple ways of setting up a system of variables to exhibit the same emergent behavior.%\ntodo{Not really thought about this before, but interesting question what cyclic causal networks are identifiable by having unique emergent dynamics}

%Cite the Nicolas Cage movies cause shark attacks example.  As with event data, there is a high risk that apparently propagated noise is actually noise from a shared source since, going far enough back, everything has a common ancestor.


\subsection{Modeling inference and interventions}

\cite{davis2018ctcv} devised a simple experiment to explore human interventional learning about continuous valued variables in continuous time.  As in the studies discussed in Sections~\ref{section:CBN} and \ref{section:DN}, participants were tasked with identifying the structure of a number of causal devices by interacting with them on the computer screen.  As in \cite{bramley2017dynamic} they were given a short period to interact with each device.  However, rather than being visualized as clickable nodes, the variables were visualized with vertical sliders.  The underlying network could contain generative $w=1$ and preventative $w=-1$ connections and feedback loops.  Unless intervened on, the sliders' positions updated continually, %10 times per second 
appearing to jitter and track up and down as in Figure~\ref{fig:DN}b. By intervening and moving the sliders, participants could hold variables at desired values or drag them up and down.%was achieved by clicking on it, which instantly moved it to the clicked on value, and by hold-clicking participants could keep the variable at a desired location or drag it around smoothly within the sliders' range.
\footnote{See \url{https://neilrbramley.com/experiments/ctcv/demo.html} to try an example trial.}

Through interacting with the systems, participants were able to identify the large majority of the connections.  As in the other causal learning experiments discussed, the most frequent error was a failure to distinguish direct and indirect causal influences.  Participants would often mark direct connections from intervened on variables to indirect effects.  Due to the convenient mathematical properties of OU processes, Bayesian inference could again be used to estimate a posterior for each trial and used as a normative standard against which to compare participants' judgments.  \cite{davis2018ctcv} compared full Bayesian inference against a local computations model that assumed learners built up their models by focusing on one pair of variables at a time rather than the whole structure, finding that around two thirds of participants were better described by this heuristic \citep[cf][]{fernbach2009causal}.  The normative analysis also revealed that with only one exception, participants interventions dramatically increased the strength of the available evidence.

The role of interventions in this continuous context is intuitively have an additional property compared to the previous settings.  Because the variables are capable of taking a wide range of values but naturally move only a little per timestep, interventions allow learners to inject dramatic swings and signals into the system, or hold variables at extreme or unusual levels \citep[Figure~\ref{fig:OU},][]{davis2018ctcv}.  For example, Figure~\ref{fig:OU}c shows a learner performing several dramatic interventions.  First they intervene to hold $X_1$ at 50 (here, for 40 ``timesteps'').  This results in a marked increase in $X_2$ and, with more noise and lag, a reduction in $X_3$.  They then intervene to hold $X_2$ at -50 which only increases $X_3$, while $X_1$ returns to moving randomly, suggesting the structure is a chain.  Third, they intervene on $X_3$ in a sinosoidal pattern leads to a (lagged and noisy) sinosoidal pattern in $X_2$.  While \cite{davis2018ctcv} did not analyze participants precise intervention patterns, a large proportion appeared to involve moving and holding variables at extreme values, or rapidly dragging them up and down the full range of the sliders.  It is worth noting that in this setting, interventions naturally act as both ``shocks'' and ``blocks''.  Intervening on a variable for an extended period not only takes over its state but also blocks it from participating in feedback dynamics.

\subsection{Continuous time control}

The interventions participants could perform in \cite{davis2018control} were often extended in time, meaning participants were able to react to the movements of the other variables during their intervention, for example they might be manipulating one variable in such a way as to keep another (effect) variable in a particular position.  %challenges the use of \citeapos{pearl2000causality} ``do'' operator.  Learners could react to the movements of the other variables during their intervention, for example manipulating one variable in such a way as to keep another effect variable in a particular position.  %In doing this their extended action is no longer a purely exogenous influence on the system, but actually becomes part of a larger system involving the reactive influence of the observed dynamics on the control.  
This reactivity brings the interventional learning problem increasingly close to an adaptive or ``dual control'' problem \citep{feldbaum1961dual,guez2015phd,klenske2016dual,schulz2017control}. This means one faces the ``dual'' problem of learning how the system works while already attempting to control it. For example, we learn to play tennis, largely, while attempting to play tennis. At first our shots are wild and do not go where we intend. But, we slowly learn to adapt our swing to different angles and speeds of the incoming ball hopefully building a causal control model of tennis in the process.

A key question therefore, is to what extent people will spontaneously learn causal structure while attempting to master the control of a causal system.  To investigate this, \cite{davis2018control} used their OU driven causal models as a class of control problems.   Their task was similar to to that in \cite{davis2018ctcv}, except that participants' interventions were restricted to one ``control'' variable, and limited single step increments up or down.  Participants' goal was to keep another ``target'' variable in a reward region.  To master this task in a model-based way, a participant would have to learn the structure of the network and use this knowledge to plan the actions that maximize the time that the target variable stays in the reward region.  %$For some simple structures explored by \cite{davis2018control}, it was enough to exercise model-free control  \citep{dayan2014model}.  This means simply learning a mapping directly from the control variable to the target variable, without building a model of the relationships between the variables.  As it turned out, some structures that involved a mixture of generative and preventative connections were particularly hard to control, requiring the participant not just to understand the causal structure of the system, but also to plan their model based control strategy many frames ahead.  In these systems, the only effective way to cause the target variable to enter the reward region involved a inputting a periodic oscillation of the control variable.  
Overall, participants learned to control all in most causal systems well above chance, and learning profiles were broadly consistent with the idea that they first explored then exploited the causal model.  Future work will need to carefully probe what representation participants for during a control period.


\subsection{Summary and discussion}

In summary, networks, made up of OU-related continuous variables provide a powerful extension of causal models for reasoning about fine-grained continuous time relationships, as well as providing an interesting test bed of control environments.  The dynamics of these systems map onto real world problems in intuitive ways and display the kinds of emergent properties we see in real world dynamic causal systems.  These networks also have convenient mathematical properties that make normative inference tractable and so allow assessment of the evidential value of interventions.  OU processes are unique in being Gaussian, Markovian, and stationary, all of which make likelihood estimation closed form and straightforward \citep{uhlenbeck1930theory}.  %By locating all non-stationarity in the changing target values determined by the a variable's causes, the resulting system is straightforward to analyze in spite of its rich emergent dynamics. \ttodo{maybe drop previous sentence}  
People were able to learn robustly through intervention in this setting, suggesting that they are not limited to reasoning about causality at the level of independent trials or even of real time event cascades.

Interventions in these systems can be used to create dramatic signals that are hard to mistake if they appear in the dynamics of downstream variables.  Participants were generally sensitive to this in that they used dramatic swings or held variables at extreme values, which allows for maximally strong causal signal that is easily spotted as it propagates to other variables.  However, the richness and immediacy of the evidence also seemed to push participants toward adopting a local strategy focusing on pairs of variables at a time and so struggling to infer the correct structure in the case of indirect causation.  As with the event networks, one way to use interventions to get clearer local information in this setting would be to allow combination interventions in which one variable is held in place while another is wiggled around.  For instance holding $X_2$ stationary and moving $X_1$ up and down in Figure~\ref{fig:OU} would make it easy to establish that $X_1$ does not directly influence $X_3$ except through $X_2$.  Intuitively, this is a very natural kind of intervention to perform in everyday life, but one that is not captured by the simpler and more abstract notions of intervention and causal influence embodied by the CBN or even the delay networks in Section~\ref{section:DN}.  %\footnote{One analogous case though is \cite{steyvers2003intervention}, an early study looking at interventional learning in a CBN setting where aliens read one another's minds.  In \citeapos{steyvers2003intervention} study, aliens' spontaneous thoughts were three letter strings using three different letters of the alphabet (``TUS'' or ``POR'').  However interventions always make them think ``ZZZ'', which they never normally think.  Thus when another alien was revealed to be thinking ``ZZZ'' it was possible to be very sure that this was effect of the intervention.} \ttodo{probably ok to skip the footnoe}  
This highlights a key property of real-world interventions.  Sometimes it is possible to insert a unique signal with an intervention by setting a variable to a highly unique value or moving it in a highly unusual way.  The subsequent inferences is then a kind of message detection, where one looks for traces of the original signal reappearing in other variables.
%We also argued that as interventions become extended and reactive actions, interventional learning mutates into a continuous control learning problem.  We discuss this further in the general discussion, this may mean we should start to move away from the analogy of interventional causal learning as a form of optimal experimentation, and toward the idea that causal learning is a process of balanced explorative control.

\section{General Discussion}

There are a number of reasons why CBNs provide a good starting point for thinking about causal cognition.  However, we have argued that there are also fundamental reasons why they do not provide a fully adequate account.  CBNs represent time only as a partial ordering of influence, while real learning contexts demand a deeper sensitivity to time's arrow.  We highlighted recent work that introduce new formalisms and use them to explore the ways in which human causal learners are sensitive to time.  This revealed sophisticated time sensitivity in inferring causal structure but also in timing interventions and control.  The representation developed by \cite{bramley2018time} captured causality relating events in time and showed how a preference for reliably timed causal influences can guide structure inference, as well as how sensitivity to the possibility of delayed effects shaped participants intervention behavior.  The extended representation, developed by \cite{davis2018control}, modeled causality at a finer grain, in terms of ongoing influences between continuous-valued variables. Comparison of human learners in such a setting revealed that people are capable of interpreting rich continuous causal evidence and adept at using the full range of relevant variables to generate distinctive interventions whose signatures could be easily tracked as they propagated through the system.

\subsection{Interventions in rich domains}

Much of the work on intervention selection has used the metaphor of optimal experimental design \citep{fedorov1972theory}.  However, exploring richer interventions and learning of richer causal representations seems to suggest a different metphor \citep{coenen2017asking}.  We can think of interventions in richer temporally extended settings as inject signals into causal systems, similar to how a plumber might pour dye down a sink while trying to figure out a network of pipes in an old house.  Wherever the dye shows up must be downstream of the sink, and the length of time it takes to get there says something about the network of pipes in between.  When the propagating signal is not very distinct --- such as the time of occurrence of an event as in Section~\ref{section:DN}, or the final states of the variables after a causality has been and gone in Section~\ref{fig:CBN} --- collecting multiple approximately independent trials is still important.  However, the learner again must be able to use her knowledge about the domain to create situations that are approximately independent and identically distributed. 

One other consideration is that punctate interventions, or ``shocks'' allow a dynamic system to continue to play out as before,  while sustained interventions also act as ``blocks'', and stop feedback loops.  This is valuable in that it can break a cyclic structure learning problem into several acyclic ones.  For instance, one can first check whether $X_i$ affects $X_j$ by performing a sustained intervention on $X_i$ before checking whether $X_j$ also affects $X_i$.  However, this approach will short circuit the natural dynamics that might themselves be a useful source of evidence.  As an analogy, one might contrast the punctate (and probably very destructive) intervention of throwing a spanner into a washing machine while it is spinning, to a more systematic sustained intervention in which one holds and turns one of the gears in the mechanism and observes what else moves.  %Link it loosely to signal detection theory/information theory, you want to maximize your chance of recapturing signal in other variables.  Analogy: throwing a coin in a washing machine when it is spinning vs clamping a gear and turning it on.

\subsection{The role of theory}

The representations we considered in this chapter are abstract in the sense that they do not encode anything specific about the mechanisms involved, notably saying nothing about how the parts of the systems are arranged in space.  However, we clearly make use of our knowledge of physical mechanisms when reasoning causally \citep{ahn1995role,bramley2018physics}.  Indeed, much of the recent work in causal cognition has emphasized the ``top down'' role of domain theories on causal inferences \citep{griffiths2009theory,griffiths2005causes,lake2015human}.  Such domain knowledge is modeled as a hierarchical prior, affecting people's expectations about what kinds of structure and parameters are plausible in different domains and contexts.  For example, basic medical knowledge will tell you that diseases normally cause symptoms rather than the reverse and that medicines can take on the order of hours to work.  Basic knowledge of electronics will tell you to expect causal influences to propagate at the sub-second level rather than across the eons.  Thus, while the current frameworks show how causal beliefs can arise without much specific domain knowledge they are also readily compatible with other factors and knowledge playing into the choice of sensible priors.  Another way of thinking of specific domain knowledge is as rich functional forms governing the interactions between variables.  For example, \cite{bramley2018physics} explore interventional learning about the masses and forces relating objects in simulated physical microworlds.  Here learners' sophisticated understanding of how objects with different properties normally interact clearly plays into both the judgments they make and the actions they take in service of learning.  Interventional behaviors in this domain start to take on recognizable physical form such as shaking and throwing of objects and one another, but still subscribe to the principles exposed here of creating strong interventional signals that propagate through causal relationships.  Learners appear to be able to compare the resulting trajectories in time against expectations simulated from their intuitive representation of physics and so back out the relevant properties.  In these domains, our interventions and our interpretations of them become deeply theory laden, depending profoundly on our beliefs about how the domain works in general \citep{bramley2017neurath}.



%    % \item Role of theory: Discretization and tracking of summary statistics/limited measurements (just how long for the ball to drop, strength of headache after fixed delay, etc) can be derived from the richer understanding of the problem
    % \item The minimal criteria for causation, that $X_1$ affects $X_2$ at some future time is very weak.  We cannot try all possible time lags and effect may fade in, fade out, reverse (e.g. effect of coffee on arousal: first there is no effect, then you feel more awake, then you feel more sleepy, then there is no effect anymore).  Domain knowledge lets us bootstrap expectations to focus on a good time window etc. \ntodo{Fit in the Buehner etc type expected delay results here briefly + certeris paribus shorter delays more likely to be causal as a domain general prior?}
    %\item Abstraction, discretization \& aggregation (i.e. to tabular, CBN-ready data) becomes possible only once we have robust knowledge of when and how to measure effects (i.e. for electrical circuits CBN appropriate because effects are virtually synchronous, other domains we must use our theory to determine the right time windows and sufficient measurements).
    %     \item To extent our representations are rich in functional form, i.e. OU process, and intuitive physics, our interpretation of interventional evidence is richer too \citep{bramley2017physics}.  E.g. with physics we can compare the trajectories in time against expectations simulated from our theory

    % \item Relate this theory-ladenness of learning to Neurath's ship
 %\end{enumerate}

\subsection{When to aggregate}
    
This is not to say that there is no place for the CBN in causal cognition.  %Causal beliefs can be seen as a framework on which to hang evidence.
The richer representations explored here are useful for navigating causality at the real time scale.  But many other phenomena of interest are too long range and noisy to be identified through close focus on the minutia of change over time, only becoming evident as data is aggregated to a larger scale.  Time sensitive representations allow us to make the leap to this higher level of aggregation.  Once we have a robust knowledge of when and where to measure effects we can start gather evidence on a larger scale where time is increasingly abstracted away and aggregation over instances is possible.  For example, reasoning at the level of continuous OU variables might be important for someone to play tennis. But if they then want to decide whether to follow a particular strategy it will be more useful to keep track, over multiple games, of which strategy they pursued and whether it resulted in a win.  Essentially, we can use our more detailed and time sensitive theories to determine the relevant time windows and sufficient measurements for studying the subtler and longer scale causal relationships we are interested in establishing.  Indeed, it is only through this large scale organized process, supported by our expertise in the fine grained mechanisms, that humanity has been able to discover the weak relationships between behaviors and medical problems decades later, such as the link between smoking and cancer \citep{gandini2008tobacco}, or the lack of one between vaccinations and autism \citep{verschuur1996hidden}.  In general then, the best interventional strategy for a learner depends on what they are interested in learning, how much they already know about the domain, as well as the granularity of the available measurements.

%\ntodo{To squeeze in: They thus, often appear to form something more like a successor representation \citep{dayan1993improving,momennejad2017successor}, in both direct and indirect effects are collapsed together.}

\section{Conclusions}

In sum, recent work has begun to shed light on the temporal sophistication of everyday causal cognition.  We are aware, as was Heraclitus, that we ``cannot step into the same river twice''\citep{barnes2013presocratics}.  Thus, we must learn to integrate our model-based expectations about how things will play out, with careful interrogatory actions, to step many times into different but related rivers, and so learn and exploit the dynamic causal world in a single lifetime.

\clearpage
\bibliographystyle{apacite}
\bibliography{refs}
\end{document}