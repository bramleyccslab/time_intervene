\documentclass{cambridge7A}%[multi]
\usepackage[natbibapa]{apacite}
\usepackage{xcolor}
\usepackage{tikz} %used for drawing colored boxes 
\usepackage{todonotes}
\usepackage{soul}
\usepackage{rotating}
\usepackage{floatpag}
\rotfloatpagestyle{empty}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{multind}\ProvidesPackage{multind}
\usepackage{amsmath}
\usepackage{amssymb}

%\usepackage[margin=1in]{geometry}

% \usepackage{tocloft}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{gensymb} %for symbols such as the degree sign
% \usepackage{titlesec}
% \usepackage{array}% http://ctan.org/pkg/array

% \usepackage{bibentry} %for full citations
% \usepackage{subfig} %for creating panels
% \usepackage{booktabs}% http://ctan.org/pkg/booktabs
% \usepackage{ctable}% http://ctan.org/pkg/booktabs
% \usepackage[countmax]{subfloat} %for creating panels
% \usepackage{enumitem} %better environment for lists 
% \usepackage{multirow} %to have multiple row entries in tables 
% \usepackage{breakurl} %to break urls
% \usepackage{wrapfig} %to wrap figures
% \usepackage{float}  %for floating figures

% \usepackage{hyperref} %to have links within the document

% \hypersetup{
%     colorlinks,%
%     citecolor=black,%
%     filecolor=black,%
%     linkcolor=black,%
%     urlcolor=black
% }

\graphicspath{
{figures/}
{../figures/}
}


%Nice to do notes
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}
\newcommand{\sntodo}[2][]{\todo[caption={\textbf{NB}}, size=\footnotesize, color = aliceblue, #1]{#2}~}
\newcommand{\ntodo}[2][]{\vspace{0.1cm} \hfil \todo[caption={\textbf{NB}}, size=\footnotesize, color = aliceblue, inline, #1]{#2}}
\newcommand{\sttodo}[2][]{\todo[caption={\textbf{TG}}, size=\footnotesize, color = orange, #1]{#2}~}
\newcommand{\ttodo}[2][]{\vspace{0.1cm} \hfil \todo[caption={\textbf{TG}}, size=\footnotesize, color = orange, inline, #1]{#2}}

% %For possessive citing
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}

% %Operators %TODO do we use all these?
\DeclareMathOperator*{\Do}{do}
\DeclareMathOperator*{\pa}{ca} %Parents

\newcommand{\ww}{\mathbf{w}} %Parameter
\newcommand{\cald}{\mathcal{D}} %Set of data
\newcommand{\cali}{\mathcal{A}} %Set of interventions
\newcommand{\cals}{S} %Set of models
\newcommand{\E}{\mathbb{E}} %Set of models

\newcommand{\cm}{a} %Single intervention
\newcommand{\dm}{d} %datum

\newcommand{\ca}{\mathbf{a}} %Multiple interventions
\newcommand{\da}{\mathbf{d}} %Multiple trials of data

% \DeclareMathOperator*{\zz}{\mathbf{z}} %Actual causation
% \DeclareMathOperator*{\zzz}{\mathbf{Z}} %Actual causation

%\renewcommand{\baselinestretch}{2.0}

\begin{document}

% \alphafootnotes
\chapterauthor{Neil R. Bramley (NYU), Tobias Gerstenberg (MIT), Ralf Mayrhofer (G{\"o}ttingen), and David A. Lagnado (UCL)}
\chapter{Unifying Intervention and Time in Causal Learning}

% \footnotetext[1]{NYU}
% \footnotetext[2]{Supported by NSF Grant 43645.}
% \arabicfootnotes

\abstract{Much of what we know about the world comes from acting on it, and observing the consequences of our actions. In the literature, causal learning from interventions and from observing temporal dynamics have largely received separate attention due to the different datasets they are usually applied to.  However, we argue that in human cognition, interventions and temporal dynamics are inseparable.  We trace how causal inference tools developed in data science have been applied to understanding human causal learning and reasoning, highlight the current shortcomings of both intervention-based and time-based approaches taken separately, and describe recent work that starts to bring the two together.  We end by sketching an account of interventional and temporal evidence as constituents of a unified online causal learning process.
\ttodo{not sure about the title of the paper; it sounds a little strange to unify time and intervention since these are conceptually different things}
}

% \abstract{Both interventions --- actions that manipulate the world --- and temporal information have received considerable attention in psychology as cues for learning causal structure.  Inverventional learning has been studied within the frameworks of causal Bayesian networks and optimal experimental design.  We argue that, while a useful starting point, these frameworks do not provide an adequate account of human active causal learning.  We propose a more nuanced perspective:  Interventions inject signals into causal systems.  Evidence about the structure of those systems then comes via detecting the propagation of these signals.}

\section{Introduction}

Uncovering and describing the deep causal structure of reality is a fundamental goal of science, but it is also at the heart of human cognition.  We are born into a ``blooming, buzzing confusion'' \citep{james1890principles} \ttodo{need to add page number of direct quote}of sensory information, and spend much of our development building up a causal model of reality that is both rich enough and accurate enough to guide us in pursuing our changing goals.   On this view, science plays a supporting role in causal cognition: extending the domain of causal understanding beyond what can be inferred through everyday experience; integrating evidence at scales beyond the capabilities of the brain; and so helping resolve disagreements about hypothesized causal connections.  However, attempts to understand the psychology of causal leaning and reasoning have recently flowed in the other direction. \ttodo{wasn't obvious to me what was meant by `other direction' here} Bayesian networks have become a ubiquitous tool for modeling both non-causal and causal inference in large data sets across the sciences \citep{pearl1988probabilistic}.  Among other things, they provide a convenient calculus for learning from, and reasoning about, \emph{interventions} --- actions or experiments that manipulate variables in the world \citep{pearl2000causality,woodward2003making}. \ttodo{what are `variables in the world'? they exist in a graph but not really in the world} As a result they have also been applied to the task of modeling human active causal learning and reasoning.  Separately, a plethora of statistical methods are used for drawing causal inferences from time series data \citep{friston2014granger,granger2004time} and a separate line of research explores the interplay between time, causal beliefs, and predictive perception \citep[e.g.,][]{buehner2006temporal,bechlivanidis2016time}.% and predictive processing accounts suggest that cognition is often engaged in real time causal prediction \citep{clark2013whatever}.\sntodo{Would be good to revisit predictive brain in a sentence in section 1.2.}%real time causal prediction.

In this chapter, we contend that the division in the scientific literature between intervention-driven and time-driven modes of causal inference is an artifact of the kinds of datasets that scientists have to work with.  Experimental datasets are typically aggregated over many independent samples from a population in a setting where temporal dynamics are unavailable or unmeasured, while nonexperimental datasets often track multiple factors across time.  We argue that there is no such data distinction in human experience --- we experience the world as a single ongoing event stream and are constantly choosing how and when to next intervene. \ttodo{this might be a little strong -- it's probably true about the experience of directly interacting with the world, but we also learn from data that takes more abstract forms; so the statement is probably only true about learning before cultural artifacts (also ignoring social learning of course)} We must be sensitive what goes on before, during, and after our actions if we want them to be effective or informative.  As such, we argue that we should move toward modelling \ttodo{make sure to use american or british spelling consistently}human causal learning as an inherently online process, involving the continuous integration of both temporal and interventional information.  We illustrate some shortcomings of treating interventions and time as separate cues through reference to past research, and then highlight new approaches and data that pave the way for a new conception of the role of intervention and time in human causal learning. \ttodo{we don't want to talke prior research down; rather spin it positively: how we are building on it}

\ttodo{maybe we should have a little more detailed roadmap for the rest of the paper here?}

\section{The Causal Bayesian network account of learning from interventions}\label{section:CBN}

A core challenge for causal inference both in science and in everyday human learning is distinguishing genuine causal relationships from spurious or coincidental ones.  Many things in the world are statistically associated --- sun exposure is associated with melanin production; smoking is associated with lung cancer;  %the number of clicks an advert gets online is related to its position on a website, 
%police numbers are related to crime levels;
ice cream sales are associated with deaths by drowning; and homeopathic remedies are often associated with positive health outcomes.  Some of these associations are due to direct causal relationships --- sun exposure really causes skin to tan and smoking really causes cancer. %and ad position really does matter.  
But, in many others cases, the relationship is due to shared causal factors --- people are both more likely to eat ice cream and go swimming when the weather is warm, %police numbers and crime levels might both be effects of poverty or other aspects of governance rather, 
and people often feel better after taking a medicine they believe works, regardless of whether it is actually effective \citep{di2001influence}.  Intervention is a way of assessing whether an association is really causal.  Interventions are actions that perturb the world and so reveal its structure in ways that are unavailable from mere observation \citep{woodward2003making}.  In science we call these experiments, and plan them carefully, systematically manipulating things on a large enough scale to resolve causal questions of interest in the face of irreducible noise.  When we manipulate one variable we no longer have to worry that a resulting association between that variable and another is due to a shared cause.  For instance, an experiment systematically exposing some participants to sunlight and others to darkness, measuring their skin tones before and after, will reveal a positive relationship between the intervened-on factor (sun exposure) and its putative effect (skin tone).  %Assigning different policing policies in different regions and comparing their before and after crime statistics reveals that additional police do lower 
In contrast, systematically giving some people homeopathic remedies and others sugar pills without telling them which, normally removes any relationship between treatment (whether the pill is a homeopathic remedy) and its putative effect (health outcomes), revealing that it is the belief in the efficacy of the drug rather than the drug itself that causes the association.  
\ttodo{i'd remove the last paragraph about homeopathy}

Interventions in human experience are more diverse and ubiquitous than those practiced in science.  Our every action affects our proximal world in some way, from small movements that affect our pose or field of view, to the extended actions through which we interact with other objects and one another.  Sometimes we act with the goal of resolving some particular causal uncertainty (``What does this button do?'', ``What does this taste like?'',``What will happen if I shout loudly in the library?''), other times we act primarily to pursue our goals (turning on the PC, feeding ourselves, getting someone's attention).  Causal Bayesian networks \citep{pearl2000causality} provide a framework for drawing inferences based on both observations and interventions. 
% In both science and causal cognition, intervention choice (experimental design) and  causal inference have been modeled within the framework of causal Bayesian networks \citep{pearl2000causality}.\footnote{Structural equation models can be interpreted in a similar way but we restrict our discussion to Causal Bayesian networks for simplicity.}  
We now briefly describe this framework and how it has been applied to the study of causal cognition.

%Optimal experimental design \citep{fedorov1972theory}, provides mathematical answers to what interventions best resolve particular forms uncertainty.  

\subsection{The causal Bayesian network framework}

Causal Bayesian networks \citep[hereafter ``CBNs'', ][]{pearl2000causality} capture aggregate patterns of covariation between variables in terms of probabilistic causal dependencies. \ttodo{first sentence is most likely confusing to a less informed reader -- maybe just desribe what they are first, before saying what they do}Nodes represent variables (i.e. the component parts of a causal system); arrows represent causal connections; and parameters encode the combined influence of parents (the source of an arrow) on children (the arrow's target, see Figure~\ref{fig:CBN}a for an example).  CBNs can represent discrete or continuous valued variables and the functional dependence between the state of an effect on the states of its causes can take arbitrary form.  However, the majority of psychology research has focused on  systems of binary $\{0=\mathrm{absent},1=\mathrm{present}\}$ variables and has commonly assumed a simple parameterization for both generative and preventative causal relationships that we describe in more detail below \citep{cheng1997from}.  %In this setting, each variable $X_i\in\mathbf{X}$ has some basic probability $w_{0i}$ of occurrence encapsulating the influence of any causes exogenous to the model.  Generative connections \emph{increase} the probability that the effect is present given the cause is present via a noisy-OR combination of baserate $w_{0i}$ and the \emph{power} or strength $w_{ji}$ of any of its causes $X_j$ that are present, giving 
%\begin{equation}
%P(x_i=1|\pa(x_i),\ww_s)=1-(1-w_{0i})\prod_{j\in \pa(X_i)}(1-w_{ji})
%\label{noisyor} 
%\end{equation}
%where $\pa_s(X_i)$ denotes the causes of $X_i$ in causal structure $s$ and $\ww_s$ denotes the collection of $w_{ji}$ parameters in $s$, where $i\in1\ldots N, j\in0\ldots N$. Preventative causal connections decrease the probability that the effect is present via a noisy-AND-NOT combination with the baserate\footnote{For a mixture of generative and preventative influences on a single variable, one must also model their order of influence.  We do not do this here but see \ref{stephan2018preemption}.\ntodo{Has anyone ever modelled this case before?}}, giving
%\begin{equation}
%P(X_i=1|\pa(X_i),\ww_s)=w_{0i}\prod_{j\in \pa(X_i)}(1-w_{ji}).
%\label{noisyandnot} 
%\end{equation}

Figure \ref{fig:CBN}a depicts a causal network relating three binary variables $X_1$, $X_2$ and $X_3$, with one generative $X_1\!\stackrel{+}\rightarrow\!X_2$ connection and one preventative $X_2\!\stackrel{-}\rightarrow\!X_3$ connection.  Under this model, $X_1$, $X_2$ and $X_3$ all occur with their own ``base rate'' probability (i.e. due to causes exogneous to the model).  However the probability that $X_2$ and $X_3$ occur also depend on the state of their causes, such that  $P(X_2=1)$ is higher than baseline when $X_1$ is present while $P(X_3=1)$ is lower than baseline when $X_2$ is present.

%$P(X_1=1)$ is always equal to its base rate $w_{01}$ because it has no causes.  However, the probability of $X_2$ depends on the state of $X_1$ and $X_3$ depends on the state of $X_2$.  If $X_1=0$, $P(X_2=1) = w_{02}$ but if $X_1=1$ it is higher (i.e., $1-(1-w_{02})(1-w_{12})$).  If $X_2=0$, $P(X_3=1) = w_{03}$ but if $X_2=1$, it is lower (i.e., $w_{03}(1-w_{23})$).   
Figure~\ref{fig:CBN}b depicts some possible observations $\da=\{\dm_1\ldots \dm_8\}$ produced by the causal system in Figure~\ref{fig:CBN}a. %in which different combinations of $X_1\ldots X_3$ are observed to be present or absent on what we assume are independent observations.    
Any parameterized causal model $s$ over variables $\mathbf{X} =\{X_1 \ldots X_N\}$ assigns a probability to each datum ${\dm}=\{X_1=x_1\ldots X_N=x_n\}$, meaning that Bayesian inference can be used to assess which of a set of potential CBNs (which $s\in\mathcal{S}$) does the best job of accounting for data.  This will be whichever model(s) capture the pattern of statistical (in)dependencies between the variables with the minimum number of connections.  %conditioning on known parameters $\ww_s$ or integrating over them if they are unknown.% corresponding to a true network in which $w_{01}=, w_{02}=, w_{03}=, w_{12}=$, and $w_{23}=$.

Bayesian networks embody the ambiguity described above, about how observed correlations relate to causality.  Without causal insight, we might construe the dependence between $X_1$ and $X_2$ and between $X_2$ and $X_3$ in several different ways.  One is as depicted ($X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$), but the reverse ($X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\leftarrow\!X_3$) is also possible, as is the case where where both $X_1$ and $X_3$ depend on $X_2$ (i.e., $X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$).  In each case, the best fitting parameters $\ww_s$ differ, but the marginal and conditional probability structure is 
% still captured 
preserved and the overall goodness of fit to the data will be the same.  This property of Bayesian networks is known as the Markov condition \citep{pearl2000causality}.

Adding the notion of an intervention breaks this deadlock.  Metaphysically, an intervention in a CBN is conceived as the learner first setting the values of some of the variables, and then allowing the causal system to propagate this through the rest of the network. \ttodo{maybe state this more generically -- it's not necessarily about a `learner'} This is analogous to reaching into the causal system, changing things within it, then observing what happened as a result.  We model interventions in a CBN by fixing ``intervened on'' variables to their chosen values and disconnecting them from their normal causes, using Pearl's $\Do[.]$ operator \citep{pearl2000causality} to denote what is fixed on a given test.  Figure~\ref{fig:CBN}c gives an example in which $X_2$ is intervened on and set to 1 (i.e., $\Do[X_2=1]$).  If the model is correct, we should expect $X_1$ to be present with its base rate probability, while $X_3$ should be subject to the preventative influence of $X_2$ (i.e. it should be less likely to occur than if $X_2$ had been set to 0).  Figure~\ref{fig:CBN}d gives examples of interventional evidence, under which various subsets of the variables are set through intervention either to 1 or 0 with the remaining variable states generated by running the causal system.  
\ttodo{for figure d) it might be more straightforward to just show interventional data for the intervention shown in c); that's at least the default way in which a reader is likely to initially interpret the figure (since that pattern holds for a) and b)}

%The space of all possible interventions $\cali$ is made up of all possible combinations of fixed and unfixed variables, and for each intervention $\cm$ the possible data $\cald_\cm$ is made up of all combinations of absent/present on the unfixed variables.  
%Incorporating an intervention $\cm$, the probability of datum $\dm$, is just the product of the probability of each variable that was not intervened upon, given the states of its parents in the model
%\begin{equation} P({\dm}|s, \ww;\cm) = \prod\nolimits_{X_i\in (\mathcal{X}\notin \cm)}
 % P(X_i|\{{\dm},\cm\}_{\pa(X_i)},\ww). \label{likelihood}
%where $\{{\dm},\cm\}_{\pa(X_i)}$ indicates that those parents might either be observed (part of $\dm$) or fixed by the intervention (part of $\cm$).\sntodo{I know this is all still too fussy! but I'm adapting it from psych rev paper, so at least its comprehensive.  What can we cut?}

% For instance, $\Do[X_1\!=\!1,X_2\!=\!0]$ means a variable $X_1$ has been fixed ``on'' and variable $X_2$ has been fixed ``off'', with all other variables free to vary.
%, propagating information from the variables that are fixed through intervention $\ci$, to the others. The space of all possible interventions $\cali$ is made up of all possible combinations of fixed and unfixed variables, and for each intervention $\ci$ the possible data $\cald_$ is made up of all combinations of absent/present on the unfixed variables.  
  %Interventions allow a learner to override the normal flow of causal influence in a system, initiating activity at some components and blocking potential influences between others.  This means they can provide information about the presence and direction of influences between variables that is typically unavailable from purely observational data \citep[see][ for a more detailed introduction]{pearl2000causality,bramley2015fcs}, without additional cues such as temporal information \citep{bramley2014order}. For instance, in Figure~\ref{fig:cbn}b, we fix $y$ to 1 and leave $x$ and $z$ free ($\ci=\Do[y\!=\!1]$).  Under the $x\rightarrow y\rightarrow z$ model we would then expect $x$ to activate with probability $w_B$ and $z$ with a probability of $1-(1-w_B)(1-w_S)$.

% Working within a space of possible models (e.g. the space of possible causal Bayesian networks) allows quantification of the evidence that observations and interventions lend to particular causal theories.\footnote{Aside from explicitly Bayesian inference --- working backwards from data to determine which of a class of generative models is most likely to have produced it --- many practical frequentist statistical techniques like mediation analysis \citep{baron1986moderator} and granger causality \citep{granger1969investigating} provide ways of testing for causality without being tied to a particular generative model.}  Thus, a natural strategy for studying causal learning and reasoning in human cognition has been assume a class of causal models that has proved successful in science, is roughly how humans represent causality.  Then try to make sense of human learning and judgment as operating within that space.\ntodo{Make clearer}  For last 20-30 years, the dominant framework for studying human causal cognition has been the CBN \citep{pearl2000causality}.

% \subsubsection{Structure inference}

% We can apply Bayesian inference to the problem of identifying the true causal structure from observational or interventional data.  Considering true model to be a random variable $S$, our prior belief $P(S)$ is then an assignment of probabilities, adding up to 1 across possible models $s\in S$. %in the set of models $\mathcal{S}$.  
% When we observe some data $\da=\{{\dm}_i\}$, associated with interventions $\ca=\{\cm_i\}$, we can update these beliefs with Bayes theorem by multiplying our prior by the probability of the observed data under each model, integrating over possible settings of the parameters $p(\ww_s)$, and dividing by the weighted average probability of those data across all the possible models:
% \begin{equation}
% P(s|\da;\ca)=\frac{\int_{\ww}P(\da|s,\ww;\ca)p(\ww)P(s)\ \mathrm{d}\ww}{\sum_{s'
%      \in S}\int_{\ww}P(\da|s',\ww;\ca)p(\ww)P(s')\ 
%    \mathrm{d}\ww}
% \end{equation}

% Crucially, the data must be independent and identically distributed, so that 
% $P(\da|m,\ww;\ca)=\prod_i P({\dm}_i|m,\ww;\cm_i)$.


\begin{figure}[t]
   \centering
   \includegraphics[width = .7\columnwidth]{CBN}
   \caption{a) A causal Bayesian network containing a generative connection (``+'' symbol) and a preventative connection (``-'' symbol) parameterized with base rates $w_{0i}$ and causal strengths $w_{ji}$.  b) Example observational data.  Yellow shading indicates a variable was present (i.e., took the value 1) while indicates a variable was absent (i.e., took the value 0) c) An intervention $\Do[X_2 = 1]$, disconnecting $X_2$ from $X_1$ and any background factors. d) Example interventional evidence.%\ntodo{I have the noisy OR probabilities $1-(1-w_{02})(1-w_{12})$ etc written out, could add them in.}
   }
   \label{fig:CBN}
\end{figure}

\subsection{Modeling intervention choice}

Different interventions yield different outcomes, which in turn have different probabilities under different models.  This means that which interventions are valuable for identifying the true structure depends strongly on the hypothesis space and prior.  For instance fixing $X_2$ to 1 ($\Do[X_2\!=\!1]$) is (probabilistically) diagnostic if you are primarily unsure whether $X_2$ causes $X_3$ because $P(X_3|\Do[X_2\!=\!1])$ differs depending whether $X_2$ causes $X_3$.  However, it is not diagnostic if you are primarily unsure whether $X_1$ causes $X_2$ because $X_2$ will take the value 1 irrespective of the causes of $X_2$.  Optimal experimental design theory allows us to reason about the expected value of different interventions relative to a notion of uncertainty \citep{fedorov1972theory,raiffa1974applied}.  For instance, we can define the value of an intervention as the expected reduction in uncertainty about the true model after seeing what happened. This expectation can be calculated by averaging, prospectively, over the different possible outcomes of each potential intervention %${\dm}^\prime \in \da_{\cm}$ where $\da_{\cm}$ is the space of possible outcomes of intervention $\cm$ weighted by their marginal likelihoods under the prior.  When performing interventions one after another, as in $\cm^1,\ldots,\cm^t$, we take $P(S|\dm_{t-1},\ww;\cm_{t-1})$ as our prior each time. The most valuable intervention $\cm_t$ at a given time point is then
%\begin{equation}
%\arg\max_{\cm\in \cali}\E_{{\dm}^{\prime} \in \da_{\cm}}
%\left[\Delta \mathrm{H}(S|{\dm}^{\prime}, \da^{t-1},\ww;\ca^{t-1},\cm)\right] ,
%\label{eq:info_gain}
%\end{equation}
%where $\E[.]_{{\da}^\prime \in \mathcal{D}_{C}}$ denotes the average over outcomes ${\da}'$ and $\Delta \mathrm{H}(.)$ denotes
and computing the
reduction in uncertainty in each case \citep[e.g.,][]{shannon1951prediction}. See \cite{bramley2017neurath} for a detailed mathematical account of this.%Shannon is just one of a broad family of possible entropy measures \citep{nielsen2011closed}.  However, it is one that has proved at least as long-run successful as a number of variants when applied as a greedy strategy for choosing interventions \citep{bramley2014should} or asking binary questions \citep{nelson2005finding}.

A key property illustrated by Figure~\ref{fig:CBN} is that well-chosen interventions can be much more informative, on average, than mere observations of the causal system.  The depicted observational data (Figure~\ref{fig:CBN}b) is much less informative than the interventional data (Figure~\ref{fig:CBN}d) with respect to the set $\mathcal{S}$ of all possible causal Bayesian networks relating the three variables.  We demonstrate this in the Figure \ttodo{which figure?} by computing the posterior Shannon entropy $\mathrm{H}(P(\mathcal{S}|\da;\ca))$ after each observation or intervention, based on an initially uniform prior over structure hypotheses $P(\mathcal{S})$ and model parameters $p(\ww_s)$, including these calculations in the Appendix.\ntodo{If we think we need this...?  There are lots of mathematical details commented out here that could be repatriated to an appendix.
\ttodo{personally, i don't think we should have an appendix. i'd say we just don't want any mathematical details -- it's fine to just refer to some of your work here like you've done before}
}  This is partly because the observational data does not distinguish between models with the same dependency structure
% (i.e. between $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$, $X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\leftarrow\!X_3$, and $X_1\!\stackrel{+}\leftarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$) 
\ttodo{commented out these structures as you had mentioned them before}
, but also because a well-selected sequence of interventions can systematically target and resolve residual sources of uncertainty, for instance by testing a particular causal connection that previous tests have not provided clear evidence about.

\ttodo{this section feels like it's part of causal cognition already (since it's about a learner trying to minimize uncertainty -- so the next subheading is a little surprising}

\subsection{Interventions in causal cognition}

%CBNs have been widely adopted by psychologists interested in how people learn and reason about causality \citep[e.g.][]{lagnado2002learning,lagnado2004advantage,lagnado2006time,lagnado2007cues,sloman2005do,sloman2005causal,waldmann2006beyond,griffiths2009theory,gopnik2004maps,mayrhofer2011heuristics,rehder2003causal,coenen2015strategies,steyvers2003intervention,rehder2001causal, oppenheimer2013categorization, kemp2009structured,oppenheimer2004spontaneous, lee2008role, meder2014structure}.  %They have also entered into philosophical debate, providing an interventionist perspective on how one might formally ground causal claims \citep[e.g.][]{woodward2011mechanisms,danks2014unifying}.

%The CBN approach found its first success in psychology in making sense of everyday examples of reasoning that seem inherently causal yet are hard to capture under a traditional associative account of human learning and representation \citep{holyoak2011causal}.   In particular, the CBN framework captures the ways in which people's judgments exhibit ``explaining away'' --- where multiple potential causes of a common effect compete as explanations.  For example, suppose you learn that someone suffers from tinnitus.  Other things being equal, this might increase your belief that they play in a rock band.  However, if you now learn that they work in an extremely noisy factory, your suspicion that they play in a rock band will be reduced.  %CBNs also capture another phenomenon called ``screening off'', in which the information that one variable provides about another is reduced to the extent that one already knows the values of any causally intervening variables.  This is like saying that, if you already know someone is in a rock band, learning that they also like rock music will have less influence on your expectation that they have tinnitus, removing it completely if you believe the association between liking rock music and having tinnitus is entirely mediated by playing in a rock band.\ntodo{Too offtopic?}

%CBNs have also helped make sense of people's judgments of causal strength from covariation information.  Before the development of the CBN, there was a long-running debate about how people go from contingency information (i.e. patterns of cases in which variables take different as depicted in the observational evidence in \ref{fig:CBN}b) to judgments of the strength of a causal relationship.  An early proposal was that causal strength judgments reflect the change in probability of an effect given a cause --- known as $\Delta P$ \citep{allan1980note,jenkins1965judgment,lopez1998rational}.  After finding systematic judgments that $\Delta P$ could not capture, \cite{cheng1997from} proposed that causal strength estimates were better understood through the concept of \emph{causal power}, which captures the probability that a cause brought about its effect, after accounting for the chance it was caused by other, background,  factors.  \cite{griffiths2005structure} found that putative strength judgments across a number of experiments could often be better explained by the idea that learners are actually making \emph{structure judgments} within the CBN framework.  So, rather than assuming a relationship and judging its strength, participants' estimates reflected the rational judgments of the probability of a causal relationship given the evidence they had been shown.  \emph{Causal power} turns out to embody a natural way of parameterizing CBNs on binary variables, known as noisy-OR for generative causal connections and noisy-AND-NOT for preventative connections.\footnote{For a mixture of generative and preventative influences on a single variable, one must also model their order of influence.  We do not do this here but see \cite{stephan2017preemption}}  Noisy-OR considers the probability of an effect to be the probability that at least one of its causes was effective while noisy-AND-NOT captures preventative causation as the probability that the effect as caused by exogenous factors and that none of its endogenous causes was effective at blocking it.   Subsequent work on causal cognition has frequently assumed both the CBN framework and the noisy-OR/noisy-AND-NOT parametrization \citep[e.g.][]{yeung2015identifying,lu2008bayesian,lagnado2006time,coenen2015strategies}.  The example in Figure~\ref{fig:CBN}b and d is, in fact, a noisy-OR parameterized causal Bayesian network, in which variables activate by chance with probability 0.3, and both the generative and preventative causal connections have a strength of 0.8.\ntodo{Too offtopic?  Could lose some or all of prev two paragraphs on CBNs early successes (they're stolen from my thesis) since story is more about the interventions...}

CBNs were initially adopted in psychology because of their ability to account for qualitative patterns of human judgments that are hard to capture under simple associative accounts of learning \citep[e.g.,][]{rescorla1972theory}.  For example, CBNs capture ``explaining away'' effects where the causal roles of variables lead to asymmetries in judgments of their associative strength --- competing causes share associative strength while effects do not \citep{holyoak2011causal}. \ttodo{i don't think the `explaining away' will be understood as currently stated} They also capture ``blocking'' effects in which learning the state of a mediator or common-cause overrides inference from one of the distal variables to the other. \ttodo{this sentence is also a little too abstract -- maybe start with the concrete example and then mention the more general principle; also ``blocking'' is only informative for someone who already knows the associative learning terminology -- many philosophers won't}  For example, in Figure~\ref{fig:CBN}, knowing that $X_1=1$ only tells you something about $P(X_3)$ (i.e., that it is less likely) if you do not already know the value of $X_2$.  More generally, CBNs have shed light on how people go from from contingency information to judgments of causal strength through the idea that people interpret evidence through the lens of their favored causal model, meaning background factors and surrounding causes affect the interpretation of evidence \citep{cheng1997from,waldmann2000competition,griffiths2005structure}.  \ttodo{last sentens is too long and has to manny ``through''s; i would shorten this previous paragraph}Pearl's ``Do'' calculus has also proven to be an effective way of capturing the different inferences people make from observations and interventions \citep{sloman2005do,waldmann2005seeing}, %Imagine three pool balls $X_1$,$X_2$, and $X_3$ in a line such that the motion of $X_1$ causes $X_2$ to move and $X_2$ causes $X_3$ to move.  If someone blocks $X_2$, you will not expect $X_3$ to move but will still expect $X_1$ to move, however if you observe that $X_2$ does not move, then you will also infer that $X_1$ did not move either.  
and how they reason about counterfactuals \citep{lagnado2013causal,rips2013inference,rips2010two}. \ttodo{you might need to say a little more about what counterfactuals are if you mention it here} The broad idea is that observations license backward inferences while interventions do not.  Observing that your officemate arrives at the office soaked suggests may be raining outside, while if you had intervened and poured a bucket of water on them, their wet clothes would not tell you anything about the weather.  %Figure~\ref{fig:lit_evidence}c gives an example of an inference based on an observation that $X_2=1$ (e.g. someone is in a rock band) and Figure~\ref{fig:lit_evidence}d for inference from an intervention  (e.g. making someone join a rock band) which we write as $\Do[X_1=1]$.  The greyed arrow and scissors symbol between $X_1$ and $X_2$ indicates that the intervention overrides the normal causal relationship.  In both cases the probability of $X_2$'s direct and indirect descendants are affected as can be seen by their higher probabilities (green shading) relative to their marginal probabilities shown in Figure~\ref{fig:lit_pgm}a and b. However, only the observation affects the probability of $X_2$'s normal cause $X_1$ (e.g. liking rock music), or a knock-on effect on $X_1$'s other effect(s) (e.g. $X_9$ owning many rock records).   
The notion that people can imagine virtual interventions helps explain important aspects of thinking.  For example, counterfactual ``what if...'' inferences are often consistent with the idea that people imagine an intervention that makes the counterfactual true with minimal revision of its causal history \citep{gerstenberg2013back,rips2010two,lagnado2013causal}. 
\ttodo{this section needs a bit of work i think -- maybe best to just start off with an intuitive example that illustrates the difference between intervention and observation; say that this can be captured in a CBN but not within an associative framework, and then say something about interventions for hypothetical planning, and for explanations (via counterfactuals)}

%CONTINUE HERE

%Intervening is a form of \emph{active learning}.  Active learning is the study of situations in which learners exert control over the evidence they see \citep{settles2012active,gureckis2012self}.  
The importance of interventional learning is well established in education, and developmental psychology, where self-directed ``play'' is seen as vital to healthy development \citep[e.g.][]{piaget1930child, bruner1976play}.  Accordingly, a number of developmental psychologists have adopted a ``child as scientist'' analogy, which views children as fundamentally engaged in causal hypothesis testing within the CBN framework \citep{gopnik2000detecting,gopnik2004maps,sobel2006importance}.  In adults, a number of studies have found that people benefit from the ability to perform (or watch others performing) interventions during causal learning \citep{lagnado2002learning, lagnado2006time, lagnado2004advantage, schulz2001do, sobel2006importance}.

Several recent studies have also explored \emph{how} people select what interventions to perform, comparing adults' and children's choices against the dictates of optimal intervention selection,  considering constrained and heuristic variants of this \citep{bramley2015fcs,bramley2017neurath,mccormack2016children,coenen2015strategies,steyvers2003intervention}.  \cite{mccormack2016children} had children learn about the deterministic causal relationships between three spinning wheels sticking out of a black box toy.  They were allowed to optionally insert a ``Stop'' sign to block one of the wheels from turning (equivalent to fixing that variable to 0 i.e., \emph{absent}), and would also choose a wheel to spin, so initiating activity in the system (equivalent to fixing a variable to 1, i.e., \emph{present}) and would then see which non-fixed wheel(s) spun as a result.  The children then selected which of several causal structure pictures described the internal mechanism.  \cite{mccormack2016children} found that the quality of children's intervention choices improved with age and was a determinant of judgment accuracy.  However, children of all ages also had a strong tendency to intervene by turning on $X_1$ which was the root component of all three possible structures.  This intervention was completely uninformative because it made all three variables spin whatever the structure (the possible mechanisms were $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$, $X_1\!\stackrel{+}\rightarrow\!X_3\!\stackrel{+}\rightarrow\!X_2$, or $X_2\!\stackrel{+}\leftarrow\!X_1\!\stackrel{+}\rightarrow\!X_3$).

\cite{coenen2015strategies}, explored adults interventional learning --- learners had to establish which of two wiring diagrams described a circuit boards by activating one of the components and seeing which others activated.  They found that learners sometimes chose the most informative intervention, but as in \cite{mccormack2016children}, also frequently intervened on the component that made the most other components activate even when this was uninformative, doing so more often when under time pressure.

\cite{bramley2015fcs} and \cite{bramley2017neurath} explored adults' interventional causal learning in a series of tasks in which the target causal systems of interest were noisy CBNs with nodes visualized simply as abstract colored circles on a blank background that would change color when ``activated'' on a given trial (as in Figure~\ref{fig:CBN}c).  Participants performed interventions by explicitly setting the states of any subset of these nodes and observing the results while making accuracy-incentivised judgments about the hidden causal relationships.  \citeapos{bramley2015fcs} analysis showed that people's intervention choices were motivated by uncertainty reduction across the full space of possibilities but that learners' ability to integrate the evidence across trials was limited.  %n particular, they exhibit a dual pattern of recency (over-reliance on the latest data) and conservatism (unwillingness to abandon connections they had inferred on previous tests), with sequences of interventions that were repetitive and stereotyped in comparison to ideal learner norms.  
\cite{bramley2017neurath} accounted for judgments and interventions pattern as resulting from a bounded inference process in which learners do not represent the full space of possibilities but rather make local changes to a single working hypothesis.  However, a common judgment error in these studies was not captured by the bounded learning analysis.  Participants still exhibited a tendency to mark direct connections from causes to both their direct and indirect effects, suggesting that people are hesitant about judging there to be indirect causation when shown multiple outcomes occurring simultaneously \citep[see also][]{mccormack2015temporal,fernbach2009causal}.%resulting in a successor representation \citep{dayan1993improving,momennejad2017successor}% which is typically extremely large, but rather incrementally update a single working hypothesis by making local changes that account for evidence as it arrives, and choosing interventions that help resolve the local rather than the global uncertainty.

%$Across the sciences, numerous representations of causal systems have been developed that express specific causal theories at different degrees of granularity.  These include mechanistic simulations \citep[e.g.][]{karr2012whole,sjostrand2008brief} that predict precise patterns of events and fluctuations in variables across time, dynamic point processes \citep[e.g.][]{poisson1837recherches,karlin2014first}, state space models \citep[e.g.][]{david2006dynamic, friston2014granger}, but also graphical models such as causal Bayesian networks \citep[hereafter ``CBNs'', ][]{pearl2000causality} that capture aggregate patterns of statistical covariation.  \citep{pearl2000causality}.

% \subsection{Successes of framework}
% Survey the many studies based on tabular data and stylized examples. \citep{bramley2017neurath,bramley2015fcs,sloman2005do,lagnado2002learning,rehder2001causal,deverett2012learning,kemp2010learning,griffiths2005structure,holyoak2011causal,cheng1997from,fernbach2009causal,mccormack2016children}

\ntodo{The section above might be possible to cut substantially}

\subsection{Shortcomings of CBNs for modelling everyday interventions}

In this section we highlight several mismatches between CBNs and the kinds of causal inference problems that characterize everyday causal cognition.  These break down into: 1. The requirement that evidence be gathered over independent trials; 2. the absence of temporal considerations relating interventions and subsequent measurements of the system; and 3. problems with representing cycles and feedback dynamics.

As we noted above, CBNs are very limited in their representation of time.  The interventional calculus embodies the minimal assumption that causes precede their effects, but it says nothing about the relative delays of competing causal pathways. %For example, a CBN in which $X_1\leftarrow X_2\rightarrow X_3$ implies the partial ordering $X_1\succ\{X_2,X_3\}$, but does not say anything about whether one should expect to see $X_1$ or $X_3$ change state first if influenced by $X_2$.  They also do not not capture that, other things being equal, we might expect a chain of causal influences (e.g., $X_1\rightarrow X_2\rightarrow X_3$) to take longer to play out than a single causal variable directly causing many different variables.  
Thus, CBNs are a natural partner to evidence that really has been gathered across multiple independent trials, in which causal relationships play out too fast to measure, or in which each variable is only measured once.  By design, these features are standard in real experimental data sets.  For instance, medical studies will typically involve a procedure for randomized recruitment to ensure participants are roughly independent samples from the desired population.  Then, there will be a protocol for when to measure outcome variables (such as blood pressure, insulin levels etc) following an intervention (i.e. delivery of a treatment).  The use of a fixed trial protocol makes it straightforward to aggregate results across the sample of patients.  However, choosing an appropriate schedule of treatment and measurement seems to require substantial preexisting causal expertise about how long the relevant mechanisms will take to work and when any effects will be most measurable.  %For any single participant, much more fine grained information may be available such as continuous measures of outcome factors before, during, and after the delivery of the medicine.
%Because the mechanisms are reasonably well established, it is appropriate to aggregate and throw out majority of the more fine grained evidence that could be measured in an individual patient (e.g. continuous measures of outcome factors before during and after the delivery of the medicine).    %Presumaby the latter timeseries data is used in other contexts such as choosing an appropriate schedule but is not so easily aggregated or reported.%; a software engineer may roll out one version of a website to some users and a different version to others, and later measure their propensity to make purchases or spend time on the site.  Thus, in these settings the outcome factors are measured at some fixed, prechosen time after treatment and aggregated over many independent patients or users.

A consequence of the CBN framework being widely used to study causal cognition is that many of the experiments in the literature provide learning data that is already ``packaged'' into a CBN suitable format.  %This means the experimental set up implies that there evidence comes from independent trials, and that, within a trial, temporal information is unavailable or uninformative.  We highlight several ways in which this setting does not match well onto everyday causal learning, before turning to the role of time in causal learning. %suitably analogous to the data science problems that CBNs excel at.
%\paragraph{Independent trials}
In early studies, training data was sometimes literally provided in tabular form, with participants were shown a table or icon array detailing how often several variables appeared together or separately and asked to make judgments directly from this \citep{cheng1990probabilistic}.   In more recent studies, participants were shown a series of cases \citep{gopnik2000detecting,sobel2004children,deverett2012learning,lagnado2002learning},  or invited to choose a sequence of interventions of their own devising \citep{coenen2015strategies,bramley2015fcs,bramley2017neurath}.  But in these studies participants are instructed or given a cover story implying that these should be treated as completely independent trials.   % \citep{deverett2012learning,lagnado2002learning} or interventions \citep{gopnik2000detecting,sobel2004children}, or invited to choose a sequence of independent interventions of their own devising \citep{coenen2015strategies,bramley2015fcs,bramley2017neurath}.  
In some cases, the cover stories involved artificial mechanisms that reset on each trial --- this includes the ``black box'' toys used in developmental studies \citep{mccormack2016children,gopnik2007causal,coenen2017beliefs}; but also a range of artificial mechanisms involving lights, sensors, circuits, and switches \citep{waldmann2000competition,sobel2006importance,coenen2017beliefs}.  In other cases, the nature of variables are not described at all \citep{bramley2015fcs,bramley2017neurath,rehder2005feature}, or independence is established by instructing participants that each trial is performed with a different sample from a population %--- such as different petri dishes of bacteria, or different shrimp pulled from Lake Victoria %with different levels of retirement savings, investment rates and trade deficits a different shrimp pulled from Lake Victoria, 
\citep{rehder2003causal, rottman2012causal, rehder2017failures}.

While ingenious, these cover stories create situations that rarely obtain in everyday learning.  The causality we encounter in physical and social systems of everyday life do not, generally, reset themselves between each interaction.  In life, one learning episode typically bleeds into the next, with no clear boundaries.  We might try a medicine on multiple occasions, but if we want to treat these tests as independent we had better leave a substantial amount of time between each test, relative to our beliefs about how long the drug takes to pass through our system.  We also must keep in mind the ongoing evolution of our health and potential adaptation in our receptivity to the medication. Choosing when to act, and how to delineate between and aggregate over ``trials'' in continuous experience are important aspects of real world causal reasoning (\hl{episodic cognition ref?}), that are missed by the studies focusing on CBN-packaged data.  %A story of causal cognition that does not capture these aspects of intervention selection and inference seems to miss the larger picture.  %In life, unlike in science, one cannot ``jump into the same river twice'' \citep{barnes2013presocratics}.

As well as separation \emph{between} trials being hard to achieve in everyday life, sensitivity to how things play out \emph{before, during} and \emph{after} a real world interventions is also intuitively important.   \cite{rottman2012causal} point out that it is often \emph{change} in the state of the variable from one time point to another that people see as an effect needing an explanation.  Furthermore, it is hard to imagine intervening on something in the world without first observing its pre-interventional state.  The metaphysics of an intervention in a CBN not only presumes independence of one test from one another, but also imposes an implausible sequence of actions and measurements on the part of the learner.  An idealized intervention involves setting variables substantially \emph{before} observing the system.  Any causal effects of one's interventions are assumed to have entirely propagated through the system by the time the observation is made, while the trace of the resulting variable states must persist long enough to be measured together at the end.  %So, if the true structure is a strong $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$ chain and a learner performs the intervention $\Do[X_1=1]$, they expect to then look and see both $X_2=1$ and $X_3=1$.  
To make this work in psychology experiments, cover stories are used that are generally either: vague about the measurement of time, pertain to variables whose states are only observable after the fact, or  involve causal relationships that would propagate too fast to be observed.  For example, \citeapos{steyvers2003intervention} cover story involved aliens reading one another's minds.  It was implied that aliens would keep thinking the same symbol string (i.e. ``XYZ'') unless they chose to read another alien's mind, in which case they would then think the same thought as that other alien and keep doing so until the measurement was made. 

%With the Blicket detector, the outcome variable (playing a song) ensures thatwhile the mechanism is instantaneous, the outcome continues long enough to be observed.   \cite{lagnado2002learning} use several cover stories including one where the temperature and pressure inside a rocket are related to whether it successfully launches.  In these cases, there is an outcome variable that clearly occurs later but two other variables that are ambiguous enough that they  might play role of cause, mediator or effect, requiring the timing of their measurement to be left quite vague.  A very common and puzzling finding in these studies is that people struggle to separate direct and indirect causal influences %(i.e. where $X_1$ has a direct influence on $X_2$ but only an indirect one on $X_3$ in the $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$ chain)
% \citep{bramley2017neurath,fernbach2009causal,mccormack2016children}.  They thus, often appear to form something more like a successor representation \citep{dayan1993improving,momennejad2017successor}, in both direct and indirect effects are collapsed together.


%\paragraph{Cycles}

CBNs are based on factorization of a joint probability distribution, meaning they cannot naturally represent relationships that form loops or cycles.  However, such dynamic relationships seem pervasive in the world --- e.g. that liking rock music might make you join a rock band which  might further increase your enjoyment of rock music and so on.  All sorts of real world processes, from population change \citep{malthus1888essay} to economic, biological and physical interactions, are characterized by reciprocal and dynamical causal processes.  In experiments, people frequently report causal beliefs that include cyclic relationships when allowed to do so \citep{nikolic2015there,kim2002clinical,sloman1998feature}.
While there are ways of adapting the CBN formalism to capture cycles --- \citep[e.g.][]{dean1989model,lauritzen2002chain} and see \cite{rehder2016cycles} for a recent review --- these either evoke a sequence of equally spaced discrete time steps or model the equilibrium behavior of the system.  Thus, none of these proposals capture how cause--effect relationships unfold in continuous time, where some relationships might occur much faster or slower than others.%\footnote{Although see \cite{pacer2015upsetting,pacer2011rational} for a model that covers related, rate-based, cases.}

%\ntodo{Seems too long and cycles doesn't fit in well}
 %In the experiments measured above, the learner is invited to trust the experimenter that these sequential trials are truly independent.  However, none of these things really hold in everyday human causal learning.  We experience the world as a single ongoing event stream.  As Hereclitus put it, one ``cannot step into the same river twice'' and so are stuck in a situation where independent trials are a luxury brought about through careful curated scenarios, rather than the basic mode of data.  %and so must act and synthesize a control condition.

%Thus, in all these studies, there participants are expected to trust the experimenter that the right amount of time was left between any interventions and measurements of other variables such that any effects will be visible, and the allocation of people to testing conditions was done appropriately randomly


\subsection{Summary and discussion}

In sum, as we move away from carefully curated experimental scenarios toward more naturalistic interventional learning data, the assumptions that lend the CBN framework its mathematical simplicity also make it less adequate for the challenge.  %The intervener is supposed to set variables without checking what state they are already in.  And they then wait ``long enough'' for all the causal processes to have taken place before looking at the new state of the system.  They are then expected to either discard the current sample and pick a new one independently from a population, or to wait long enough to be sure that there is no residual dependence in the system state before starting their next test.  
%The CBN's time free representation make a lot of sense when reasoning about experimental protocols.  I.e. if testing the efficacy of a drug, it makes sense to use one's prior expectations of how long the drug's mechanism should take to work on the body, and so choose a single appropriate time after drug delivery at which to measure all participant's health.  The resulting data then naturally aggregates over instances and allows for probabilities and propensities to be derived.  
When we interact with the causal world, we typically have access to a lot more evidence than independent joint state measurements.  We can monitor variables continuously, tracking when events occur relative to one another and how continuous quantities ebb and flow over time.  We are sensitive not just to the ``final'' states of variables after causality has been and gone, but their states prior to interventions and their subsequent transitions.  To intervene effectively and to draw sensible causal conclusions by aggregating over extended experience, we must not only worry about what variables are relevant, but also when they should be measured, how long to leave between tests, or how to best ``reset'' a system before testing it anew.  
%These considerations suggest that much of the richness of causal evidence available from action and experience is buried by the CBNs decoupling from temporal evidence. %it is assumed that one already knows \emph{when} and where to expect effects. %(i.e., give medication, \emph{wait 2 hours}, measure health factors, discretise, fill contingency table)
This suggests that much of the most important and interesting causal inference work takes place while packaging a learning problem into a CBN suitable format.  Independent trials are a luxury brought about through carefully curated scenarios, and aggregation to the level of contingencies and probabilities only becomes possible when we have rich enough knowledge of functional form to abstract away from time's arrow.  Thus, it is instructive to model this finer grain of human causal learning and reasoning in which time's arrow is fully represented.     %We are instead stuck in a situation where 
In the next sections, we build on these considerations and classic results about learning from temporal information, sketching two new approaches to modelling causal representation and interventional learning in continuous time.

%It would also be perverse to take a new medication without first taking into account our state of health prior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Incorporating time: Event data}\label{section:DN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Existing research}

%The order of events provide an obvious cue to causal directionality since causes, by definition, precede their effects \citep{hume1740treatise}.  In addition to event order, expectations about the length and reliability of delays between causes and effects can provide substantial additional information.  
%A basic associative learning result is that, as the average interval between two events increases, the associative strength between the two events decreases \citep{wolfe1921effect,shanks1987associative, grice1948relation}.  %Cognitively, this might be because the more distant two events are in time, the more costly it is to sustain the first event in working memory long enough to relate it to the second, leading to monotonic reduction in causal judgments \citep{ahn1995role,einhorn1986judging}. Normatively, it also makes sense since, ceteris paribus, the longer the gap between putative cause and effect, the more likely it is that other events may have occurred in the meantime that could have also caused the effect \citep{buehner2003rethinking, lagnado2010influence}.  However, shorter intervals do not always lead to stronger causal inferences. Rats form food aversions even when sickness is induced hours after eating --- reflecting the true time course of food poisoning \citep{garcia1966learning}.   
%Likewise, longer fixed-interval reinforcement schedules in pigeons result in longer delays between the appearance of the reinforcer and pecking responses \citep{skinner1938behaviour,gallistel2000time}.  %These results show that time-delayed associations are formed and can be used to guide action in animals.\footnote{Although, whether or not all these behaviors rely on causal beliefs is debated \citep{clayton2006rational,blaisdell2012rational}.}
People have been shown to make systematic use of both event order \citep{bramley2014order} and delay information in inferences about causal relationships \citep{buehner2003rethinking,buehner2004abolishing,buehner2006temporal}. Causal beliefs have also been shown to influence time perception \citep{bechlivanidis2013does,buehner2009causal,haggard2002voluntary}.  A basic associative learning result is that, as the average interval between two events increases, the strength with which they are associated decreases \citep{wolfe1921effect,shanks1987associative, grice1948relation}.  However, people do not always see shorter intervals as more causal, but rather prefer intervals that match their expectations, where these might come from prior experience or though familiarity with causal mechanisms.  
Both shorter-than-expected and longer-than-expected intervals have been shown to reduce causal strength judgments \citep{buehner2004abolishing, schlottmann1999seeing,buehner2002knowledge,greville2010temporal,hagmayer2002temporal,buehner2003rethinking,greville2016temporal}.  Reliability also appears to be key to strong causal attributions from time, with variability in inter-event intervals normally associated with reduced causal judgments \citep{greville2010temporal,lagnado2010influence,greville2013structural}.\footnote{See \cite{young2009problem} for a counterexample.} 

Supporting the notion that temporal considerations inevitably feed into causal judgments, several studies have pitted temporal order cues against statistical contingencies.  These studies generally found that causal judgments were dominated by temporal information \citep{lagnado2006time,lagnado2004advantage,burns2009temporal,schlottmann1999seeing,frosch2012causal}.  For example,  \cite{lagnado2006time} explore a setting where a virus propagates through a network of computers infecting each with some probability, but also with variable delays in transmission from one computer to another. Participants' task was to infer the structure of these computer networks based on having observed viruses spreading through the network on multiple trials.  Participants preferred causal models that matched the experienced order in which computers displayed infection, even when covariational information (which subset of computers got the virus on each trial) suggested a different structure.

%Furthermore, when researchers have tried to instruct participants to ignore event timing, participants still often treated the observed timings of events to be diagnostic \citep{white2006structure,mccormack2016children}.


%Since effects cannot precede their causes, the order in which events occur is an important cue to causality.  In line with this, much of the human and animal learning literature is built around the notion that learners %both segment their experience into events \citep{eichenbaum1993memory} and 
%readily form associations from one event to the next \citep{watson1913psychology,pavlov1928lectures,skinner1938behaviour}.  Associative theories try to account for learning as an automatic pairwise association of stimuli. However, recent re-analyses of classical conditioning phenomena have suggested that learning is often better understood as involving inferences about the causal structure that is responsible for the observed events \citep{courville2006latent,courville2003model,courville2006bayesian,courville2004similarity,gershman2012exploring}.

%In the causal learning tradition, several papers have explored the role of temporal information in structure inference. \cite{rottman2012causal} investigated how people infer the causal structure of multiple variables measured at discrete time points at which variables may be subject to exogenous influences or interventions.  For example, suppose you are interested in an amoeba that occasionally produces two different hormones.  Suppose it is producing neither hormone at time $t-1$.  If, at time $t$, you stimulate the production of one of the hormones and the other hormone is also produced, this invites the inference that the first hormone causes the production of the second.  Importantly, this inference is based on the fact that the second hormone's level changed state relative to the preceding time point, while pure covariational inference would treat each measurement as independent.  
%In seven experiments, the authors found that people readily attribute causal relationships from variables influenced at time $t$ to others whose state changed relative to $t-1$, doing so even if a cover story strongly suggests independence (i.e., if a new amoeba is measured at each time point). %consider a series of measurements which shows that barometric pressure was high on Monday and Friday, and that it rained on Tuesday and Saturday. This evidence invites the inference that high pressure on one day causes rain on the next day. 

%Experienced event order also affects people's causal judgments when events take place in continuous rather than discretized time.  \cite{lagnado2006time} explored a situation that contrasted trial-by-trial covariation with temporal order cues. In  their experiment, a virus propagates through a network and infects computers at different times. Participants' task was to infer the structure of these computer networks based on having observed the virus spreading through the network multiple times.  Participants preferred causal models that matched the experienced order in which computers displayed infection, even when covariation cues went against temporal order cues.


%Not only the order in which events occur but also their exact timing is important for causal inference \citep{hagmayer2002temporal}.  
%A basic associative learning result is that, as the average interval between two events increases, the associative strength between the two events decreases \citep{wolfe1921effect,shanks1987associative, grice1948relation}. %Early cognitive theories predict this effect by suggesting that the more distant two events are in time, the more costly it is to sustain the first event in working memory long enough to relate it to the second, leading to monotonic reduction in causal judgments \citep{ahn1995role,einhorn1986judging}. 
%Cognitively, this might be because the more distant two events are in time, the more costly it is to sustain the first event in working memory long enough to relate it to the second, leading to monotonic reduction in causal judgments \citep{ahn1995role,einhorn1986judging}. Normatively, it also makes sense since, ceteris paribus, the longer the gap between putative cause and effect, the more likely it is that other events may have occurred in the meantime that could have also caused the effect \citep{buehner2003rethinking, lagnado2010influence}.  However, shorter intervals do not always lead to stronger causal inferences. Rats form food aversions even when sickness is induced hours after eating --- reflecting the true time course of food poisoning \citep{garcia1966learning}.   
%Likewise, longer fixed-interval reinforcement schedules in pigeons result in longer delays between the appearance of the reinforcer and pecking responses \citep{skinner1938behaviour,gallistel2000time}.  %These results show that time-delayed associations are formed and can be used to guide action in animals.\footnote{Although, whether or not all these behaviors rely on causal beliefs is debated \citep{clayton2006rational,blaisdell2012rational}.}
%Humans make causal inferences that are sensitive to expectations about delays due to causal mechanisms.
%Seeing shorter-than- as well as longer-than-expected intervals leads to reduced causal strength judgments \citep{buehner2004abolishing, schlottmann1999seeing,buehner2002knowledge,greville2010temporal,hagmayer2002temporal,buehner2003rethinking,greville2016temporal}.  Variability in inter-event intervals has usually been found to reduce causal judgments \citep{greville2010temporal,lagnado2010influence,greville2013structural}.\footnote{Although see \citep{young2009problem} for a counterexample}  %Young and Nguyen explain this increase as a consequence of experiencing occasional very short delays when there is high variability.  While these studies have focused on situations in which there is a single candidate cause--effect pair, we are interested in the more general problem of inferring the causal structure of multiple variables based on observations of events in time.  

%\cite{griffiths2005causes} showed how different expectations about delay distributions allow for strong one-shot causal structure inferences. 
%In his experiments, participants made causal judgments about ``nitroX'' barrels that were causally connected and exploded in different sequences. Because different causal models imply different event timings, the Bayesian model rapidly inferred the causal structure from an observed sequence of exploding barrels. Building on this work, \cite{pacer2012elements} model causal inference in situations where a discrete event affects the rate of occurrence of another variable in continuous time \citep[cf.][]{greville2007influence}, and 
%\cite{pacer2015upsetting} capture situations where causal influences last for some time before they gradually dissipate.

%Pacer and Griffiths' approach is well-suited for capturing situations where events alter the \emph{rate} of occurrence of other events. It does not readily apply to situations in which causes bring about their effects exactly once. In this paper, we focus on situations in which the relationship between causes and effect is singular.




%Before the causal bayesian network... human and animal learning modelled as based in association between things that occur close together in time.
%The considerations in the previous section highlight that CBNs are not equipped for modeling evidence that arrives in continuous time. Since causes must precede their effects, and we often observe events as they unfold, timeseries information contains important cues to causality. Thee fact that the light turned on, right after we switched the switch, intuitively makes us confident in the evidence provided by our intervention.  If it had come on 10 minutes later things would have been much less clear.  On the other hand, if the doorbell rings seconds after I hang up on my delivery order, I would not expect my action (ordering pizza) to have caused the bell ring; it was too quick.  So it is clear we often know how long things should take and that this influences what we learn from our observations and actions.

%As well as failing to account for the role of temporal information in learning, CBNs also cannot account for the natural problems that we expect our causal beliefs to support in everyday life, such as when to act EXAMPLE, and when to expect the outcome of our actions --- e.g. over what time scale should we expect a medication to be affecting our health, or a .

 %--- i.e. since sun exposure often precedes reddened skin, or economic downturns often precede increases in extremism, we sensibly attribute the former as causing the latter.  The formation of associations between time-locked events --- famously exemplified by Pavlov's dog salivating upon hearing a bell previously paired with his supper --- is the most basic associative learning mechanism, common to all humans and animals \citep{pavlov1941lectures}.  
%However, relying on time alone is also notoriously fallible.  %Many things are associated in time without the former causing the latter --- cockerels crow reliably before sunrise without causing it and 





\subsection{Representing causal events and interventions in time}

\begin{figure}[t]
   \centering
   \includegraphics[width = .5\columnwidth]{DN}
   \caption{a) Example causal delay network with a generative $X_1\!\stackrel{+}\rightarrow\!X_2$ preventative $X_2\!\stackrel{-}\rightarrow\!X_3$ connection \citep[see][]{bramley2018time,bramley2017dynamic}. b) Example observational evidence.  Rows denote variables and white circles mark activations over time.  c) Independent interventions (rows) on a chain with unreliable $X_1\!\stackrel{+}\rightarrow\!X_2$ connection and reliable $X_2\!\stackrel{+}\rightarrow\!X_3$ connection.   The timing of $X_2$ predicts the timing of $X_3$ because it is on the causal pathway.  d) Same for common cause with reliable but longer $X_1\!\stackrel{+}\rightarrow\!X_3$ connection.  Here timing of $X_2$ does not predict timing of $X_3$ because it is not on the causal pathway. e) Example freeform interventions that are not well spaced.   Rows denote variables, hand symbols and thick borders denote interventions.  Dashed gray lines show the actual causal influences.  f) As in e) but interventions more widely spaced.  g) Interventions in a cyclic network. h) Including ``blocking'' interventions that temporarily prevent activations of the blocked variable and so break feedback loops.}
   \label{fig:DN}
\end{figure}

To link these results to the problem of structure induction over multiple variables, we need a causal representation rich enough to encode beliefs about causal inter-event delays.  % as well as contingencies.  
\cite{bramley2018time} recently developed a normative framework that does this.  % for the case of causally related events.
Concretely, their framework captures causality between events (hereafter ``activations'') that occur at components of a system $X_1\ldots X_n$ at particular points in time, but have no measurable duration of their own \citep{cox1980point}.  \citeapos{bramley2018time} approach captures activation patterns within a causal system as being produced by mixture of exogenous influences and endogenous causal relationships with parametric delays governed by Gamma distributions with some mean $\mu$ and variability $\alpha$. Figure~\ref{fig:DN}a gives an example of an  $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$ chain analogous to the CBN in Figure~\ref{fig:CBN}.  %, but now capturing the temporal dynamics of the causal relationships.  %The model is of a $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$ causal chain with a generative and a preventative connection.  
As in a CBN we can assume all three variables are activated due to exogenous factors with their own ``base rate'' probability.  This can be captured by a Gamma distribution where shape $\alpha$ is set to 1, equivalent to an Exponential distribution with rate $1/\mu$.  This means that the expected delay before the next activation of $X_1$ is constant, regardless how long it has been since the last activation of $X_1$ or any other variable.  Activations of $X_1$ then cause activations of $X_2$ with a typical delay, here captured by a Gamma($\mu=1,\alpha=10$) -- i.e., with a mean of 1 second and standard deviation of $0.1$ seconds (see Subfigure~\ref{fig:DN}a, ii).  These caused activations are in addition to any activations of $X_2$ caused by its base rate.  Thus, the model implies that we should expect to see $X_2$ activate shortly after observing $X_1$ activating.  Activations of $X_2$ then have a preventative effect on $X_3$.  This is modeled by having activations of $X_2$ temporarily censor the next activation of $X_3$.  Mathematically, this changes the distribution for $X_3$ from Exponential$(\lambda = 1/y)~\equiv~$Gamma$(\mu = y, \alpha = 1)$ to Gamma$(\mu = 2y, \alpha = 2)$, doubling the time we expect to wait until seeing $X_3$ again and introducing a temporary dependence between latest $X_2$ and the next $X_3$ (see Subfigure~\ref{fig:DN}a, iii).  %Upon the next occurrence of $X_3$, the prevention must be complete, and the probability of $X_3$ is once again exponentially distributed.  
\cite{bramley2018time} provide the mathematical details for estimating model parameters and incorporating base rates and failure rates (similar to causal strengths in CBNs), as well as modelling conjunctive or disjunctive combined causal influences. However, here we simply highlight at a high level what we learn about causal cognition by applying this framework to human judgment and intervention patterns.  %with some probability $w_{1,2}$ as in the CBN case,

\subsection{Modeling inference}

Unlike the CBN case, \citeapos{bramley2018time} approach captures how timing information informs structure judgments across potentially open ended learning periods, without the necessity of independent trials.  If a learner already has a strong domain prior, for instance that a particular causal relationship will take certain length of time to work, this will drive their judgments from time information in a straightforward way, they will only assume an interval is causal if it is close to the expectation.  However, different structures map inter-event intervals to different causes-effect delays, meaning that even without a specific prior expectations about delays, the ability of candidate structures to parsimoniously account for the data can be compared.  If a learner starts out with no expectations about how long or how reliable causal delays will be --- mathematically, if they start out with an uninformative ``improper'' prior on $\mu$ and $\alpha$ --- Bayesian Ockham's razor will still favor whatever structure can assign the highest likelihood to the patterns of time dependence observed between variables, with the fewest separate delay parameters \citep{bramley2018time}.   For example, the reliable $X_1-X_2$ intervals %and unreliable $X_2-X_1$ delays 
in Figure~\ref{fig:DN}b will favor models that include a generative $X_1\!\stackrel{+}\rightarrow\!X_2$ connection since this can explain the activations of $X_2$ better than a model presuming them to be exponentially distributed to caused by any of the other variables.   %However, \cite{bramley2018time} framework goes beyond this to integrate these expectations into the causal representation and show how they can be learned through mere exposure to event data.

\cite{bramley2018time} show that people spontaneously make structure judgments from event data, broadly in line with the normative framework described above.  In three experiments, participants were asked to judge which of a range possible causal structures best describe the activations of components of a set of mystery devices.    In one experiment, participants were presented with four video clips of each mystery device, each showing all of the devices' components activate over 3-4 seconds.  Each video begins with the activation of the root component ($X_1$) followed by the activation of three other output components ($X_2$ and $X_3$ and $X_4$) in varying orders with varying intervals.  Participants not only ruled out structures that could not have produced all of the observed patterns, but also assigned more probability to structures to the extent that could reliably have produced the observed delay patterns. %and a minimum number of exogeneous activations.

\subsection{Modeling interventions}

Just as with the CBN-packaged observational data in Figure~\ref{fig:CBN}b, pure observations of events in time cannot unambiguously reveal causal structure.  For example, it could be the case that the tight temporal relationship between $X_1$ and $X_2$ in Figure~\ref{fig:DN}b comes about because the variables share an unmeasured cause that brings about $X_1$ faster than it brings about $X_2$.  Fortunately, intervention again makes it possible to resolve these kinds of ambiguity.  Any temporal dependence between the intervened on variable and activations of other variables suggests the presence of a causal influence.\footnote{For this to hold interventions must be performed at random or prechosen intervals (thus are unrelated to ongoing activity).}  Concretely, for two variables $X_i$ and $X_j$, if the distribution of inter-event delays $p(t_{\Do[X_j], X_i})$ has a best fitting $\alpha$ parameter $>1$, this means that $X_j$ hastens $X_i$ relative to its baseline, meaning that $X_j$ (directly or indirectly) influences $X_i$.\sntodo{Prevention is a little more complicated:()}
A key property of \citeapos{bramley2018time} modelling framework is that it captures how the sequence of events following an intervention carries information about structure. Post-interventional event \emph{order} places constraints on what could have actually caused what on a given occasion.  But beyond this, the causal delays inherit time variability from their parents relative to interventions.  Thus, even if an intervention causes the same activations in the same order each time, if the timing of one of these downstream events carries information about another later one, this tells us something about the causal ordering of the variables.  Figures~\ref{fig:DN}c and d visualize  evidence produced by interventions on the root component of a chain and a common cause device.  The device in Figure~\ref{fig:DN}i is in fact a fork where $X_3\!\stackrel{+}\leftarrow\!X_1\!\stackrel{+}\rightarrow\!X_2$ with one fast connection $X_1\!\stackrel{+}\rightarrow\!X_2$ and one slow one $X_1\!\stackrel{+}\rightarrow\!X_3$.  Here the $X_1\!\stackrel{+}\rightarrow\!X_2$ and the $X_1\!\stackrel{+}\rightarrow\!X_2$ delays are uncorrelated.  However below we see a $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_2$ chain structure with two fast connections, one unreliable connection from $X_1\!\stackrel{+}\rightarrow\!X_2$ and another reliable connection $X_2\!\stackrel{+}\rightarrow\!X_3$.  This is revealed by the data for the chain structure, a late $X_2$ tends to mean a means late $X_3$ as well.


We can think of the propagation of variability through the system as acting akin to a game of ``broken telephone'' in which the accumulation of errors in a signal (here in the lateness or earlyness of the event timing) can reveal the sequence of message passers.  As with broken telephone, if there is no noise (if everyone passes the message perfectly; or if all causal relationships are perfectly reliable) there is noway to tell in what order the message was passed on. But, a moderate about of noise can lead to a ``blessing of variability'' effect.   \cite{bramley2018time} show that people are sensitive to this subtle signal, favoring chain structures as for a range of scenarios in which a mediating variable carries information about a distal effect $X_3$ and fork structure when it does not.%, although also finding evidence that people rely on easier to track statistics than correlation to make these judgments.

%Our post-interventional observations can also tell us about structure relating multiple downstream variables.  Postinterventional event \emph{order} places constraints on what could have caused what.  Moreover, delays inherit time variability from parents relative to intervention, therefore interventions + noise reveals structure (i.e. if late $X_2$ means late $X_3$?, like how Chinese whispers accumulation of errors implies the order of the people asked).   \hl{Demonstrate with chain vs fork inference example both from order} (currently in Figure~\ref{fig:DN} a) and from pure variability \hl{adapt figure from role\_of\_time}.  [NB note the possibility of unmeasured shared mediators resurfaces though]


%But this could easily be ancestral or indirect (Figure \ref{fig:DN}a \hl{to add, noninterventional events in time example}).

%Time is also intimately related to the idea of intervention.   

\subsection{Choosing when and where to intervene}

\citeapos{bramley2018time} approach is applicable to situations where variables may activate or be intervened on multiple times during a single learning period, and in which the underlying structure might contain cycles.  Thus, \cite{bramley2017dynamic}\sntodo{Maybe by time it comes out we'll have a journal ref} also used the framework to explore learners choices of interventions in an extended interaction with a causal device of interest.  Instead of giving participants a sequence of independent trials in which to set variables as in traditional interventional learning studies, participants were given 45 seconds to interact with a dynamic causal system and perform up to 6 interventions by clicking on components on the computer screen to activate them.\footnote{See \url{https://neilrbramley.com/experiments/it/videos/example\_trial.html} for a video of a trial.}  In this setting, learners had to choose both when and where to intervene.  \cite{bramley2017dynamic} explored learning about a mixture of acyclic structure (no feedback loops) and cyclic structures (containing at least one feedback loop), and contrasted learning about devices whose causal delays were known to be a reliable 1.5 seconds (standard deviation of 0.1 second) with those that were known to be unreliable (with standard deviation 0.7 seconds).  Causal connections had a failure rate of .1 and there were no exogenous activations aside from the learners' own interventions.  Consistent with the predictions of the normative model sketched above, \cite{bramley2017dynamic} found that participants were able to identify the majority of the causal connections based on the delayed activation information they produced, and were more accurate at identifying the structure of the devices when the delays were reliable.  %However, unlike the normative model predictions, people were considerably worse at identifying cyclic structure than acyclic.  %Figure~\ref{fig:DN}f and g give example timelines of interactions with acyclic and cyclic devices.  

\cite{bramley2017dynamic} found that successful participants tended to leave large and regular intervals between each intervention, especially when the true causal relationships were unreliable.  Successful participants also tended to wait longer since the most recent activation before intervening again, suggesting they waited until they had a low expectation that any other confounding activity would occur during their intervention.  For example, Figure ~\ref{fig:DN}e shows a poorly chosen set of interventions that occur close together, making it ambiguous which activations caused which.  By contrast, the well spaced interventions in Figure ~\ref{fig:DN}f make it clear that $X_1$ is a common cause of $X_2$ and $X_3$.  Both $X_2$ and $X_3$ reliably follow from the two interventions at $X_1$, but on the second occasion $X_2$ and $X_3$ reverse their order of activation, essentially ruling out the $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{+}\rightarrow\!X_3$ chain hypothesis that was a plausible until that point.  No study has explored preventative causation in the domain of continuous time causal events. However, by contra-position to the above, the ideal time to test for a preventative relationship (as in the $X_2\!\stackrel{-}\rightarrow\!X_3$ link in Figure~\ref{fig:DN}a) should be when one has a strong expectation the effect is otherwise about to occur.


For the acyclic devices tested in \cite{bramley2017dynamic}, participants exhibited a substantial preference for intervening on the root components.  Unlike than in most CBN based experimental settings, these positive test interventions \emph{are} informative about the relationships between the downstream variables due to the blessing of variability described above.   By intervening at the root of the causal device, learners could then observe how the causality propagated through the system making use of ordering and delay correlations to uncover the causal sequencing (e.g., Figure~\ref{fig:DN}c).

\citeapos{bramley2017dynamic} experiment is also the first comparison of human learning about acyclic and cyclic devices in continuous time.  %The two obvious differences in the evidence available in the two conditions were its abundance and its degree of ambiguity about which events caused which even when conditioning on a causal structure \citep{halpern2016causality}.  
Participants' interventions on cyclic devices often led to sustained patterns of activation that would continue until a failed connection would cause the system to acquiesce.  This meant that participants experienced substantially more events in total even though they tended, reactively, to perform substantially fewer interventions on the cyclic devices.  While this resulted in stronger evidence about structure for the cyclic problems according to the normative learning model, the data was also much more computationally taxing to deal with in real time given the variability in the true causal delays.  The pattern in Figure~\ref{fig:DN}g demonstrates this.  It is clear from the repeated activations that $X_1$, $X_2$ and $X_3$ are related with a feedback loop.   However, establishing the exact set of connections is much harder. There are also many plausible patterns of actual causation that could have given rise to this data --- that is, there are many ways one could draw dashed lines as in the figure, to attribute each activation uniquely to a unique previous activation or intervention.   To estimate the posterior probability distribution over potential structures it was necessary to sum over large numbers of patterns of possible actual causation \citep{halpern2016causality}.  Thus, in is not surprising that human learners with their limited cognitive resources, were substantially worse at identifying the structure of the cyclic devices.  \cite{bramley2017dynamic} account for these pattern by positing that human learners build up their causal models incrementally, reacting to each new activation by attributing it to a recent intervention or activation.  When there are many causal influences going on simultaneously, this approach becomes less accurate.

\subsection{Summary and discussion}

\cite{bramley2018time} developed a normative framework for modeling interventional causal learning and representation in continuous time.  Application of this framework to experimental data demonstrated that people use temporal delays between events systematically to infer the causal structure that most parsimoniously explains patterns of observed delays.  %
The framework also showed that people are able to choose sensibly when to intervene, using their expectations about ongoing dynamics to wait until it is unlikely that ongoing processes will confound the interventional evidence.  The framework captured why people have a preference toward initiating the root component of causal systems --- unlike the CBN-packaged data, this more natural temporally extended data often contains evidential signals about the structure of causality as action propagates though the system.

The analyses also shed light on the intuition that cyclic systems are naturally harder to learn reason about. The computational cost of exact inference in \citeapos{bramley2018time} framework increased with the number of events that might be related, and cyclic systems tended to produce more activations.  The interventions investigated by \cite{bramley2017dynamic} can be thought of as ``shocks'' to the system, where an additional event is created by the learner.  One way that learners might get clearer structural evidence about dynamic systems in real world contexts is by using preventative interventions to ``block'' rather that just ``shock'' the system.  Figure~\ref{fig:DN}h imagines a case in which a hypothetical learner uses a mixture of punctate ``shock'' interventions that create activations of variables similar to above, with extended ``block'' interventions that prevent a variable from activating for an extended period.  This latter action is analogous to setting a variable to 0 in the CBN setting explored in \ref{section:CBN}.  %, since causal influences in the CBN were interpreted as working when the cause was active.  
Blocking breaks the feedback in the system allowing the learner to use interventions to identify one part of the structure at a time as they would in the acyclic cases.  Future work could explore how people use a combination of such ``shocks'' and ``blocks'' to learn about cyclic systems.
%The interventions investigated by \cite{bramley2017dynamic} can be thought of as ``shocks'' to the system, where an additional event is created by the learner.  Another kind of intervention t hat might be more useful for uncovering the component parts of cyclic systems could be a block, where a component is temporarily disabled, potentially short-circuiting the feedback loop within the target system and allowing learning to proceed similarly as in the acyclic systems.  

\cite{lagnado2004advantage} proposed that real time interventions act like a strong order cue, with events happening shortly thereafter liable to be associated causally with the action.  \citeapos{bramley2018time} framework captures under what conditions this heuristic will work well, but also formalizes how interventions provide structure evidence regardless of whether this holds.  If the base rates of variable activations are low relative to the length of actual causal delays, interventions that activate the cause will tend to create a reliable order cue.  That is, effects of the intervention will frequently be the next activations to occur as they are in Figure~\ref{section:DN}e.  The fact that participants struggled when there was a dense flurry of activations in \cite{bramley2017dynamic} is consistent with the idea that people really do rely on this kind of cue to build up their hypotheses.  
However, the normative framework also captures how inference is possible even when this does not hold.  Wherever interventions affect the delay distribution for another (e.g. hastening or delaying it relative to its baseline) this is evidence for some form of causation.

%As with CBNs, interventions must be performed to rule out specter of unmeasured common causes.  However, unlike the CBN case, the post-interventional dynamics also carry useful evidence that human learners pick up on.  
%FROM P7!
%Finally, we noted in the previous section that children and adults' often displayed a preference for ``positive tests'' that made all the variables activate but were typically not very informative in the CBN setting due to the lack of observable variable ordering \cite[e.g., in][]{coenen2015strategies,mccormack2016children}.  A common explanation for these patterns is that they reflect the expectation that a positive test gives the best chance of observing a sequence of causal influences propagating through the system.  %PASTED SECTION ENDS
Unlike the CBN, a learner who forms a time-extensive representation is also positioned to make real-time predictions about ongoing dynamics \citep{clark2013whatever}.  This allows them to control for their expectations about ongoing dynamics in choosing when to intervene.  This was evident in \citeapos{bramley2017dynamic} experiment in which successful participants would wait for activity to die out before intervening again.  %By comparison, recall that CBNs only support atemporal judgments about variables propensity to occur ``together'' within predefined independent trial windows.
%It is instructive to imagine dividing the evidence depicted in Figures~\ref{fig:DN}e--h into independent trials and measure the joint states of the variables in each --- i.e.,  which variables activated within each trial window.  This makes it clear that much of the important information in the evidence would be lost and that the choice of final window size and separation between windows would have a large effect on causal conclusions one might draw.

One weakness of \citeapos{bramley2018time} approach is that normative inference scales rapidly with the number of events experienced, meaning that cyclic or densely connected structures producing large numbers of events become very hard to evaluate.  One has to reason about many paths of actual causation playing out simultaneously, and deal with potentially large numbers of possible paths of actual causation.  Fortunately, as the number of events becomes unmanageable for reasoning at the level of event-to-event cause--effect mappings, one can start to reason instead about whether activations and interventions affect the \emph{rates} of occurrence of other variables.  \cite{pacer2015upsetting} model this setting, reanalyzing an experiment from \cite{lagnado2010influence} in which the occurrence of several types of ``seismic wave'' could temporarily alter the rate at which earthquakes occur.  This approach matches with a range of findings suggesting that beyond a small number of unique entities, people switch to approximate counting, known as subitizing, rather than attempting to focus on each entity exactly \citep{mandler1982subitizing}.
%On \citeapos{pacer2015upsetting} approach, there is no exact mapping between a cause and an individual effect,  the inference can then be performed for situations where there are arbitrarily large numbers of events

Another limitation of this approach is that it is built on the assumption that causality relates to point events.  This idealization seems reasonable for processes where the causal influences are occasional and sudden --- such as spiking neurons, earthquakes, gunshots, explosions and so on.  However, many other causal influences are continuous in time, and many causally relevant variables can take a continua of values with no transition or state being causally privileged.  Thus, in the next section we introduce another new framework able to handle learning and inference about variables that evolve and effect one another continuously in time.

% Representing causality in this way supports predictions about \emph{when} causally related events will occur and %ccur, have occurred, or would occur following interventions, as well as 
% captures how observed event timings provide evidence that helps identify the true causal model.  
%\citep{stephan2018latency}


% We survey the way that event timing adds to the evidence available from interventions
% \begin{enumerate}
% \item At a more fine grained level, 
% \item We partially solve this problem by intervening in time \citep[][]{lagnado2010influence,pacer2012elements} -- this at least lets us rule out the shared parent worry (show how it solves ambiguity from above).
% % \ttodo{relevant references by \cite{stephan2018preemption} and they have a more recent cogsci paper coming out this year as well}
% \end{enumerate}


% While events \emph{can} be collected up into frequencies and 'probabilities of co-ocurance' within time windows (e.g. over some time following an intervention, in raw experience they occur in time and are separated by delays and it is not clear under what conditions, ``binning'' this data will give us reliable causal evidence. 

%Events are a very general class of phenomena that need not be construed as punctate.   We discuss this assumption below, and consider how to model casual influences between temporally continuous variables in Section~\ref{section:ctcv}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Incorporating time: Continuous variables}\label{section:ctcv}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


While events are often a natural level for reasoning about the world, they need not be construed as discrete or punctate, and change need not thought of as being constituted by \emph{events} at all.  Many variables change continuously over time without clear change points, and their consequences can often ``linger on and mix with the effects of other actions'' \citep{jordan1992forward}. This means that in many contexts, better causal evidence can be had, and finer grained causal predictions can be made, if one represents the causal world as a continuous system relating continuous variables.  Many real world applications of causal inference methods focus on this kind of data.  Financial forecasting, climate research, and many aspect of structure estimation in neuroscience involve reasoning about causal influences between continuous variables.   The most well established of these approach is, so called ``Granger causality'', which uses time delayed regressions to assess one variable predicts the later values of another \citep{granger1969investigating}.  However, this approach suffers from the same limitations of any observational method for assessing causality.\footnote{Although, see \citep{eichler2010granger} for preliminary work linking interventions with estimates of Granger causality.}  Frequently variables \emph{Granger cause} one another without truly causing one another, due to their sharing influences of some unmeasured common cause.  For example, because neural activity waxes and wanes due to natural circadian rhythms, regressing one neural signal on another, even with a short delay, will often result in a significant correlation because both measurements share patterns of variation that stem from the brain's overall energy consumption rather than from specifically directed communication.  %This is the case with the example at the start, ice cream sales may Granger cause drowning deaths but fluctuations in both these variables are much more likely to be due to changes in weather and season.  
A number of studies have also explored how people learn continuous valued functional relationships more generally, but not in the context of inferring causal structure \citep{pacer2011rational,griffiths2009modeling,schulz2017compositional}.  So, in this last section, we survey a new framework for modelling interventional causal learning about continuous valued variables that influence one another in continuous time.  



\subsection{Representing continuous causality}

\begin{figure}[t]
   \centering
   \includegraphics[width = \columnwidth]{OU}
   \caption{Observations and interventions on a causal system relating continuous variables in an OU network.  Adapted from \cite{davis2018ctcv}.    Black dashed lines indicate interventions that hold variables in some position or move them systematically.}
   \label{fig:OU}
\end{figure}

\cite{davis2018ctcv} recently developed a new framework for modelling networks of continuous time causal influence.  Their approach is based on the Ornstein-Uhlenbeck (OU) process \citep{uhlenbeck1930theory}.  An OU process is a stationary Gaussian Markov process capturing random motion that tends to revert to some mean value.  Thus, OU processes describe something like Brownian motion that is subject to a corrective force that increases the further the variable strays from its mean. OU processes have been used to model a variety of phenomena including physical \citep{lacko2012planning} and financial \citep{barndorff2001non} systems as well as attention during multiple object tracking \citep{vul2009explaining}.  \citeapos{davis2018ctcv} insight was to treat all the relationships in a causal network as simultaneously evolving OU processes, such that each variable is noisily attracted to some function of the most recent states of its cause(s).  

In a normal OU process, the change in $X_i$ from time $t$ to $t$+1 is defined as

\begin{equation}
\Delta X_i^t =  N(\theta(\mu - x^t),\sigma)
\end{equation}
where $\mu$ is the mean that the process reverts to in asymptote, $\sigma$ is the noise level and $\theta$ controls how strongly the process is attracted to its mean.  To model ongoing causal relationships, the static $\mu$ is replaced by some function of the latest value of the putative cause variable $X^t_j$.  \cite{davis2018ctcv} focus on linear relationships, with a single multiplicative parameter $w_{ji}$, such that the updated value of $X_i^{t^{\prime}}$ is given by

\begin{equation} \label{cont_OU_def}
X_i^{t^{\prime}} = X_i^{t} + N(\theta ( w_{ji} \cdot X_j^t - X_i^t),\sigma).
%dx_t = \theta (\beta_{YX} \cdot y_t - x_t)dt + \sigma dW_t
\end{equation}
Thus, if $w_{ji}>0$, $X_i$ will be attracted to some positive fraction of the value of the latest value of $X_j$.  In line with the previous sections, we call this a ``generative'' connection as it implies that increases in $X_j$ cause increases in $X_i$.  If $w_{ji}<0$, $X_i$ will be attracted to some negative fraction of the latest value of $X_j$, with $X_i$ tending to decrease as $X_j$ increases and visa versa.  Values of $w_{ji}$ between $-1$ and $1$ will make $X_i$ undershoot the absolute value of $X_j$, while values greater than 1 or less than -1 will make $X_i$ overshoot. There is no causal relationship if $w_{ji}=0$ or $\theta_{i}=0$.  In the former case, the value of $X_i$ simply trends toward zero, and in the latter its movement is a classical random walk.



\cite{davis2018ctcv} assume multiple causal influences sum such that

\begin{equation}
\mathbb{E}(\Delta X_i^t) = \theta \Big(\sum_{j=1}^{n} w_{ji} X_j^t - X_i^t \Big)
\end{equation}
and

\begin{equation}
p(\ww, \Theta| \Delta X_i^t, X_j^t) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(\Delta X_i-\mathbb{E}(\Delta X_i^t))^2}{2 \sigma^2}}
%\ln(p(\beta_{iX} | dx_t, y_{i,t})) \propto -(dx_t - \theta \Big(\sum_{i=1}^n \beta_{iX} y_{i,t} - x_t \Big) dt)^2
\end{equation}
where $\ww$ collects all $w_{ji}$ parameters for $i,j\in N$ and similarly for $\Theta$ and $\theta_i$.  Figure~\ref{fig:DN}a shows an example continuous variable network $X_1\!\stackrel{+}\rightarrow\!X_2\!\stackrel{-}\rightarrow\!X_3$ analogous to those explored in Sections~\ref{section:CBN} and \ref{section:DN}.  In it, $X_1$ has no causes ($\theta_{1}=0$).  $X_1$ is then a generative cause of $X_2$ ($\theta_2=.1, w_{21} = 1.25$) which is a preventative cause of $X_3$ ($\theta_3=.1, w_{32} = -1.25$).  %Each variable moves $\theta=.1$ of the way between their current and asymptotic state per ``timestep'' with $\sigma=4$ normally distributed noise.  

Figure~\ref{fig:OU}b shows example data generated by the network in Figure~\ref{fig:OU}a.  The root component $X_1$ (red) drifts around randomly, while $X_2$ (green) chases noisily behind $X_1$, and $X_3$ (blue) chases the inverse of $X_2$.  As with previous settings, it is not possible to tell for certain what the right causal relationships are based on this purely observational data.  This is partly because all three variables could be effects of some common cause with different time lags (i.e. with different $\theta$ values). Additionally though, the sustained corrective causal influences mean the downstream variables $X_2$ and $X_3$ rapidly asymptote to be close to their target values, making it hard to see which moved first unless $X_1$ happens to move dramatically.  

One interesting property about these continuous time networks that is not captured by the event networks in Section~\ref{section:DN} is their ability to produce rich emergent feedback dynamics of the kinds observed all over the natural world.  Figure~\ref{fig:OU}c--e give examples of cyclic networks with very different emergent behavior.  Figure~\ref{fig:OU}c shows an inhibitory feedback loop ($w_{21}=w_{12}=.5$) where both variables trend toward zero no matter where they start.  Figure~\ref{fig:OU}d shows an excitatory feedback loop ($w_{21}=w_{12}=2$), where both variables trend toward either positive or negative infinity.  Finally, Figure~\ref{fig:OU}e shows an oscillatory ($w_{21}=2, w_{12}=-2$) feedback loop where variables chase one another up and down in a wave of increasing magnitude.  Given the ubiquity of emergent complex behavior in nature, it is counts in the favor of these simple networks that they are able to produce complex emergent dynamics.  However it remains to be seen how uniquely observation of such high level dynamics reveals the internal structure of the system as it may be that there are multiple ways of setting up a system to exhibit the same complex dynamics.\sntodo{Note I've not added subplots for this yet.  Its my favorite part of the cogsci paper but perhaps it doesn't belong here?}

%Cite the Nicolas Cage movies cause shark attacks example.  As with event data, there is a high risk that apparently propagated noise is actually noise from a shared source since, going far enough back, everything has a common ancestor.


\subsection{Modeling inference and interventions}

\cite{davis2018ctcv} devised a simple experiment to explore human interventional learning about continuous valued variables in continuous time.  As in the studies discussed in Sections~\ref{section:CBN} and \ref{section:DN}, participants were tasked with identifying the structure of a number of causal devices through interacting with them on the computer screen.  As with \cite{bramley2017dynamic} they were given 45 seconds to interact with the each device.  However, rather than clickable nodes, the variables were represented by vertical sliders.  The underlying network could contain generative $w=1$ and preventative $w=-1$ connections and feedback loops.  Unless intervened on, the sliders' positions updated 10 times per second appearing to jitter and track up and down across the (-100)--(+100) range as in Figure~\ref{fig:DN}b.  Intervening on a slider was achieved by clicking on it, which instantly moved it to the clicked on value, and by hold-clicking participants could keep the variable at a desired location or drag it around smoothly within the sliders' range.\footnote{See \url{https://neilrbramley.com/experiments/ctcv/demo.html} to try an example trial.}

Through interacting with the systems, participants were able to identify the large majority of the connections.  As in the other causal learning experiments discussed, the most frequent error was a failure to distinguish direct and indirect causal influences.  Participants would often mark direct connections from intervened on variables to indirect effects.  By marginalizing over possible $\ww$, $\Theta$ values under each possible structure, with $\sigma$ assumed to be known, Bayesian inference could again be used to estimate a posterior for each trial and used as a normative standard against which to compare participants' judgments.  \cite{davis2018ctcv} compared full Bayesian inference against a local computations model that assumed learners focused on one pair of variables at a time rather than the whole structure at once, finding that around two thirds of participants were better described by this heuristic \citep[cf][]{fernbach2009causal}.  The normative analysis also revealed that with only one exception, participants interventions dramatically increased the strength of the available evidence.

The role of interventions in this continuous context is intuitively somewhat different from in the previous settings.  Because the variables are capable of taking a wide range of values but naturally move only a little per timestep, interventions allow learners to inject dramatic swings and signals into the system, or hold variables at extreme or unusual levels \citep[Figure~\ref{fig:OU},][]{davis2018ctcv}.  For example, Figure~\ref{fig:OU}c shows a learner performing several dramatic interventions.  First they intervene to hold $X_1$ at 50 for (here, for 40 ``timesteps'').  This results in a marked increase in $X_2$ and, with more noise and lag, a reduction in $X_3$.  They then intervening to hold $X_2$ at -50 which only increases $X_3$, while $X_1$ returns to moving randomly.  Third, they intervening on $X_3$ in a sinosoidal pattern leads to a (lagged and noisy) sinosoidal pattern in $X_2$.  While \cite{davis2018ctcv} do not analyze participants precise intervention patterns, a large proportion appeared to involve moving and holding variables at extreme values, or rapidly dragging them up and down the full range of the sliders.  It is worth noting that in this setting interventions naturally act as both ``shocks'' and ``blocks''.  Intervening on a variable for an extended period not only takes over its state but also blocks it from participating in feedback dynamics.

\subsection{Continuous time control}

The interventions participants could performed in \cite{davis2018control} were often extended in time meaning participants likely to react to the movements of the other variables during their intervention, for example manipulating one variable in such a way as to keep another effect variable in a particular position.  %challenges the use of \citeapos{pearl2000causality} ``do'' operator.  Learners could react to the movements of the other variables during their intervention, for example manipulating one variable in such a way as to keep another effect variable in a particular position.  %In doing this their extended action is no longer a purely exogenous influence on the system, but actually becomes part of a larger system involving the reactive influence of the observed dynamics on the control.  
This reactivity brings the interventional learning problem increasingly close to an adaptive or ``dual control'' problem \citep{feldbaum1961dual,guez2015phd,klenske2016dual,schulz2017control}. This means one faces the ``dual'' problem of learning how the system works while already attempting to control it. For example, learning to play tennis takes place, largely, while attempting to play tennis. At first our shots are wild and do not go where we intend. But, we slowly learn to adapt our swing to different angles and speeds of the incoming ball hopefully building a causal control model of tennis in the process.  
A key question therefore, is to what extent people will spontaneously learn causal structure while attempting to master the control of a causal system of continuously related variables.  To investigate this, \cite{davis2018control} used their OU driven causal models as a class of control problems.   Their task was similar to to that in \cite{davis2018ctcv}, except that participants interventions were restricted to one ``control'' variable, and limited single step increments up or down.  Participants' goal was to keep another ``target'' variable in a reward region.  To master this task in a model based way, a participant would have to learn the structure of the network and use this knowledge to plan the actions that maximize the time the target variable spends in the reward region.  For some simple structures explored by \cite{davis2018control}, it was enough to exercise model-free control  \citep{dayan2014model}.  This means simply learning a mapping directly from the control variable to the target variable, without building a model of the relationships between the variables.  As it turned out, some structures that involved a mixture of generative and preventative connections were particularly hard to control, requiring the participant not just to understand the causal structure of the system, but also to plan their model based control strategy many frames ahead.  In these systems, the only effective way to cause the target variable to enter the reward region involved a periodic oscillation of the control variable.  Overall participants learned to control all but these structures well above chance, and learning profiles were broadly consistent with a causal model based controller but also with several other control systems including a deep neural network.  Thus future work will need to carefully probe what representation participants learn during a control period.  %Future work will use changing goals and control variables to explore the extent to which participants learn a causal model in this task that lets them generalize rapidly to new control tasks.


\subsection{Summary and discussion}

In summary, networks, made up of OU related continuous variables provide a powerful new representation for fine grained continuous time causal structure as well as an interesting test bed of control environments.  They map onto real world problems in intuitive ways and display the kinds of emergent properties we see in real world dynamic causal systems.  These networks also have a convenient mathematical properties that allow for normative inference and the assessment of the evidential value of interventions.  OU processes have the properties of being Gaussian, Markovian, and stationary all of which make likelihood estimation closed form and straightforward \citep{uhlenbeck1930theory}.  By having all nonstationarity be due to the changing target values determined by the a variable's causes, the resulting system is straightforward to analyze in spite of its rich emergent dynamics.   People were able to learn robustly through intervention in this setting, suggesting that they are not limited to reasoning about causality at the level of independent trials or even of real time event cascades.

Interventions in the CTN environment can be used to create dramatic signals that are hard to mistake if they appear in the dynamics of downstream variables.  Participants were generally sensitive to this in that they used dramatic swings or held variables at extreme values, which allows for maximally strong causal signal that is easily spotted as it propagates to other variables.  However, the richness and immediacy of the evidence also seemed to push participants toward adopting a local strategy focusing on pairs of variables at a time and so struggling to infer the correct structure in the case of indirect causation.  As with the event networks, a way to use interventions to get clearer local information in this setting would be to allow combination interventions in which one variable is held in place while another is wiggled around.  For instance holding $X_2$ stationary and moving $X_1$ up and down in Figure~\ref{fig:OU} would make it easy to establish that $X_1$does not directly influence $X_3$ except through $X_2$.  Intuitively, this is a very natural kind of intervention to perform in everyday life, but one that is not captured by the simpler and more abstract notions of intervention and causal influence embodied by the CBN or even the delay networks in Section~\ref{section:DN}.  One analogous case though is \cite{steyvers2003intervention}, an early study looking at interventional learning in a CBN setting where aliens read one another's minds.  In \citeapos{steyvers2003intervention} study, aliens' spontaneous thoughts were three letter strings using three different letters of the alphabet (``TUS'' or ``POR'').  However interventions always make them think ``ZZZ'', which they never normally think.  Thus when another alien was revealed to be thinking ``ZZZ'' it was possible to be very sure that this was effect of the intervention.  This highlights a key property of real world interventions that is hidden in the simpler representations.  Sometimes it is possible to insert a unique signal with an intervention by setting a variable to a highly unique value or moving it in a highly unusual way.  The subsequent inferences is then a kind of message detection, where one looks for traces of the original signal reappearing in other variables.
%We also argued that as interventions become extended and reactive actions, interventional learning mutates into a continuous control learning problem.  We discuss this further in the general discussion, this may mean we should start to move away from the analogy of interventional causal learning as a form of optimal experimentation, and toward the idea that causal learning is a process of balanced explorative control.

\section{General Discussion}

There are a number of reasons why CBNs provide a good starting point for thinking about causal cognition.  However, we have argued that there are also fundamental reasons why they do not provide a fully adequate account.  CBNs represent time only as a partial ordering of influence, while real learning contexts demand a deeper sensitivity to time's arrow.  We highlighted recent work introducing two new formalisms using them to explore the ways in which human causal learners are sensitive to time, both in informing their causal beliefs but also in timing their interventions and control.  The first representation, developed by \cite{bramley2018time}, captured causality relating events in time and showed how a preference for reliably timed causal influences can guide structure inference, as well as how sensitivity to the possibility of delayed effects shaped participants intervention behavior.  The second representation we discussed was the delay network developed by \cite{davis2018control}.  This latter approach captured causality at an even finer grain, modelling it terms of continuous influences between continuous valued variables. Comparison of human learners in a setting involving these factors again showed that people are capable of interpreting rich continuous evidence as well as using the broad support of the relevant variables to generate distinctive extended and controlled interventions whose signatures could be easily tracked as they propagate through the system.

\subsection{Interventions in rich domains}
While much of the work on intervention selection has taken the  perspective of intervention selection as a problem optimal experimental design \citep{fedorov1972theory}.  Exploring richer interventions and learning of richer causal representations seems to require a different perspective.  Interventions in richer contexts inject signals into causal systems, similar to how a plumber might pour dye down a sink while trying to figure out network of pipes in an old house.  Wherever the dye shows up must be downstream of the sink, and the length of time it takes to get there says something about the network of pipes in between.  When the propagating signal is not very distinct (such as the time of occurrence of an event as in Section~\ref{section:DN}, or the final states of the variables after a causality has been and gone (as in Section~\ref{fig:CBN}, collecting multiple approximately independent trials is still important.  However, the learner again must be able to use their knowledge about the domain to create situations that are approximately independent and identical. 

One other consideration is that punctate interventions, or ``shocks'' allow a dynamic system to continue to play out as before,  while sustained interventions also act as ``blocks'', and stop feedback loops.  This is valuable in that it can break a cyclic structure learning problem into several acyclic ones.  For instance, one can first check whether $X_i$ affects $X_j$ by performing a sustained intervention on $X_i$ before checking whether $X_j$ also affects $X_i$.  However, this approach will shortcircuit the natural dynamics that might themselves be a useful source of evidence.  As an analogy, one might contrast the punctate (and probably very distructive) intervention of throwing a coin into a washing machine while it is spinning to a more systematic sustained intervention in which one olds and turns one of the gears in the mechanism and observes what else moves.  %Link it loosely to signal detection theory/information theory, you want to maximize your chance of recapturing signal in other variables.  Analogy: throwing a coin in a washing machine when it is spinning vs clamping a gear and turning it on.

\subsection{The role of theory}

All of the representations we considered in this chapter are abstract in the sense that they do not encode anything specific about the mechanisms involved, notably saying nothing about how the parts of the systems are arranged in space.  However, we clearly make use of our knowledge of physical mechanisms when reasoning causally \citep{ahn1995role,bramley2017physics}.  Indeed, much of the recent work in causal cognition has emphasized the top down role of domain theories on causal inferences \citep{griffiths2009theory,griffiths2005causes,lake2015human}.  Such domain knowledge is modeled as affecting people's priors about what kinds of structure and parameters are plausible in a given situation.  For example basic medical knowledge will tell you that diseases cause symptoms rather than the reverse and that medicines often take on the order of hours to work.  Basic knowledge of electronics will tell you to expect causal influences to propagate at the sub-second level rather than across the aeons.  Thus, while the current frameworks show how causal beliefs can arise without much specific domain knowledge they are also happily compatible with other factors and knowledge playing into the choice of sensible priors.  Another way of thinking of specific domain knowledge is as rich functional forms governing the interactions between variables.  For example \cite{bramley2017physics} explore interventional learning about the physical properties of objects in simulated physical microworlds.  Here learners sophisticated understanding of how objects with different properties normally interact clearly plays into both the judgments they make and the actions they take in service of learning.  Interventional behaviors in this domain start to take on recognizable physical form such as shaking and throwing of objects and one another, but still subscribe to the principles exposed here or creating strong interventional signals that propagate through causal relationships (where these are literally magnet-style forces  through which the objects in the scenes push and pull one another around).  Learners appear to be able to compare the resulting trajectories in time against expectations simulated from their intuitive representation of physics and so back out the relevant properties.  In these domains our interventions and our interpretations of them become deeply theory laden, depending profoundly on our beliefs about how the domain works in general \citep{bramley2017neurath}.



\begin{enumerate}
    % \item Role of theory: Discretization and tracking of summary statistics/limited measurements (just how long for the ball to drop, strength of headache after fixed delay, etc) can be derived from the richer understanding of the problem
    % \item The minimal criteria for causation, that $X_1$ affects $X_2$ at some future time is very weak.  We cannot try all possible time lags and effect may fade in, fade out, reverse (e.g. effect of coffee on arousal: first there is no effect, then you feel more awake, then you feel more sleepy, then there is no effect anymore).  Domain knowledge lets us bootstrap expectations to focus on a good time window etc. \ntodo{Fit in the Buehner etc type expected delay results here briefly + certeris paribus shorter delays more likely to be causal as a domain general prior?}
    \item Abstraction, discretization \& aggregation (i.e. to tabular, CBN-ready data) becomes possible only once we have robust knowledge of when and how to measure effects (i.e. for electrical circuits CBN appropriate because effects are virtually synchronous, other domains we must use our theory to determine the right time windows and sufficient measurements).
    %     \item To extent our representations are rich in functional form, i.e. OU process, and intuitive physics, our interpretation of interventional evidence is richer too \citep{bramley2017physics}.  E.g. with physics we can compare the trajectories in time against expectations simulated from our theory

    % \item Relate this theory-ladenness of learning to Neurath's ship
 \end{enumerate}

\subsection{When to aggregate}
    
This is all not to say that there is no place for the CBN in causal cognition.  %Causal beliefs can be seen as a framework on which to hang evidence.
The richer representations explored here are useful for navigating causality at an everyday real time scale.  But many other phenomena of interest are too long range, and noisy to be identified through close focus on the minutea of change over time, and only become evident as data is aggregated to a much larger scale.  Time sensitive representations allow us to make the leap to this higher level of aggregation.  Once we have a robust knowledge of when and where to measure effects of medications for example, in our own case we can start gather evidence on a larger scale where time is increasingly abstracted away.  For example, reasoning at the level of continuous OU variables might help someone to play tennis, but then when deciding whether to follow a particular strategy it will be more useful to aggregate over multiple games of tennis in which different strategies were pursued, keeping track of how often they work.  Essentially we can use our more detailed and time sensitive theories to determine the relevant time windows and sufficient measurements for studying the subtler and longer scale causal relationships we are interested in establishing.  Indeed, it is only through this large scale organized process supported by expertise in the fine grained mechanisms, that humanity has been able to discover the weak relationships between behaviours and medical problems in later life such as the link between smoking and cancer \citep{gandini2008tobacco}, or the lack of one between vaccinations and autism \citep{verschuur1996hidden}.

So.. the thought is that intuitive theories/domain knowledge is what ends up allowing us to make the radical simplifications to high level models like CBNs in certain cases.  BUT that the raw temporal inference comes first and is more of an important part of human causal learning etc.

In general then the best interventional strategy for a learner depends on what they are interested in learning, how much they already know about the domain, as well as the granularity of available measurement.


\section{Conclusions}

In sum, this recent work is beginning to shed light on the sophistication and nuance involved in everyday causal cognition.  We are aware, as was Heraclitus, that one ``cannot step into the same river twice''\citep{barnes2013presocratics}.  And so we must rely on our sophisticated model based expectations combined with careful interrogatory actions to learn and exploit the causal world.

\clearpage
\bibliographystyle{apacite}
\bibliography{refs}
\end{document}